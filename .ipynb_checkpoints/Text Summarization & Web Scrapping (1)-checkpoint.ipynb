{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapping the text from webpage Using Beautiful Soup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs                                                           # BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scrape_webpage(url):\n",
    "       \n",
    "    scraped_textdata = urllib.request.urlopen(url)\n",
    "    textdata = scraped_textdata.read()\n",
    "    parsed_textdata = bs.BeautifulSoup(textdata,'lxml')\n",
    "    paragraphs = parsed_textdata.find_all('p')\n",
    "    formated_text = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        formated_text += para.text\n",
    "    \n",
    "    return formated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = _scrape_webpage('https://en.wikipedia.org/wiki/Natural_language_processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7690"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(mytext)\n",
    "len(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TOKENIZATION, PUNCTUATION, DIGIT AND STOP WORD REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7580\n"
     ]
    }
   ],
   "source": [
    "a = re.sub( r'\\d+', '', mytext)\n",
    "#print(a)\n",
    "#print(len(mytext))                               # digit removal\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after word tokenizing:  1294\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'an', 'interdisciplinary', 'subfield', 'of', 'computer', 'science', 'and', 'linguistics', '.', 'It', 'is', 'primarily', 'concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'support', 'and', 'manipulate', 'human', 'language', '.', 'It', 'involves', 'processing', 'natural', 'language', 'datasets', ',', 'such', 'as', 'text', 'corpora', 'or', 'speech', 'corpora', ',', 'using', 'either', 'rule-based']\n",
      "7580\n",
      "7690\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(a)\n",
    "print(\"Number of words after word tokenizing: \", len(tokens))                 #tokenizing the words\n",
    "print(tokens[:50])\n",
    "print(len(a))\n",
    "print(len(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Punctuation Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after word tokenizing with removing punctuation:  1124\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'an', 'interdisciplinary', 'subfield', 'of', 'computer', 'science', 'and', 'linguistics', 'it', 'is', 'primarily', 'concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'support', 'and', 'manipulate', 'human', 'language', 'it', 'involves', 'processing', 'natural', 'language', 'datasets', 'such', 'as', 'text', 'corpora', 'or', 'speech', 'corpora', 'using', 'either', 'rule', 'based', 'or', 'probabilistic', 'i', 'e', 'statistical']\n",
      "7580\n",
      "7690\n",
      "1124\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "regexp_tokens = tokenizer.tokenize(a.lower())                                   #punctuation removal\n",
    "print(\"Number of words after word tokenizing with removing punctuation: \", len(regexp_tokens))\n",
    "print(regexp_tokens[0:50])\n",
    "print(len(a))\n",
    "print(len(mytext))\n",
    "print(len(regexp_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stop Words Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words without stop words:  697\n",
      "['natural', 'language', 'processing', 'nlp', 'interdisciplinary', 'subfield', 'computer', 'science', 'linguistics', 'primarily', 'concerned', 'giving', 'computers', 'ability', 'support', 'manipulate', 'human', 'language', 'involves', 'processing', 'natural', 'language', 'datasets', 'text', 'corpora', 'speech', 'corpora', 'using', 'either', 'rule', 'based', 'probabilistic', 'e', 'statistical', 'recently', 'neural', 'network', 'based', 'machine', 'learning', 'approaches', 'goal', 'computer', 'capable', 'understanding', 'contents', 'documents', 'including', 'contextual', 'nuances']\n",
      "7580\n",
      "7690\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_tokens = [token for token in regexp_tokens if token not in stopwords.words('english')]\n",
    "print(\"# of words without stop words: \", len(stopwords_tokens))\n",
    "print(stopwords_tokens[0:50])                                                               #stopword removal\n",
    "print(len(a))\n",
    "print(len(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural => Natural\n",
      "language => language\n",
      "processing => processing\n",
      "( => (\n",
      "NLP => NLP\n",
      ") => )\n",
      "is => be\n",
      "an => an\n",
      "interdisciplinary => interdisciplinary\n",
      "subfield => subfield\n",
      "of => of\n",
      "computer => computer\n",
      "science => science\n",
      "and => and\n",
      "linguistics => linguistics\n",
      ". => .\n",
      "It => It\n",
      "is => be\n",
      "primarily => primarily\n",
      "concerned => concern\n",
      "with => with\n",
      "giving => give\n",
      "computers => computer\n",
      "the => the\n",
      "ability => ability\n",
      "to => to\n",
      "support => support\n",
      "and => and\n",
      "manipulate => manipulate\n",
      "human => human\n",
      "language => language\n",
      ". => .\n",
      "It => It\n",
      "involves => involve\n",
      "processing => process\n",
      "natural => natural\n",
      "language => language\n",
      "datasets => datasets\n",
      ", => ,\n",
      "such => such\n",
      "as => a\n",
      "text => text\n",
      "corpora => corpus\n",
      "or => or\n",
      "speech => speech\n",
      "corpora => corpus\n",
      ", => ,\n",
      "using => use\n",
      "either => either\n",
      "rule-based => rule-based\n",
      "or => or\n",
      "probabilistic => probabilistic\n",
      "( => (\n",
      "i.e => i.e\n",
      ". => .\n",
      "statistical => statistical\n",
      "and => and\n",
      ", => ,\n",
      "most => most\n",
      "recently => recently\n",
      ", => ,\n",
      "neural => neural\n",
      "network-based => network-based\n",
      ") => )\n",
      "machine => machine\n",
      "learning => learn\n",
      "approaches => approach\n",
      ". => .\n",
      "The => The\n",
      "goal => goal\n",
      "is => be\n",
      "a => a\n",
      "computer => computer\n",
      "capable => capable\n",
      "of => of\n",
      "`` => ``\n",
      "understanding => understanding\n",
      "'' => ''\n",
      "the => the\n",
      "contents => content\n",
      "of => of\n",
      "documents => document\n",
      ", => ,\n",
      "including => include\n",
      "the => the\n",
      "contextual => contextual\n",
      "nuances => nuance\n",
      "of => of\n",
      "the => the\n",
      "language => language\n",
      "within => within\n",
      "them => them\n",
      ". => .\n",
      "The => The\n",
      "technology => technology\n",
      "can => can\n",
      "then => then\n",
      "accurately => accurately\n",
      "extract => extract\n",
      "information => information\n",
      "and => and\n",
      "insights => insight\n",
      "contained => contain\n",
      "in => in\n",
      "the => the\n",
      "documents => document\n",
      "as => as\n",
      "well => well\n",
      "as => a\n",
      "categorize => categorize\n",
      "and => and\n",
      "organize => organize\n",
      "the => the\n",
      "documents => document\n",
      "themselves => themselves\n",
      ". => .\n",
      "Challenges => Challenges\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "frequently => frequently\n",
      "involve => involve\n",
      "speech => speech\n",
      "recognition => recognition\n",
      ", => ,\n",
      "natural-language => natural-language\n",
      "understanding => understanding\n",
      ", => ,\n",
      "and => and\n",
      "natural-language => natural-language\n",
      "generation => generation\n",
      ". => .\n",
      "Natural => Natural\n",
      "language => language\n",
      "processing => processing\n",
      "has => have\n",
      "its => it\n",
      "roots => root\n",
      "in => in\n",
      "the => the\n",
      "s. => s.\n",
      "Already => Already\n",
      "in => in\n",
      ", => ,\n",
      "Alan => Alan\n",
      "Turing => Turing\n",
      "published => publish\n",
      "an => an\n",
      "article => article\n",
      "titled => title\n",
      "`` => ``\n",
      "Computing => Computing\n",
      "Machinery => Machinery\n",
      "and => and\n",
      "Intelligence => Intelligence\n",
      "'' => ''\n",
      "which => which\n",
      "proposed => propose\n",
      "what => what\n",
      "is => be\n",
      "now => now\n",
      "called => call\n",
      "the => the\n",
      "Turing => Turing\n",
      "test => test\n",
      "as => a\n",
      "a => a\n",
      "criterion => criterion\n",
      "of => of\n",
      "intelligence => intelligence\n",
      ", => ,\n",
      "though => though\n",
      "at => at\n",
      "the => the\n",
      "time => time\n",
      "that => that\n",
      "was => be\n",
      "not => not\n",
      "articulated => articulate\n",
      "as => a\n",
      "a => a\n",
      "problem => problem\n",
      "separate => separate\n",
      "from => from\n",
      "artificial => artificial\n",
      "intelligence => intelligence\n",
      ". => .\n",
      "The => The\n",
      "proposed => proposed\n",
      "test => test\n",
      "includes => include\n",
      "a => a\n",
      "task => task\n",
      "that => that\n",
      "involves => involve\n",
      "the => the\n",
      "automated => automated\n",
      "interpretation => interpretation\n",
      "and => and\n",
      "generation => generation\n",
      "of => of\n",
      "natural => natural\n",
      "language => language\n",
      ". => .\n",
      "The => The\n",
      "premise => premise\n",
      "of => of\n",
      "symbolic => symbolic\n",
      "NLP => NLP\n",
      "is => be\n",
      "well-summarized => well-summarized\n",
      "by => by\n",
      "John => John\n",
      "Searle => Searle\n",
      "'s => 's\n",
      "Chinese => Chinese\n",
      "room => room\n",
      "experiment => experiment\n",
      ": => :\n",
      "Given => Given\n",
      "a => a\n",
      "collection => collection\n",
      "of => of\n",
      "rules => rule\n",
      "( => (\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "a => a\n",
      "Chinese => Chinese\n",
      "phrasebook => phrasebook\n",
      ", => ,\n",
      "with => with\n",
      "questions => question\n",
      "and => and\n",
      "matching => match\n",
      "answers => answer\n",
      ") => )\n",
      ", => ,\n",
      "the => the\n",
      "computer => computer\n",
      "emulates => emulate\n",
      "natural => natural\n",
      "language => language\n",
      "understanding => understanding\n",
      "( => (\n",
      "or => or\n",
      "other => other\n",
      "NLP => NLP\n",
      "tasks => task\n",
      ") => )\n",
      "by => by\n",
      "applying => apply\n",
      "those => those\n",
      "rules => rule\n",
      "to => to\n",
      "the => the\n",
      "data => data\n",
      "it => it\n",
      "confronts => confronts\n",
      ". => .\n",
      "Up => Up\n",
      "to => to\n",
      "the => the\n",
      "s => s\n",
      ", => ,\n",
      "most => most\n",
      "natural => natural\n",
      "language => language\n",
      "processing => process\n",
      "systems => system\n",
      "were => be\n",
      "based => base\n",
      "on => on\n",
      "complex => complex\n",
      "sets => set\n",
      "of => of\n",
      "hand-written => hand-written\n",
      "rules => rule\n",
      ". => .\n",
      "Starting => Starting\n",
      "in => in\n",
      "the => the\n",
      "late => late\n",
      "s => s\n",
      ", => ,\n",
      "however => however\n",
      ", => ,\n",
      "there => there\n",
      "was => be\n",
      "a => a\n",
      "revolution => revolution\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "with => with\n",
      "the => the\n",
      "introduction => introduction\n",
      "of => of\n",
      "machine => machine\n",
      "learning => learning\n",
      "algorithms => algorithm\n",
      "for => for\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "This => This\n",
      "was => be\n",
      "due => due\n",
      "to => to\n",
      "both => both\n",
      "the => the\n",
      "steady => steady\n",
      "increase => increase\n",
      "in => in\n",
      "computational => computational\n",
      "power => power\n",
      "( => (\n",
      "see => see\n",
      "Moore => Moore\n",
      "'s => 's\n",
      "law => law\n",
      ") => )\n",
      "and => and\n",
      "the => the\n",
      "gradual => gradual\n",
      "lessening => lessening\n",
      "of => of\n",
      "the => the\n",
      "dominance => dominance\n",
      "of => of\n",
      "Chomskyan => Chomskyan\n",
      "theories => theory\n",
      "of => of\n",
      "linguistics => linguistics\n",
      "( => (\n",
      "e.g => e.g\n",
      ". => .\n",
      "transformational => transformational\n",
      "grammar => grammar\n",
      ") => )\n",
      ", => ,\n",
      "whose => whose\n",
      "theoretical => theoretical\n",
      "underpinnings => underpinnings\n",
      "discouraged => discourage\n",
      "the => the\n",
      "sort => sort\n",
      "of => of\n",
      "corpus => corpus\n",
      "linguistics => linguistics\n",
      "that => that\n",
      "underlies => underlie\n",
      "the => the\n",
      "machine-learning => machine-learning\n",
      "approach => approach\n",
      "to => to\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "In => In\n",
      ", => ,\n",
      "word => word\n",
      "n-gram => n-gram\n",
      "model => model\n",
      ", => ,\n",
      "at => at\n",
      "the => the\n",
      "time => time\n",
      "the => the\n",
      "best => best\n",
      "statistical => statistical\n",
      "algorithm => algorithm\n",
      ", => ,\n",
      "was => be\n",
      "overperformed => overperformed\n",
      "by => by\n",
      "a => a\n",
      "multi-layer => multi-layer\n",
      "perceptron => perceptron\n",
      "( => (\n",
      "with => with\n",
      "a => a\n",
      "single => single\n",
      "hidden => hidden\n",
      "layer => layer\n",
      "and => and\n",
      "context => context\n",
      "length => length\n",
      "of => of\n",
      "several => several\n",
      "words => word\n",
      "trained => train\n",
      "on => on\n",
      "up => up\n",
      "to => to\n",
      "million => million\n",
      "of => of\n",
      "words => word\n",
      "with => with\n",
      "a => a\n",
      "CPU => CPU\n",
      "cluster => cluster\n",
      "in => in\n",
      "language => language\n",
      "modelling => modelling\n",
      ") => )\n",
      "by => by\n",
      "Yoshua => Yoshua\n",
      "Bengio => Bengio\n",
      "with => with\n",
      "co-authors => co-authors\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "In => In\n",
      ", => ,\n",
      "Tomáš => Tomáš\n",
      "Mikolov => Mikolov\n",
      "( => (\n",
      "then => then\n",
      "a => a\n",
      "PhD => PhD\n",
      "student => student\n",
      "at => at\n",
      "Brno => Brno\n",
      "University => University\n",
      "of => of\n",
      "Technology => Technology\n",
      ") => )\n",
      "with => with\n",
      "co-authors => co-authors\n",
      "applied => apply\n",
      "a => a\n",
      "simple => simple\n",
      "recurrent => recurrent\n",
      "neural => neural\n",
      "network => network\n",
      "with => with\n",
      "a => a\n",
      "single => single\n",
      "hidden => hidden\n",
      "layer => layer\n",
      "to => to\n",
      "language => language\n",
      "modelling => modelling\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "and => and\n",
      "in => in\n",
      "the => the\n",
      "following => following\n",
      "years => year\n",
      "he => he\n",
      "went => go\n",
      "on => on\n",
      "to => to\n",
      "develop => develop\n",
      "Wordvec => Wordvec\n",
      ". => .\n",
      "In => In\n",
      "the => the\n",
      "s => s\n",
      ", => ,\n",
      "representation => representation\n",
      "learning => learning\n",
      "and => and\n",
      "deep => deep\n",
      "neural => neural\n",
      "network-style => network-style\n",
      "( => (\n",
      "featuring => feature\n",
      "many => many\n",
      "hidden => hidden\n",
      "layers => layer\n",
      ") => )\n",
      "machine => machine\n",
      "learning => learn\n",
      "methods => method\n",
      "became => become\n",
      "widespread => widespread\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "That => That\n",
      "popularity => popularity\n",
      "was => be\n",
      "due => due\n",
      "partly => partly\n",
      "to => to\n",
      "a => a\n",
      "flurry => flurry\n",
      "of => of\n",
      "results => result\n",
      "showing => show\n",
      "that => that\n",
      "such => such\n",
      "techniques => technique\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "can => can\n",
      "achieve => achieve\n",
      "state-of-the-art => state-of-the-art\n",
      "results => result\n",
      "in => in\n",
      "many => many\n",
      "natural => natural\n",
      "language => language\n",
      "tasks => task\n",
      ", => ,\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "in => in\n",
      "language => language\n",
      "modeling => modeling\n",
      "[ => [\n",
      "] => ]\n",
      "and => and\n",
      "parsing => parse\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "This => This\n",
      "is => be\n",
      "increasingly => increasingly\n",
      "important => important\n",
      "in => in\n",
      "medicine => medicine\n",
      "and => and\n",
      "healthcare => healthcare\n",
      ", => ,\n",
      "where => where\n",
      "NLP => NLP\n",
      "helps => help\n",
      "analyze => analyze\n",
      "notes => note\n",
      "and => and\n",
      "text => text\n",
      "in => in\n",
      "electronic => electronic\n",
      "health => health\n",
      "records => record\n",
      "that => that\n",
      "would => would\n",
      "otherwise => otherwise\n",
      "be => be\n",
      "inaccessible => inaccessible\n",
      "for => for\n",
      "study => study\n",
      "when => when\n",
      "seeking => seek\n",
      "to => to\n",
      "improve => improve\n",
      "care => care\n",
      "[ => [\n",
      "] => ]\n",
      "or => or\n",
      "protect => protect\n",
      "patient => patient\n",
      "privacy => privacy\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "Symbolic => Symbolic\n",
      "approach => approach\n",
      ", => ,\n",
      "i.e. => i.e.\n",
      ", => ,\n",
      "the => the\n",
      "hand-coding => hand-coding\n",
      "of => of\n",
      "a => a\n",
      "set => set\n",
      "of => of\n",
      "rules => rule\n",
      "for => for\n",
      "manipulating => manipulate\n",
      "symbols => symbol\n",
      ", => ,\n",
      "coupled => couple\n",
      "with => with\n",
      "a => a\n",
      "dictionary => dictionary\n",
      "lookup => lookup\n",
      ", => ,\n",
      "was => be\n",
      "historically => historically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the => the\n",
      "first => first\n",
      "approach => approach\n",
      "used => use\n",
      "both => both\n",
      "by => by\n",
      "AI => AI\n",
      "in => in\n",
      "general => general\n",
      "and => and\n",
      "by => by\n",
      "NLP => NLP\n",
      "in => in\n",
      "particular => particular\n",
      ": => :\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "such => such\n",
      "as => a\n",
      "by => by\n",
      "writing => write\n",
      "grammars => grammar\n",
      "or => or\n",
      "devising => devise\n",
      "heuristic => heuristic\n",
      "rules => rule\n",
      "for => for\n",
      "stemming => stem\n",
      ". => .\n",
      "Machine => Machine\n",
      "learning => learn\n",
      "approaches => approach\n",
      ", => ,\n",
      "which => which\n",
      "include => include\n",
      "both => both\n",
      "statistical => statistical\n",
      "and => and\n",
      "neural => neural\n",
      "networks => network\n",
      ", => ,\n",
      "on => on\n",
      "the => the\n",
      "other => other\n",
      "hand => hand\n",
      ", => ,\n",
      "have => have\n",
      "many => many\n",
      "advantages => advantage\n",
      "over => over\n",
      "the => the\n",
      "symbolic => symbolic\n",
      "approach => approach\n",
      ": => :\n",
      "Although => Although\n",
      "rule-based => rule-based\n",
      "systems => system\n",
      "for => for\n",
      "manipulating => manipulate\n",
      "symbols => symbol\n",
      "were => be\n",
      "still => still\n",
      "in => in\n",
      "use => use\n",
      "in => in\n",
      ", => ,\n",
      "they => they\n",
      "have => have\n",
      "become => become\n",
      "mostly => mostly\n",
      "obsolete => obsolete\n",
      "with => with\n",
      "the => the\n",
      "advance => advance\n",
      "of => of\n",
      "LLMs => LLMs\n",
      "in => in\n",
      ". => .\n",
      "Before => Before\n",
      "that => that\n",
      "they => they\n",
      "were => be\n",
      "commonly => commonly\n",
      "used => use\n",
      ": => :\n",
      "In => In\n",
      "the => the\n",
      "late => late\n",
      "s => s\n",
      "and => and\n",
      "mid-s => mid-s\n",
      ", => ,\n",
      "the => the\n",
      "statistical => statistical\n",
      "approach => approach\n",
      "ended => end\n",
      "a => a\n",
      "period => period\n",
      "of => of\n",
      "AI => AI\n",
      "winter => winter\n",
      ", => ,\n",
      "which => which\n",
      "was => be\n",
      "caused => cause\n",
      "by => by\n",
      "the => the\n",
      "inefficiencies => inefficiency\n",
      "of => of\n",
      "the => the\n",
      "rule-based => rule-based\n",
      "approaches => approach\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "[ => [\n",
      "] => ]\n",
      "The => The\n",
      "earliest => early\n",
      "decision => decision\n",
      "trees => tree\n",
      ", => ,\n",
      "producing => produce\n",
      "systems => system\n",
      "of => of\n",
      "hard => hard\n",
      "if–then => if–then\n",
      "rules => rule\n",
      ", => ,\n",
      "were => be\n",
      "still => still\n",
      "very => very\n",
      "similar => similar\n",
      "to => to\n",
      "the => the\n",
      "old => old\n",
      "rule-based => rule-based\n",
      "approaches => approach\n",
      ". => .\n",
      "Only => Only\n",
      "the => the\n",
      "introduction => introduction\n",
      "of => of\n",
      "hidden => hidden\n",
      "Markov => Markov\n",
      "models => model\n",
      ", => ,\n",
      "applied => apply\n",
      "to => to\n",
      "part-of-speech => part-of-speech\n",
      "tagging => tagging\n",
      ", => ,\n",
      "announced => announce\n",
      "the => the\n",
      "end => end\n",
      "of => of\n",
      "the => the\n",
      "old => old\n",
      "rule-based => rule-based\n",
      "approach => approach\n",
      ". => .\n",
      "A => A\n",
      "major => major\n",
      "drawback => drawback\n",
      "of => of\n",
      "statistical => statistical\n",
      "methods => method\n",
      "is => be\n",
      "that => that\n",
      "they => they\n",
      "require => require\n",
      "elaborate => elaborate\n",
      "feature => feature\n",
      "engineering => engineering\n",
      ". => .\n",
      "Since => Since\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "the => the\n",
      "statistical => statistical\n",
      "approach => approach\n",
      "was => be\n",
      "replaced => replace\n",
      "by => by\n",
      "neural => neural\n",
      "networks => network\n",
      "approach => approach\n",
      ", => ,\n",
      "using => use\n",
      "word => word\n",
      "embeddings => embeddings\n",
      "to => to\n",
      "capture => capture\n",
      "semantic => semantic\n",
      "properties => property\n",
      "of => of\n",
      "words => word\n",
      ". => .\n",
      "Intermediate => Intermediate\n",
      "tasks => task\n",
      "( => (\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "part-of-speech => part-of-speech\n",
      "tagging => tagging\n",
      "and => and\n",
      "dependency => dependency\n",
      "parsing => parse\n",
      ") => )\n",
      "have => have\n",
      "not => not\n",
      "been => be\n",
      "needed => need\n",
      "anymore => anymore\n",
      ". => .\n",
      "Neural => Neural\n",
      "machine => machine\n",
      "translation => translation\n",
      ", => ,\n",
      "based => base\n",
      "on => on\n",
      "then-newly-invented => then-newly-invented\n",
      "sequence-to-sequence => sequence-to-sequence\n",
      "transformations => transformation\n",
      ", => ,\n",
      "made => make\n",
      "obsolete => obsolete\n",
      "the => the\n",
      "intermediate => intermediate\n",
      "steps => step\n",
      ", => ,\n",
      "such => such\n",
      "as => a\n",
      "word => word\n",
      "alignment => alignment\n",
      ", => ,\n",
      "previously => previously\n",
      "necessary => necessary\n",
      "for => for\n",
      "statistical => statistical\n",
      "machine => machine\n",
      "translation => translation\n",
      ". => .\n",
      "The => The\n",
      "following => following\n",
      "is => be\n",
      "a => a\n",
      "list => list\n",
      "of => of\n",
      "some => some\n",
      "of => of\n",
      "the => the\n",
      "most => most\n",
      "commonly => commonly\n",
      "researched => researched\n",
      "tasks => task\n",
      "in => in\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      ". => .\n",
      "Some => Some\n",
      "of => of\n",
      "these => these\n",
      "tasks => task\n",
      "have => have\n",
      "direct => direct\n",
      "real-world => real-world\n",
      "applications => application\n",
      ", => ,\n",
      "while => while\n",
      "others => others\n",
      "more => more\n",
      "commonly => commonly\n",
      "serve => serve\n",
      "as => a\n",
      "subtasks => subtasks\n",
      "that => that\n",
      "are => be\n",
      "used => use\n",
      "to => to\n",
      "aid => aid\n",
      "in => in\n",
      "solving => solve\n",
      "larger => large\n",
      "tasks => task\n",
      ". => .\n",
      "Though => Though\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "tasks => task\n",
      "are => be\n",
      "closely => closely\n",
      "intertwined => intertwine\n",
      ", => ,\n",
      "they => they\n",
      "can => can\n",
      "be => be\n",
      "subdivided => subdivide\n",
      "into => into\n",
      "categories => category\n",
      "for => for\n",
      "convenience => convenience\n",
      ". => .\n",
      "A => A\n",
      "coarse => coarse\n",
      "division => division\n",
      "is => be\n",
      "given => give\n",
      "below => below\n",
      ". => .\n",
      "Based => Based\n",
      "on => on\n",
      "long-standing => long-standing\n",
      "trends => trend\n",
      "in => in\n",
      "the => the\n",
      "field => field\n",
      ", => ,\n",
      "it => it\n",
      "is => be\n",
      "possible => possible\n",
      "to => to\n",
      "extrapolate => extrapolate\n",
      "future => future\n",
      "directions => direction\n",
      "of => of\n",
      "NLP => NLP\n",
      ". => .\n",
      "As => As\n",
      "of => of\n",
      ", => ,\n",
      "three => three\n",
      "trends => trend\n",
      "among => among\n",
      "the => the\n",
      "topics => topic\n",
      "of => of\n",
      "the => the\n",
      "long-standing => long-standing\n",
      "series => series\n",
      "of => of\n",
      "CoNLL => CoNLL\n",
      "Shared => Shared\n",
      "Tasks => Tasks\n",
      "can => can\n",
      "be => be\n",
      "observed => observe\n",
      ": => :\n",
      "[ => [\n",
      "] => ]\n",
      "Most => Most\n",
      "higher-level => higher-level\n",
      "NLP => NLP\n",
      "applications => application\n",
      "involve => involve\n",
      "aspects => aspect\n",
      "that => that\n",
      "emulate => emulate\n",
      "intelligent => intelligent\n",
      "behaviour => behaviour\n",
      "and => and\n",
      "apparent => apparent\n",
      "comprehension => comprehension\n",
      "of => of\n",
      "natural => natural\n",
      "language => language\n",
      ". => .\n",
      "More => More\n",
      "broadly => broadly\n",
      "speaking => speak\n",
      ", => ,\n",
      "the => the\n",
      "technical => technical\n",
      "operationalization => operationalization\n",
      "of => of\n",
      "increasingly => increasingly\n",
      "advanced => advanced\n",
      "aspects => aspect\n",
      "of => of\n",
      "cognitive => cognitive\n",
      "behaviour => behaviour\n",
      "represents => represent\n",
      "one => one\n",
      "of => of\n",
      "the => the\n",
      "developmental => developmental\n",
      "trajectories => trajectory\n",
      "of => of\n",
      "NLP => NLP\n",
      "( => (\n",
      "see => see\n",
      "trends => trend\n",
      "among => among\n",
      "CoNLL => CoNLL\n",
      "shared => share\n",
      "tasks => task\n",
      "above => above\n",
      ") => )\n",
      ". => .\n",
      "Cognition => Cognition\n",
      "refers => refers\n",
      "to => to\n",
      "`` => ``\n",
      "the => the\n",
      "mental => mental\n",
      "action => action\n",
      "or => or\n",
      "process => process\n",
      "of => of\n",
      "acquiring => acquire\n",
      "knowledge => knowledge\n",
      "and => and\n",
      "understanding => understand\n",
      "through => through\n",
      "thought => thought\n",
      ", => ,\n",
      "experience => experience\n",
      ", => ,\n",
      "and => and\n",
      "the => the\n",
      "senses => sens\n",
      ". => .\n",
      "`` => ``\n",
      "[ => [\n",
      "] => ]\n",
      "Cognitive => Cognitive\n",
      "science => science\n",
      "is => be\n",
      "the => the\n",
      "interdisciplinary => interdisciplinary\n",
      ", => ,\n",
      "scientific => scientific\n",
      "study => study\n",
      "of => of\n",
      "the => the\n",
      "mind => mind\n",
      "and => and\n",
      "its => it\n",
      "processes => process\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "Cognitive => Cognitive\n",
      "linguistics => linguistics\n",
      "is => be\n",
      "an => an\n",
      "interdisciplinary => interdisciplinary\n",
      "branch => branch\n",
      "of => of\n",
      "linguistics => linguistics\n",
      ", => ,\n",
      "combining => combine\n",
      "knowledge => knowledge\n",
      "and => and\n",
      "research => research\n",
      "from => from\n",
      "both => both\n",
      "psychology => psychology\n",
      "and => and\n",
      "linguistics => linguistics\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "Especially => Especially\n",
      "during => during\n",
      "the => the\n",
      "age => age\n",
      "of => of\n",
      "symbolic => symbolic\n",
      "NLP => NLP\n",
      ", => ,\n",
      "the => the\n",
      "area => area\n",
      "of => of\n",
      "computational => computational\n",
      "linguistics => linguistics\n",
      "maintained => maintain\n",
      "strong => strong\n",
      "ties => tie\n",
      "with => with\n",
      "cognitive => cognitive\n",
      "studies => study\n",
      ". => .\n",
      "As => As\n",
      "an => an\n",
      "example => example\n",
      ", => ,\n",
      "George => George\n",
      "Lakoff => Lakoff\n",
      "offers => offer\n",
      "a => a\n",
      "methodology => methodology\n",
      "to => to\n",
      "build => build\n",
      "natural => natural\n",
      "language => language\n",
      "processing => processing\n",
      "( => (\n",
      "NLP => NLP\n",
      ") => )\n",
      "algorithms => algorithms\n",
      "through => through\n",
      "the => the\n",
      "perspective => perspective\n",
      "of => of\n",
      "cognitive => cognitive\n",
      "science => science\n",
      ", => ,\n",
      "along => along\n",
      "with => with\n",
      "the => the\n",
      "findings => finding\n",
      "of => of\n",
      "cognitive => cognitive\n",
      "linguistics => linguistics\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "with => with\n",
      "two => two\n",
      "defining => define\n",
      "aspects => aspect\n",
      ": => :\n",
      "Ties => Ties\n",
      "with => with\n",
      "cognitive => cognitive\n",
      "linguistics => linguistics\n",
      "are => be\n",
      "part => part\n",
      "of => of\n",
      "the => the\n",
      "historical => historical\n",
      "heritage => heritage\n",
      "of => of\n",
      "NLP => NLP\n",
      ", => ,\n",
      "but => but\n",
      "they => they\n",
      "have => have\n",
      "been => be\n",
      "less => less\n",
      "frequently => frequently\n",
      "addressed => address\n",
      "since => since\n",
      "the => the\n",
      "statistical => statistical\n",
      "turn => turn\n",
      "during => during\n",
      "the => the\n",
      "s. => s.\n",
      "Nevertheless => Nevertheless\n",
      ", => ,\n",
      "approaches => approach\n",
      "to => to\n",
      "develop => develop\n",
      "cognitive => cognitive\n",
      "models => model\n",
      "towards => towards\n",
      "technically => technically\n",
      "operationalizable => operationalizable\n",
      "frameworks => framework\n",
      "have => have\n",
      "been => be\n",
      "pursued => pursue\n",
      "in => in\n",
      "the => the\n",
      "context => context\n",
      "of => of\n",
      "various => various\n",
      "frameworks => framework\n",
      ", => ,\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "of => of\n",
      "cognitive => cognitive\n",
      "grammar => grammar\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "functional => functional\n",
      "grammar => grammar\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "construction => construction\n",
      "grammar => grammar\n",
      ", => ,\n",
      "[ => [\n",
      "] => ]\n",
      "computational => computational\n",
      "psycholinguistics => psycholinguistics\n",
      "and => and\n",
      "cognitive => cognitive\n",
      "neuroscience => neuroscience\n",
      "( => (\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "ACT-R => ACT-R\n",
      ") => )\n",
      ", => ,\n",
      "however => however\n",
      ", => ,\n",
      "with => with\n",
      "limited => limited\n",
      "uptake => uptake\n",
      "in => in\n",
      "mainstream => mainstream\n",
      "NLP => NLP\n",
      "( => (\n",
      "as => a\n",
      "measured => measure\n",
      "by => by\n",
      "presence => presence\n",
      "on => on\n",
      "major => major\n",
      "conferences => conference\n",
      "[ => [\n",
      "] => ]\n",
      "of => of\n",
      "the => the\n",
      "ACL => ACL\n",
      ") => )\n",
      ". => .\n",
      "More => More\n",
      "recently => recently\n",
      ", => ,\n",
      "ideas => idea\n",
      "of => of\n",
      "cognitive => cognitive\n",
      "NLP => NLP\n",
      "have => have\n",
      "been => be\n",
      "revived => revive\n",
      "as => a\n",
      "an => an\n",
      "approach => approach\n",
      "to => to\n",
      "achieve => achieve\n",
      "explainability => explainability\n",
      ", => ,\n",
      "e.g. => e.g.\n",
      ", => ,\n",
      "under => under\n",
      "the => the\n",
      "notion => notion\n",
      "of => of\n",
      "`` => ``\n",
      "cognitive => cognitive\n",
      "AI => AI\n",
      "'' => ''\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "Likewise => Likewise\n",
      ", => ,\n",
      "ideas => idea\n",
      "of => of\n",
      "cognitive => cognitive\n",
      "NLP => NLP\n",
      "are => be\n",
      "inherent => inherent\n",
      "to => to\n",
      "neural => neural\n",
      "models => model\n",
      "multimodal => multimodal\n",
      "NLP => NLP\n",
      "( => (\n",
      "although => although\n",
      "rarely => rarely\n",
      "made => make\n",
      "explicit => explicit\n",
      ") => )\n",
      "[ => [\n",
      "] => ]\n",
      "and => and\n",
      "developments => development\n",
      "in => in\n",
      "Artificial => Artificial\n",
      "intelligence => intelligence\n",
      ", => ,\n",
      "specifically => specifically\n",
      "tools => tool\n",
      "and => and\n",
      "technologies => technology\n",
      "using => use\n",
      "Large => Large\n",
      "language => language\n",
      "model => model\n",
      "approaches => approach\n",
      ". => .\n",
      "[ => [\n",
      "] => ]\n",
      "7580\n",
      "7690\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text= stopwords_tokens\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token, tag_map[tag[0]])                         #lemmatization\n",
    "    print(token, \"=>\", lemma)\n",
    "    \n",
    "print(len(a))\n",
    "print(len(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PLOT TOP 15 WORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 417 samples and 697 outcomes>\n",
      "[('language', 24), ('natural', 16), ('nlp', 15), ('cognitive', 13), ('processing', 12), ('linguistics', 9), ('based', 9), ('e', 9), ('tasks', 9), ('approach', 9), ('statistical', 8), ('neural', 7), ('machine', 7), ('g', 7), ('learning', 6)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEyCAYAAADgEkc1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA9h0lEQVR4nO3deXhU5fXA8e9JWEII+2ZYA4ILq5oIuItWi7jVtVJ/1BXaalu11qK1VltttVi3YqvFBW0V3DcQEUQQsbIksi8Ksiiyyb6ELeT8/njvwBAmEObem5nJnM/zzJPMnblnDiGZM/ddRVUxxhhjyspIdALGGGOSkxUIY4wxMVmBMMYYE5MVCGOMMTFZgTDGGBNTtUQnEKTGjRtrXl5eXOdu376dWrVqBZtQSHFTKdew4qZSrqkWN5VyTbW4yZhrUVHRWlVtEvNBVa0yt/z8fI1XYWFh3OdWdtxUyjWsuKmUa6rFTaVcUy1uMuYKFGo576nWxGSMMSYmKxDGGGNisgJhjDEmJisQxhhjYrICYYwxJiYrEMYYY2KyAmGMMSamKjVRLh6L1mzh+c+WsmfLVvLzE52NMcYkj7QvENt27mHYlG84Iicz0akYY0xSSfsmpk7N65JTsxqrtu5h1aYdiU7HGGOSRtoXiGqZGRTkNQBgypJ1Cc7GGGOSR9oXCIAebRsBMHnx+gRnYowxycMKBNCjXUPAriCMMSaaFQigS4t6ZGUKi7/fxprN1g9hjDFgBQKA6pkZHN24OgBTllgzkzHGgBWIvTo1qQFYM5MxxkRYgfBECoR1VBtjjGMFwnNkw+pkVc9g0ZqtrN26M9HpGGNMwlmB8FTPEPLbuPkQU60fwhhjrEBE67l3PoT1QxhjjBWIKD3auQIxxfohjDHGCkS0bq3qUbNaBl+u3sL6bbsSnY4xxiRUaAVCRFqJyHgRmS8ic0XkFu/4wyKyQERmicjbIlK/nPOXishsEZkhIoVh5RmtZrVMTmgd6YewZiZjTHoL8wqiBLhdVY8FegI3i0hHYCzQWVW7Al8Bdx0kRi9VPU5VC0LMcz+RZTdsuKsxJt2FViBUdaWqfuF9vwWYD7RQ1TGqWuI9bTLQMqwc4tHDOqqNMQYAUdXwX0QkD5iIu3LYHHV8BPCqqr4U45wlwAZAgX+r6pByYg8ABgDk5ubmjxgxIq4ci4uLyc7OZtce5afvrKakFIZe3JQ6NfzV0EjcIIURM9XiplKuqRY3lXJNtbjJmGtBQUFRua00qhrqDcgBioBLyxy/G3gbr0jFOK+597UpMBM4/VCvlZ+fr/EqLCzc+/0VT/9P2wwcqR/OWRl3vFhxgxJGzFSLm0q5plrcVMo11eImY65AoZbznhrqKCYRqQ68Cbysqm9FHb8GuAC42kvwAKq6wvu6BldIuoeZa7Seba0fwhhjwhzFJMBzwHxVfTTqeG9gIHCRqhaXc25tEakT+R44F5gTVq5l9YzMh7CRTMaYNBbmFcQpQD/gLG+o6gwR6QM8CdQBxnrHngYQkeYiMso7txkwSURmAlOB91V1dIi57uf41g2oninMW7mZTdt3V9bLGmNMUqkWVmBVnQRIjIdGxTgWaVLq432/GOgWVm6HUqtGJt1a1qdw2QamLVnPDzo2S1QqxhiTMDaTuhzWzGSMSXdWIMqxb59q66g2xqQnKxDlyG/TgGoZwpzvNrF5h/VDGGPSjxWIcmTXqEbXlvUoVShauiHR6RhjTKWzAnEQkeW/J1s/hDEmDVmBOIhIR7VNmDPGpCMrEAeR36YBmV4/xNadJYc+wRhjqhArEAeRU7ManVvUY0+pUrTM+iGMMenFCsQh9Ny7P4T1Qxhj0osViEPo2TayT7UVCGNMerECcQgFeQ3IEJi1fBPFu6wfwhiTPqxAHEKdrOp0blGPEuuHMMakGSsQFdDD2x9iig13NcakESsQFWD7VBtj0pEViAo4sW1DRGDm8o1s37Un0ekYY0ylsAJRAfVqVadjbl1271Gmf2P9EMaY9GAFooKsmckYk26sQFTQ3glztj+EMSZNhFYgRKSViIwXkfkiMldEbvGONxSRsSKy0PvaoJzze4vIlyKySETuDCvPiuru9UPM+HYjO3ZbP4QxpuoL8wqiBLhdVY8FegI3i0hH4E5gnKp2AMZ59/cjIpnAP4HzgI5AX+/chKmfXYOjm9VhV0kp07/ZmMhUjDGmUoRWIFR1pap+4X2/BZgPtAAuBl70nvYi8KMYp3cHFqnqYlXdBbzinZdQtk+1MSadiKqG/yIiecBEoDPwjarWj3psg6o2KPP8y4Heqnqjd78f0ENVfxkj9gBgAEBubm7+iBEj4sqxuLiY7Ozsgz5n8vIdPPz5Rjo3qcGfzmwYWNzDFUbMVIubSrmmWtxUyjXV4iZjrgUFBUWqWhDzQVUN9QbkAEXApd79jWUe3xDjnCuAZ6Pu9wMGH+q18vPzNV6FhYWHfM66rTu1zcCRetTdo3TH7pLA4h6uMGKmWtxUyjXV4qZSrqkWNxlzBQq1nPfUUEcxiUh14E3gZVV9yzu8WkRyvcdzgTUxTl0OtIq63xJYEWauFdGwtuuH2FlSysxvNyU6HWOMCVWYo5gEeA6Yr6qPRj30HnCN9/01wLsxTp8GdBCRtiJSA7jKOy/herSLrMtk/RDGmKotzCuIU3BNQ2eJyAzv1gd4CDhHRBYC53j3EZHmIjIKQFVLgF8CH+I6t19T1bkh5lphe/epto5qY0wVVy2swKo6CZByHj47xvNXAH2i7o8CRoWTXfy6eyu7Fi3bwK6SUmpUs7mGxpiqyd7dDlPjnJq0b5rDjt2lzP5uY6LTMcaY0FiBiMO+fapt2Q1jTNVlBSIOtnCfMSYdWIGIQ2QkU9GyDezeU5rgbIwxJhxWIOLQtE4W7ZrUpnjXHmZ/Z/MhjDFVkxWIOEWamWyfamNMVWUFIk77OqqtH8IYUzVZgYhT5AqicOl6SqwfwhhTBVmBiNMR9bLIa5TNtl17mLtic6LTMcaYwFmB8MGGuxpjqjIrED70PNJbuM/2qTbGVEFWIHyIXEFMW7KePaXhb7xkjDGVyQqED83r16JVw1ps2VnCPOuHMMZUMVYgfOrZ1vapNsZUTVYgfOoR2R/CJswZY6oYKxA+9fD2h5i6ZJ31QxhjqhQrED61aphNi/q12LyjhAWrrB/CGFN1WIEIwL59qq2ZyRhTdYRWIETkeRFZIyJzoo69GrU/9VIRmVHOuUtFZLb3vMKwcgxKT5swZ4ypgkLbkxp4AXgS+E/kgKr+OPK9iDwCHGyt7F6quja07ALU0+uonrp0PaWlSkZGeVtxG2NM6gjtCkJVJwIx21xERIArgeFhvX5latWwFrn1sthYvJuv1mxJdDrGGBOIRPVBnAasVtWF5TyuwBgRKRKRAZWYV1xEZO9VxOSvrZnJGFM1iGp4QzNFJA8Yqaqdyxx/Clikqo+Uc15zVV0hIk2BscCvvCuSWM8dAAwAyM3NzR8xYkRcuRYXF5OdnR3XuQAfLS7mqaLN9GxRkztObhBY3FjCiJlqcVMp11SLm0q5plrcZMy1oKCgSFULYj6oqqHdgDxgTplj1YDVQMsKxrgP+G1Fnpufn6/xKiwsjPtcVdXF32/VNgNH6vF/HqOlpaWBxY0ljJipFjeVck21uKmUa6rFTcZcgUIt5z01EU1MPwAWqOryWA+KSG0RqRP5HjgXmBPruckkr1E2zerWZP22XSxcszXR6RhjjG9hDnMdDnwOHC0iy0XkBu+hqyjTOS0izUVklHe3GTBJRGYCU4H3VXV0WHkGRUSi9qm2fghjTOoLbZirqvYt5/i1MY6tAPp43y8GuoWVV5h6tGvIezNXMHnxevqdlJfodIwxxhebSR2gyEimKUvWRfpPjDEmZVmBCFC7xrVpnFOTtVt38fX32xKdjjHG+GIFIkAisnddJlt2wxiT6qxABGxfM5Mt3GeMSW1WIALWs21kZVfrhzDGpDYrEAFr3zSHRrVrsGbLTpastX4IY0zqsgIRsOh+CGtmMsakssMuECLSQES6hpFMVWET5owxVUGFCoSITBCRuiLSEJgJDBWRR8NNLXXtG8m03vohjDEpq6JXEPVUdTNwKTBUVfNxayqZGI5qWocG2dVZtXkHq7ftSXQ6xhgTl4oWiGoikovb5GdkiPlUCRkZQndvNNOHXxezs8SKhDEm9VS0QPwJ+BC3h8M0EWkHlLfZjwH6dMkF4L2viun18ASGTfmGXSWlCc7KGGMqrqIFYqWqdlXVm2DvgnrWB3EQF3Vrzr+uPoFWdauxYtMOfv/2bHr9fQKvTP2G3XusUBhjkl9FC8TgCh4zHhGhT5dcHj23EYP7Hk/7pjl8t3E7d741m7MemcBrhd9SYoXCGJPEDrrct4icBJwMNBGR30Q9VBfIDDOxqiJDhAu7NadPl1xGzlrBE+MWsvj7bfzujVn8c/wifn1WBy4+rjnVMm1KijEmuRzqXakGkIMrJHWibpuBy8NNrWrJzBAuPq4FY287g8d+3I22jWuzbF0xt78+k3Mem8jb05ezp9SGxBpjksdBryBU9RPgExF5QVWXVVJOVVpmhnDJ8S25sGtz3p2xgn98vJAla7dx26szefLjRfz67A5c0LU5mRmS6FSNMWmuojvK1RSRIUBe9DmqelYYSaWDapkZXJbfkouOa87b079j8McL+fr7bdzyygwGf7yIW87uwPldcsmwQmGMSZCKFojXgaeBZwEb1B+g6pkZXFnQikuOb8GbRcsZ/PEiFq3Zyq+GT+fJjxdxyw860LvTEVYojDGVrqI9oyWq+pSqTlXVosjtYCeIyPMiskZE5kQdu09EvhORGd6tTznn9haRL0VkkYjceRj/npRVPTODq7q3Zvxvz+Svl3Sheb0svly9hZte/oI+//iU0XNW2bIdxphKVdECMUJEbhKRXBFpGLkd4pwXgN4xjj+mqsd5t1FlHxSRTOCfwHlAR6CviHSsYJ4pr0a1DH7SozXj7ziT+3/Umdx6WSxYtYWfv1TE+f+YxJi5ViiMMZWjok1M13hf74g6pkC78k5Q1YkikhdHTt1xM7YXA4jIK8DFwLw4YqWsmtUy6dezDVcWtOTVad/yz/GLmLdyMwP+W0T7BtV5sf12WtSvleg0jTFVmIT5adQrECNVtbN3/z7gWtww2ULgdlXdUOacy4Heqnqjd78f0ENVf1nOawwABgDk5ubmjxgxIq5ci4uLyc7Ojuvcyoi7a48ydnExby3YxsYdpTSrncmfz2xI4+zgpqMk+88g7JgWN7yYFje8mH7jFhQUFKlqQcwHVfWQN+CnsW4VOC8PmBN1vxlugl0G8Bfg+RjnXAE8G3W/HzC4Innm5+drvAoLC+M+tzLjbizepWc9NFrbDByppw/6WFdsLA4sdqr8DMKKaXHDi2lxw4vpNy5QqOW8p1a0D+LEqNtpwH3ARYdTpbxitFpV96hqKfAMrjmprOVAq6j7LYEVh/taVVW9WtX54+kN6dyiLsvWFfOTZ6awevOORKdljKmCKlQgVPVXUbf+wPG4WdaHxVsyPOISYE6Mp00DOohIWxGpAVwFvHe4r1WV5dTI4KUbetAxty5L1m6j75DJrLEiYYwJWLwLABUDHQ72BBEZDnwOHC0iy0XkBmCQiMwWkVlAL+A277nNRWQUgKqWAL/ELS8+H3hNVefGmWeVVT+7Bi/f2INjc+uyeO02rnpmMmu2WJEwxgSnQqOYRGQEbtQSuD6EY4HXDnaOqvaNcfi5cp67AugTdX8UcMAQWLO/BrVdkfjJM5NZsGoLP3lmCsP796RJnZqJTs0YUwVUdJjr36O+LwGWqeryEPIxh6nh3iIxhS9Xb+Enz0xm+ICeNM6xImGM8aeifRCfAAtwK7k2AHaFmZQ5PI1yavJy/x50aJrDwjVbufqZKazbujPRaRljUlyFCoSIXAlMxQ1BvRKY4s1XMEmicU5NhvXvSfumOXy5egtXPzuF9dusjhtj4lfRTuq7gRNV9RpV/SlueOo94aVl4tGkTk2G9e9Buya1WbDKFYkNViSMMXGqaIHIUNU1UffXHca5phI1rZPFK/170q5xbeav3Mz/PTeFjcVWJIwxh6+ib/KjReRDEblWRK4F3sdGGSWtpnWzGNa/J3mNspm7YjP9npvKpuLdiU7LGJNiDlogRKS9iJyiqncA/wa6At1w8xuGVEJ+Jk5H1Mti+ICetGmUzezvNtHv+Sls2m5FwhhTcYe6gngc2AKgqm+p6m9U9Tbc1cPj4aZm/MqtV4vh/XvSumE2s5Zv4qfPT2XzDisSxpiKOVSByFPVWWUPqmohbiE+k+Sa16/F8AE9adWwFjO/3cg1z09lixUJY0wFHKpAZB3kMduMIEW0qO+uJFrUr8X0bzZy7dBpbN1Zkui0jDFJ7lAFYpqI9C970FtX6aBbjprk0rJBNq8McEWiaNkGrn1+qhUJY8xBHapA3ApcJyITROQR7/YJcCNwS+jZmUC1apjN8P49aV4vi8JlG7h+6DS2WZEwxpTjoAXC27/hZOBPwFLv9idVPUlVV4Wfngla60bZDB/QkyPqZjF16Xquf2EaxbusSBhjDlTRtZjGq+pg7/Zx2EmZcLVpVJvhA3rSrG5NpixZzw0vFLJ9155Ep2WMSTI2GzpNtW1cm+H9e9K0Tk0+X7yOG/8zjZ17wtuf3BiTeqxApLF2TXIY5u0f8dmidfz5k/XM/HZjotMyxiQJKxBprn3THIb370HjnJosWLebi//5GTe8MI3ZyzclOjVjTIJZgTC0b1qHMbedzo+Ork2t6pmMW7CGC5+cxI0vFjLnOysUxqSr0AqEiDwvImtEZE7UsYdFZIGIzBKRt0WkfjnnLvX2rp4hIoVh5Wj2aVi7Bv261uHTgb3of1pbsqpn8NH81VwweBI/+28h81duTnSKxphKFuYVxAtA7zLHxgKdVbUr8BVw10HO76Wqx6lqQUj5mRga59Tk7vM7MvF3vbjh1LbUrJbBh3NXc94Tn3LTy0V8uWpLolM0xlSS0AqEqk4E1pc5NkZVI4PuJwMtw3p940/TOlncc0FHPv1dL649OY8a1TIYNXsVvZ+YyM3DvmDhaisUxlR1ieyDuB74oJzHFBgjIkUiMqASczJlNK2bxX0XdWLiHb245qQ2VM/I4P1ZKzn38Yn8evh0Fq3ZmugUjTEhEdXwxr6LSB4wUlU7lzl+N1AAXKoxEhCR5qq6QkSa4pqlfuVdkcR6jQHAAIDc3Nz8ESNGxJVrcXEx2dnZcZ1b2XETmeva4j28tWAr4xZvp0TdJ4xTW2dxRcccmtepVmn5ptL/V6rFTaVcUy1uMuZaUFBQVG5TvqqGdsMtCT6nzLFrcBsOZVcwxn3Abyvy3Pz8fI1XYWFh3OdWdtxkyPXb9dv0zjdn6ZF3va9tBo7UtneO1Ntena5Lvt/qK25FJcPPoKrGTaVcUy1uMuYKFGo576mV2sQkIr2BgcBFqlpcznNqi0idyPfAucCcWM81idOyQTYPXtqF8b89k6tObIWI8NYX33H2o59wx+sz+WZdzP9eY0wKCXOY63DclcLRIrLcWyL8SaAOMNYbwvq099zmIhLZ47oZMElEZgJTgfdVdXRYeRp/WjXM5qHLujL+9jO5It+NOXi9aDlnPTKBO9+cxbfrrVAYk6piNxoHQFX7xjj8XDnPXQH08b5fjNv32qSQ1o2yefiKbtzcqz2DP17E29OX88q0b3mjaDn9uuSQn5/oDI0xh8tmUptA5TWuzSNXduOj35zBJce3YI8qL87aYkt3GJOCrECYULRrksNjPz6O609pS6nCHW/MZFdJaaLTMsYcBisQJlS/PfdojsjJZMGqLTw5flGi0zHGHAYrECZUtWpkclNBPQD+NX4R81bYmk7GpAorECZ0nZrU4KcntaGkVLnjjZns3mNNTcakAisQplIM7H0MLRvUYu6Kzfz7k68TnY4xpgKsQJhKUbtmNf52WVcA/jFuEV/ZYn/GJD0rEKbSnNK+MX27t2bXnlLueH0mJdbUZExSswJhKtXv+xxD83pZzFy+iWcnLUl0OsaYg7ACYSpVnazqPOg1NT069itbLtyYJGYFwlS6M45qwhX5LdlVUsrv3pjJntLwlpw3xsTPCoRJiD9c0JFmdWvyxTcbGfqZNTUZk4ysQJiEqFerOn+9pAsAfx/zJUvXbktwRsaYsqxAmIQ5+9hmXHJ8C3bsLuV3b86i1JqajEkqViBMQt17YUca59Rk6pL1vDRlWaLTMcZEsQJhEqp+dg0e+JHbsvyhDxbYBkPGJBErECbhenc+gvO75lK8aw93vjUrshe5MSbBrECYpPDnizrRsHYNPlu0juFTv010OsYYrECYJNEopyZ/uqgTAH8dNZ/vNm5PcEbGmNAKhIg8LyJrRGRO1LGGIjJWRBZ6XxuUc25vEflSRBaJyJ1h5WiSywVdc/lhp2Zs3VnCXW/NtqYmYxIszCuIF4DeZY7dCYxT1Q7AOO/+fkQkE/gncB7QEegrIh1DzNMkCRHh/h91pn52dSZ+9T2vFy1PdErGpLXQCoSqTgTWlzl8MfCi9/2LwI9inNodWKSqi1V1F/CKd55JA03rZHHvhe7zwP0j57Fq044EZ2RM+pIwL+NFJA8YqaqdvfsbVbV+1OMbVLVBmXMuB3qr6o3e/X5AD1X9ZTmvMQAYAJCbm5s/YsSIuHItLi4mOzs7rnMrO24q5RpPXFXlwc82UrRyJ/m5NbnrlPqIiK+YFWVxUyvXVIubjLkWFBQUqWpBzAdVNbQbkAfMibq/sczjG2KccwXwbNT9fsDgirxefn6+xquwsDDucys7birlGm/clRu3a+d7R2ubgSP1rS++DSRmRVjc1Mo11eImY65AoZbznlrZo5hWi0gugPd1TYznLAdaRd1vCayohNxMEjmiXhb3XOCamu57bx5rtlhTkzGVrbILxHvANd731wDvxnjONKCDiLQVkRrAVd55Js1ckd+S049qwqbtu7nnnTk2qsmYShbmMNfhwOfA0SKyXERuAB4CzhGRhcA53n1EpLmIjAJQ1RLgl8CHwHzgNVWdG1aeJnmJCA9e2oWcmtX4cO5q3p+9MtEpGZNWqoUVWFX7lvPQ2TGeuwLoE3V/FDAqpNRMCmlRvxZ39TmGu9+ewx/fnctJ7RrRKKdmotMyJi3YTGqT9H7SvTUnH9mI9dt2ce97djFpTGWxAmGSnojwt8u6kl0jk5GzVjJ6zqpEp2RMWrACYVJCq4bZDOx9DAB/eGcOW3aWJjgjY6o+KxAmZfTr2YbubRuydutOnp+xOdHpGFPlhdZJbUzQMjKEQZd1pfcTE5n4zQ4KHviIMhOsfdu9ezfVR38UbNAUiyulJfRd/xXXn9qWerWqBxrbpBYrECal5DWuzd19juWed+eyduvOcF5kh8V9YtxChn62hBtPa8d1p+RRJ8sKRTqyAmFSTr+T8mjF93Ts1CXw2LNmzaJr165pHXfUZ18w+lth8uL1PDr2K56btIT+p7Xl2lPaklPT3jLSif1vm5RUp0YGTetmBR63Qa3MtI/bpWlNrj0vn8+/XsdjH33F1CXr+fsYr1Cc3o5rTsqjthWKtGCd1MaYmE46shGvDujJyzf2oKBNAzYU72bQ6C85bdB4nv7ka4p3lSQ6RRMyKxDGmHKJCKe0b8zrPz+J/97QnRNa12f9tl089MECTh80nmcmLmb7rj2JTtOExAqEMeaQRITTOjThzV+czAvXnUi3VvVZu3UXfxk1n9MGjee5SUvYsdsKRVVjBcIYU2EiwplHN+Wdm05m6LUn0rVlPdZu3cn9I+dx+qDxDP3MCkVVYgXCGHPYRIRexzTl3ZtP4dmfFtCpeV3WbNnJn0bM44yHx/Ofz5eys8QKRaqzAmGMiZuI8IOOzRj5q1MZ0i+fY3PrsnrzTv747lzOfHgC/528zApFCrOxasYY30SEczsdwQ+ObcaYeat4/KOFLFi1hXvemcPTE77m5l7taZdhGz6lGisQxpjAZGQIvTvncm7HI/hgziqeGPcVX63eyu/fnk3T7Exu5xsuPaEl1TOt8SIV2P+SMSZwGRnC+V1zGX3L6Qzuezztm+awpngPA9+czdmPfMJrhd9SssdW5E12ViCMMaHJyBAu7NacD289nVt71KNdk9p8s76Y370xi7Mf/YQ3i5ZboUhilV4gRORoEZkRddssIreWec6ZIrIp6jl/rOw8jTHBycwQTmtdi7G3ncFjP+5GXqNslq0r5vbXZ3LOYxN5Z/p37Cm1PopkU+l9EKr6JXAcgIhkAt8Bb8d46qeqekElpmaMCVlmhnDJ8S25sGtz3pmxgn+MW8iStdu49dUZDP54Ib8+uwMXdG1OZkbA67ibuCS6iels4GtVXZbgPIwxlahaZgaX57dk3O1nMOiyrrRsUIuvv9/GLa/MoPfjExk5awWldkWRcIkuEFcBw8t57CQRmSkiH4hIp8pMyhhTOapnZnDlia0Y/9szeejSLrSoX4uFa7byy2HTOe+JTxk1e6UVigQS1cT88EWkBrAC6KSqq8s8VhcoVdWtItIHeEJVO5QTZwAwACA3Nzd/xIgRceVTXFxMdnZ2XOdWdtxUyjWsuKmUa6rFTWSuu0uV8Uu288b8razb7jqv29Srxo875dC9eU0kxhaC9rP1F7egoKBIVQtiPqiqCbkBFwNjKvjcpUDjQz0vPz9f41VYWBj3uZUdN5VyDStuKuWaanGTIdcdu0v0P/9boj3+8pG2GThS2wwcqec9PlHHzF2lpaWlccc9HFX1Z1sWUKjlvKcmsompL+U0L4nIEeJ9VBCR7rimsHWVmJsxJoFqVsuk30l5TLjjTO67sCNN69Rk3srN9P9PIRc9+Rnj5q+OfHg0IUrITGoRyQbOAX4WdeznAKr6NHA58AsRKQG2A1ep/TYYk3ayqmdy7Sltuap7a4ZN+YZ/Tfia2d9t4oYXC+nWsh63nnMUdeytITQJKRCqWgw0KnPs6ajvnwSerOy8jDHJKat6Jtef2pa+3Vvz8pRlPP3J18xcvonrhk4jq5pQfcSHgb/mnj17yHwv2LjVpJSr137Jjae1pX52jUBjh8HWYjLGpIxaNTK58bR2/KRHa/77+TKGTFzMum272FES0vanIcR9cvwiXvzfUq47tS03nNqWerWqB/4aQbECYYxJOdk1qvGzM47kxtPa8dnUQo477rjAX2PGjBmBxx3xSSGjl2fw6cK1/GPcQoZ+toQbTm3L9ae2pW5W8hUKKxDGmJSVmSHUrp4RyptrGHGPaVyDq3+Yz7Sl63ls7Ff87+t1PP7RQp6ftIT+p7Xj2lPyqJNEhSLRE+WMMSbtnJjXkGH9e/LqgJ70bNeQzTtKeGTsV5w2aDz/HL+IrTtDajI7TFYgjDEmQXq0a8QrA05iWP8edM9ryMbi3Tz84Zec9rePeWrC12xLcKGwAmGMMQl28pGNefVnPXnphh7kt2nAhuLd/G30Ak4fNJ4hE79m+67EbNtqBcIYY5KAiHBqh8a88fOT+M/13Tm+dX3WbdvFX0ct4LRBH/Psp4vZsbtyC4UVCGOMSSIiwulHNeGtX5zM0OtOpFvLeqzduosH3p/PaYPGM/SzJZVWKKxAGGNMEhIReh3dlHduPoXnrimgc4u6fL9lJ38aMY8zHh7Pi/9bGnqhsGGuxhiTxESEs49txlnHNOWj+Wt4bOxXzFu5mXvfm8tTE77m5rPa0yEjnOVGrEAYY0wKEBHO6diMs49pyph5q3n8o69YsGoL97wzh8a1Mvh3sw3kt2kQ6GtagTDGmBSSkSH07nwE53Zsxui5q3j8o69YunYrrRsGv8+EFQhjjElBGRlCny659O50BG+Pn0KTOjWDf43AIxpjjKk0GRlCXv1wluewAmGMMSYmKxDGGGNisgJhjDEmJisQxhhjYrICYYwxJiYrEMYYY2KyAmGMMSYmUQ1nDY9EEJHvgWVxnt4YWBtgOmHGTaVcw4qbSrmmWtxUyjXV4iZjrm1UtUmsB6pUgfBDRApVtSAV4qZSrmHFTaVcUy1uKuWaanFTKVewJiZjjDHlsAJhjDEmJisQ+wxJobiplGtYcVMp11SLm0q5plrcVMrV+iCMMcbEZlcQxhhjYrICYYwxJiYrEMYYY2JK+x3lRKQW0FpVv0x0LqbqEBEBrgbaqeqfRaQ1cISqTk1waqaSichvYhzeBBSp6oxKTuewpHUntYhcCPwdqKGqbUXkOODPqnpRYjM7kIi0A54ATgJKgc+B21R1cQCx2wAdVPUjr2BWU9UtfuMGTUQGAQ8A24HRQDfgVlV9Kc54JxzscVX9Ip64XuyncP9PZ6nqsSLSABijqifGG9OLewUwWlW3iMgfgBOAB/zk6sVtAvQH8oj64Kiq18cZbzBQ7puLqv46zrgND/a4qq6PJ26Z1zgV9/cw1Pu55KjqEh/xhgEFwAjv0PnANOAY4HVVHRRn3Fi/v5uAZapaEk/MstL9CuI+oDswAUBVZ4hIXjyBRGQLsf8gxIXWuvGluNcw4J/AJd79q4DhQA8/QUWkPzAAaAgcCbQEngbO9hk31s9jE1AI3B5nYTtXVX8nIpcAy4ErgPFAXAUCeMT7moX7A56J+//qCkwBTo0zLkAPVT1BRKYDqOoGEanhI17EPar6uvcm9kPcB5yn8Pl7ALwLfAp8BOzxGQvc/3MYinC/VxLjMQXa+QkuIvfifheOBoYC1XG/X6f4CNsIOEFVt0a9xhvA6bh/T1wFAvgX7gPCLNzPo7P3fSMR+bmqjvGRM2AFokRVN7nWAH9UtU4A+RyMqOp/o+6/JCK/DCDuzbgiOQVAVReKSNMA4j4KrMAVNsEVtCOAL4HngTPjiBnZeLcPMFxV1/v5v1PVXgAi8gowQFVne/c7A7+NO7CzW0Qy8Yqk90m01GdM2PfmfT7wlKq+KyL3BRA3W1UHBhAHAFV9MahYZeK2DSNulEuA44EvvNdbISJ+/7ZbA7ui7u/GrX+0XUR2+oi7FLhBVecCiEhH4A7gfuAtwAqET3NE5CdApoh0AH4N/C+IwN6bbFbkvqp+4zPkeBG5E3gF96bzY+D9yCW3j0vrnaq6K/JGKyLVOEjTwGHorarRn2qHiMhkrz3+93HGHCEiC3BNTDd5b7o7fGcKx0SKA4CqzvGaG/34B/A20FRE/gJcDvzBZ0yA70Tk38APgL+JSE2CGWwyUkT6qOqoAGLt5f0fDQQ6sv/fw1kBxG4AdCgTd6LPsLtUVUUkUthr+4wH7kPSZBF517t/ITDciz3PR9xjIsUBQFXnicjxqro4iA+9YH0Q2cDdwLm4T7kfAveratxvOiJyEa7pojmwBmgDzFfVTj5zPVgbqKpqXJfWXrv+RuCnwK+Am4B5qnp3PPGi4n4OPIa7lAb3BvkbVe0pIjNU9bg44zYANqvqHu8PrI6qrvKZ63BgG64pQYH/w7U79/UZ9xhcU50A41R1vp94XsxsoDcw27vaywW6xNucENUUKEBtYCfuE24gTaMiMgZ4FXdF9nPgGuB7v1crInIjcAuuSXQG0BP43G/hEZHf4orOOcCDwPXAMFUd7DNuPq7JUoBJquq7CU5EXgXW4z40gvvQ2Bjo572Gr/4uAFTVbgHecO3YjYDp3v1ewJBE53WQfDNwnZOv497M++N9cPAZtx2uU24t8L33fXugFnBqnDFvBupH3W8A3BRArlnAbbhP/G9732cl+v+mnFxviHHsoUTndZB8i7yvs6KOfRJA3Nne/9sM7/4xwKsB5XwO8DCuf+ecRP8MD5JnLeB273f2HVwRzvb+pnOCeI10v4IYQfkdqf/WOK4kIsvuishM4HhVLRWRqaraPc4cLz3Y46r6Vjxxo+JfAoxSVT9toZUi1pWHiExX1eMDiJ0Sw51F5APgJVV92bv/L6Cmqt7gM+4lwMequsm7Xx84U1Xf8Rl3srqrxg9xzW4rgDdU9Uifcaep6okiMgM3IGCnnytTE1u690EsBprgRgOBu0RbDRwFPIO7VDtcG0UkB5gIvCwiawA/Q84uLHM/UtDE+95XgQAuAh4XkYm4S9UPNYAhckEPm/RkiIio96nG6wT2PTLIaxZ82IuV1MOdgUuB90SkFDgPWK+qNwUQ915VfTtyR1U3eqNt3vEZ9wERqYf7pDsYqIu7QvNruVfE3gHGisgGXPHxxftA9jegKe5vLKhRiIETkVNwIzHbsP/fmK+RXPu9RppfQUxU1dNjHRORuRpHv4HXLr4dd5l3NVAPeFlV1/nMNQu4jP3fcFVV/+wnrhe7Ou7N5se4dtKxqnqjz5j/ww2bLCJq2KSqvukj5sO4f//TuOL4c+BbVb3dZ65FwFnAhMjViIjMUtWufuIGqcz4/zq4N8bPgD+C//H/sf69IjJbVbv4iVsZROQM3N/ZaFXddajnHyLWIuBCDaC/KGzegI3bOPBvzNd7TbR0v4JoIiKt1RthJG62a2PvscP+RfM+0b6rqj/ADWkMcqjfO7jO5C/YN3InkOquqru9pgvFtWteDPgqEAQ8bNIzEPgZ8AvcJ7sxwLMBxA1suHOIosf/R76e7918j/8HCkXkUdxcG8UNWCjyGRMReRG4RVU3evcbAI/4uZIUkQxcn0ZnAFX9xG+eUVanQnHwbFLVD8J8gXQvELcDk0Tka9wfXFvc8MnaxPHmrm5kTbGI1Iu05Qaopar2DjgmItIbN0ehF27C4LPAlQGEDnzYpKqW4iaFPRVUTE9ow52DouGP//8VcA9uxFGk+N4cQNyukeIAeycM+uoz8vr1ZkZ/uAtQoTc66B3ciK7Ia/ptyg3DeO+q+i32z9XXrPpoad3EBOCNIz8G90exIJ6O6TLxXsMNuRuLGzoJxL+0QFTcIcBgjRqvHwRxk8ReAT4IsqPaGz4ZyLBJEXlNVa8UkdnEuGry2xQUxnDnsEjspTbuV9XpCU4tJm+wxpmqusG73xA3islX05WIfAycCExl/78zX/1GIjI0xmH12XcWChEZH+OwagBzTPa+hhUI6cyBk3j+4yPeNbGOq8+ZpSIyDzdMdAnuTTfyhps07eRhEZFcVV0pbs2oA6jqsgBfKxOoraqbg4oZpEhfgbilNh7EDcX8ve4/KTGeuE2A3wGdCHBCm4j8FLiLffNhrgD+ovuvChBP3DNiHQ+4uSntpXWB8EZpnIkrEKNwHbWTVPXyROYVS9BvjiIySVVPlQPXTPI1akNEjlHVBVLOQnh+Ln9F5G9l+zViHYsj7jBch/ceXLt7PeBRVX3YT9wwRIb1isiDuMlyw4IY6hvWhDYvdkfcIIDIhEE/s4dDISK/U9VBUs4ig35bAIIkIv+nqi9J7FViUdVHg3qtdO+DuBy3Iuh0Vb1ORJrhs9NT3IznWL9gvjoRg/yU7MU71fsa9BpSv8Et/vdIjMcU90YRr3NwHdXRzotx7HB1VNXNInI17oPCQFyhSLoCQXhLbTRS1edE5BbvU/gnIhL3p3ERqev9TBsCq3DLTUQeaxjAqKvoDzY1cOt0bfMxHDXSMR3WIoNBiiz/Efb6b2lfILZ7HV4lIlIXtzSG39EgBVHfZ+EuqQ+6RHEiich/VbXfoY5VlKoO8L72CiI/L59f4JYAOVJEZkU9VAc31NOv6t5Q3x8BT3qjupL10vpK3FIbf/fmKuTiFmjza7f3daWInI+bU9DSR7xhwAXsG30VERmF5fcD035vjiLyI9yik/HGG+F9DWWRwSCp6r+9r38K+7XSvUAUepNtnsH9Im/FdXrFLcYY5MdFZBLeePUktN9cD3GL9eX7DRpwZ+ow4ANcm/udUce3+P0k6vk3bmXMmcBErzkvKfsgVLUYeEtEmnrDsgEWBBA60AltqnqB9zXs0VeR13tH3GKWvojIUbhmtjz2n3wWWMdvUCScyaj7v0Y690FEE7cPRF1VnXWo5x4iTnTbewbuiuIXqtrNT9ygichdwO9x8x6KI4dx8z+GqOpdPuMH3pkqIkcCy9Utq3Ambt+G/0QPowyKiFTTgDZdCZIcuBhka9zoO1+LQYZFRMap6tmHOhZH3OglaCJ/Z2eo6kk+487ETcQsO/nM95yQoIUxGfWA10jnAiEip8c6rj6WDC4z9KwEN+roEU3SNX5E5EG/xaCcuIF3popbd6cA94npQ+A94GhV7RNAvudz4Age37PUg+a9gZ0FfOT9fHsBfSNNez7itsRdOZyKm+Q5CTfBbXmc8bJwC8eNxw0EicxCrIsbUn2sz3yjh6OW4K4An1HVNT7jFqmq7yvoyiCVsPZUujcxRbfdZuHaMCPLLsTrBi2zW5qIVMpl9uGIjDYCXo814iiAyTZhdKaWqmqJ9+nxcVUdLN6ObX6IyNO4N7NeuEEKl+OzqTFEu1V1nYhkiEiGqo4Xkb8FEHcorinvCu/+/3nHzokz3s+AW3FXOkXsKxCbcbO1fVHV6/zGiCb7ljIZISI34VZIjZ58FkRTZtBC2cMjWlpfQZQlIq2AQepjHwAR+UJVTyhzLOk+lYjIEFUdENZkGwl43wIv5hTgcdyktgtVdYmIzFFvyQUfcSPNYZGvOcBbqnqun7hhEJGPcJ3pD+KWhVkDFKiqny0xy1sp1/cnVBH5lfrcS6GcuEfhZtQ3U9XOItIVuEhVH4gzXmT0YcytTP2OQgyDBDgZtTzpfgVR1nLcvq6HTdzmMJ2AemXaR+sS1WyRLMIYbVRGLvB+2f4CnzGvw43R/4tXHNoS/37U0bZ7X4tFpDmwDrfsSjKaieszuo19i0HmBBB3rYj8H/tWNu6L+zn4tUpE6pQZrPBAAFeoz+BaACIjemZ581niKhCV1ZkeFHHrUfVW1SBG8ZUrrQtEmUkxGcBxuD/AeByNG9ZXn/2X6N6CG2mQtETkZA4cCeH3zfxNoEBE2gPP4foLhuH2k46LN8Hq11H3lwAP+cwT3KV6fdzm8ZHOyCAWAQxDL3VrUu1dDLLM0N94XQ88idsFUHFrUQUxGuYeVX3dG6zwQ9xghacAXzO/cYtBTpX9F1gMYpn6m3GrL2/07jfA9fH8y2/sIHnD8/8O+OqUP5S0LhDsPymmBBgeb0VW1XeBd0XkJFX9PJDsKoGI/Bc4ErdtY2QkhOL/035g/QUS8lpMuDetXwCnAZ/jRoYEvSCgL2HOBRG3vMhfNZz9LyK/U+cDT6nquyJyXwBx13qj2iJ7g1wOrAwgbn9V3dtHom5xwf5AUhUIzxgRuQzXHBpKX4H1QQTMG71xAweOiEm6xb4ARGQ+biZxoL8IQfYXSMhrMYlbYHEL+5qr+uK2Ng1iVdtAeHMUGhDSXBBxO75dqD73U4gRdyTwHW6wQj6uOW+q32HfItIOGAKcDGzAjRa8OoDfhVlAt8jfg1c8ZyXjMOKoPogS3BYAgfdBpHWBKOcTaWTL0Qc0jo03ROR13MSlnwB/xrUTz1fVW3ymGwov31+rahCfvqLjdsT1F3yuqsO9/oIfq2oQTUKBEpGZZd+wYh2ryrwRZyfgmgKjV0f1ta5PGIMVvLg1caPN8nArFWwmgA20JKRNqVJVuheIQbhL4Mg6MVd5XzcDp6pq2e0+KxIzMv4/MiKmOm4bz6SbiQl7520chxvWGT2sz3dzgwS8z7MEv/5OJO4LwNOqOtm73wO4RoPZyjMliFu48gAa53IOsv9aTLHi+l2LaTT7NtCKniQWaw2ww4mbgRuiezbs25RKVfcc9MQE8fpIOrB/a0Xc87gOiJ/mBeKzssMDI8ckzu0WRWSqqnYXt8fzTbiFyqYm4zA5ILRlk0XkQlzbfg1VbSsh7PMs3vo7qvr7OM+PXEFWxw0y+Ma73waY53f4bCoStyaZquoWn3FGquoF5Qwf9T1sNIjhzalORG4EbsGtmTUDtw/N50F+GE33TuocEemhqlMARKQ7+4YMxjsiYohX1f+Au1zPwe3UlZT8FoKDuA838XCC9zozJOAJg+p//Z0LAksmxYlIAW5iXB3v/ibgeo1ziQkNfy2m/4lIFw1+A60OuH6esnvEJOMHvFtwmyZNVtVe3lD7QBfwS/cCcSPwvDcxSnBNSzeK23L0wThj/he4DNeOGVkZspnPPEMjB+4HAfv6YW7XMrPCD0OsfZ59Xa5K7PV34o7pt0OzinkeuElVPwXwhqUOxc1fiZuEtBYTbkmQa70rlCA30BoK3Isb7tsLN/cmWTcr36GqO0QEEampbh+Wo4N8gbQuEKo6DejijRAR3X/Rt9fiDPsu7g22iKg2/ST2KG5p52G4P4SrgCOAL3FvGmfGGTeMfZ6j+4Qi6+9c7DOmcbZEigOAqk7yPjzERfatxdTYu6KOXoupua9MnfMCiBFLLVUdJyLifYC4T0Q+xRWNZLPcm7/zDjBWRDbg/pYDk+59EDXZ92k/epJY3CMhUq1tVESmaJkVVkVksqr29DOSR/bf5xnc4noPaBLu82xARB7DvaEPx12V/Rg3fPRNOPy1uUTkFvatxfQd+6/F9IyqPhlI4gETkc9w82HeAD7G5f6Qqgb6yTxoXl9iPdwS+4ENVU73AjGafZ/2AxkJISJDgMFBt42GRUQ+x11OR/YMvhz4jVcgQl8t8nCIyD9iHN4EFHoTFU2cJPaaXBEab8enhLQWU1hE5ETc7nL1gftxVzyDIv2UycZrCuygqkPF7Q+Ro26FgWDip3mBCPzTvojMA9rjJu4E2TYaCm/C0RPsm7L/OW6dn++AfFWdFGfcscAVuv+SBa+o6g995DoEOAZ43Tt0GTAXaAUsVtVb441twiMinTmw09fvTP1QeJ31d+NGslX3Difl3683NLkAt+T9UeLWEXu97MhMP9K6D4JwRkKE1TYaCq8Turz5HnEVB0/j6D4ddUsWNPURD1zhPUu9jXxE5CncOPVzgJS4YktWItII185+Kq6JaRJuWLKvBfu8N7EzcQViFO7vYxL+l3IJy8u4RQBn49a7SmaXAMfj5oKgqitEJNB9qtO9QAQ+EiLVRsbIvo1iTmHfG8MtGudGMVFKRaS1qn7jvU4bfI5iAlrglhbY5N2vDTRX1T0ikgoDApLZK8BE3FUZuBUAXsUtkeHH5UA3YLqqXicizUjehRABvlfV9xKdRAXtUlUVb/90b/RloNK9QKTUp/2QBL1RTMTdwCQRicyzOB3wtesZbrXVGSIyAVfMTwf+6v1hfOQzdrprqKr3R91/wJuI6Nd2dSuPlniT8NYAyTinIOJeEXkWGMf+Kwu8lbiUyvWat0RKfXELCl6PWwY9MGndBxHhNX1Et49+k8B0KlWsjuigOqdFpDFudie4yTxrA4iZi5uAJ7gZ6oEO60tX4paOLmTf8O7LgU6q6mt4p4j8C7f3+VXA7cBWYIYGvCNcUETkJVw/11z2NTGpJu9im+fgRgoKbkmfsYHGT+cCIQduAN8Gt7Be0q3cGBZxO5S9wP4bxVwXwESmyM83su/3BFUdGUDMFrj/p+hhyYGtPZOuZN/KoJHRfJnsW7RPNYAVQkUkD6irqkHsXxGKeJfYqarSvYnpftwn3P02gE9wTpUt1kYxvj/dichDuGUAXvYO3SIip6jqXT5i/g03Pn+/T3e4tnPjg6rWEbewXtmF3/yuybV31rSqLi17LAlNFpGO6janSkrlrH4Attx3sESkUFULRGQmcLzXVjpVVbsnOrfKIiIvAreq6gbvfkPg734vqcWtq3+cut3PIuvqT/czAEBEvgS6qqp1SAesnIXf/hfvG3nUTOrxuFFM0TOpP1DVY32mHApx+6McSYoMUw9bul9BbPTWYZoIvCwiawhg28IU0zVSHMAtwywixwcUuz4QWda5XgDxFuPGpluBCF7QC7/9jH0zqYvw3mhxGzMl5SxqT+9EJ5BM0r1AXIzbiSl6A3hfG46koAwRaVDmCiKI34u/AtO9GbqREUdxNy95inGjmMqOMPl1+aeYCgp04TdVfQJ4QkT+iNtydrOI3IPblChpt+RNtWHqYUvrAqGq26LuvljuE6u2R3ATBt/AfcK7EviLn4DiNl0pxTVTnIgrEANVdZXPXN/zbiZ4YS38drmq/tlbEuIc3O/bU0CPg59mkkFa9kFUZidPKhC3PehZuH//uCA66ERkoqqefuhnmmQT5MJvsm+HxQdx244OixwLJFkTqrQsECZ8XnPCdtxs3Og9jg97q0kReU1Vr5TYe4iTrh2IqUBERuLW9foBkI/7nZga7yrBpnJZgTChkH1bTe5H49iZS0RyVXWlt1zHAazdOHl5y773xl09LPQmOnZR1TEJTs1UgBUIEwoRqYXbkzuy+NunwNOquj2hiRljKswKhAmFiLyG2xwmMlGuL1BfVa+MI5b1GRmTAFYgTChi7UbnZ4c6Y0zly0h0AqbKmi4ikYX6EJEewGcJzMcYc5jsCsKEwluy4GggsjJua9xWjqWk8dIFxqQSKxAmFOWNOIqwkUfGJD8rEMYYY2KyPghjjDExWYEwxhgTkxUIY2IQkbtFZK6IzBKRGd4orLBea4KIFIQV35h4pfVqrsbEIiInARcAJ6jqTm9v7RoJTsuYSmdXEMYcKBdYG9m5TlXXquoKEfmjiEwTkTkiMkREBPZeATwmIhNFZL6InCgib4nIQhF5wHtOnogsEJEXvauSN7x1ivYjIueKyOci8oWIvO5taIWIPCQi87xz/16JPwuTxqxAGHOgMUArEflKRP7lLX8N8KSqnqiqnYFauKuMiF3e8uZPA+8CNwOdgWtFpJH3nKOBId4ckM24tar28q5U/gD8QFVPAAqB33ibOF0CdPLOfSCEf7MxB7ACYUwZqroVtzT1AOB74FURuRboJSJTvGXHzwI6RZ0W2choNjBXVVd6VyCLgVbeY9+qamQ2+Uu4hQyj9QQ6Ap+JyAzgGqANrpjsAJ4VkUtxO+sZEzrrgzAmBlXdA0wAJngF4WdAV6BAVb8VkfuArKhTIluglrL/ntml7Ps7KzvpqOx9Acaqat+y+YhId+Bs4Crgl7gCZUyo7ArCmDJE5GgR6RB16DjgS+/7tV6/wOVxhG7tdYCDW912UpnHJwOniEh7L49sETnKe716qjoKuNXLx5jQ2RWEMQfKAQZ7ezSXAItwzU0bcU1IS4FpccSdD1wjIv8GFuL2Zt5LVb/3mrKGi0hN7/AfgC3AuyKShbvKuC2O1zbmsNlSG8ZUAhHJA0Z6HdzGpARrYjLGGBOTXUEYY4yJya4gjDHGxGQFwhhjTExWIIwxxsRkBcIYY0xMViCMMcbE9P8BS2iDrX5pIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist = nltk.FreqDist(stopwords_tokens)\n",
    "print(freq_dist)\n",
    "k = 15\n",
    "print(freq_dist.most_common(k))                       #plot the top 15 words in the text \n",
    "freq_dist.plot(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FreqDist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: 24\n",
      "natural: 16\n",
      "nlp: 15\n",
      "cognitive: 13\n",
      "processing: 12\n",
      "linguistics: 9\n",
      "based: 9\n",
      "e: 9\n",
      "tasks: 9\n",
      "approach: 9\n",
      "statistical: 8\n",
      "neural: 7\n",
      "machine: 7\n",
      "g: 7\n",
      "learning: 6\n"
     ]
    }
   ],
   "source": [
    "for word, frequency in freq_dist.most_common(k):                  #Calculate frequency of each word using freq Dist\n",
    "    print(u'{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist.B()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WEIGHTED FREQUENCY USING FreqDist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_weighted_frequency(freq_dist) -> dict:\n",
    "    #get the word frequency from FreqDist\n",
    "    word_freqs={}\n",
    "    for word in freq_dist.keys():\n",
    "        word_freqs[word] = freq_dist.get(word)\n",
    "    #find the maximum of the frequency\n",
    "    max_freq = freq_dist.most_common(1) [0] [1]\n",
    "    #find the total number of words in freq_dist                            #Weighted freq \n",
    "    total_num_words=freq_dist.B();\n",
    "    \n",
    "    #calculate weighted frequency of occurance\n",
    "    for word in word_freqs.keys():\n",
    "        word_freqs[word] =(word_freqs[word]/total_num_words)\n",
    "        print('%s' %word, ':' , word_freqs[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural : 0.03836930455635491\n",
      "language : 0.05755395683453238\n",
      "processing : 0.02877697841726619\n",
      "nlp : 0.03597122302158273\n",
      "interdisciplinary : 0.007194244604316547\n",
      "subfield : 0.002398081534772182\n",
      "computer : 0.007194244604316547\n",
      "science : 0.007194244604316547\n",
      "linguistics : 0.02158273381294964\n",
      "primarily : 0.002398081534772182\n",
      "concerned : 0.002398081534772182\n",
      "giving : 0.002398081534772182\n",
      "computers : 0.002398081534772182\n",
      "ability : 0.002398081534772182\n",
      "support : 0.002398081534772182\n",
      "manipulate : 0.002398081534772182\n",
      "human : 0.002398081534772182\n",
      "involves : 0.004796163069544364\n",
      "datasets : 0.002398081534772182\n",
      "text : 0.004796163069544364\n",
      "corpora : 0.004796163069544364\n",
      "speech : 0.009592326139088728\n",
      "using : 0.007194244604316547\n",
      "either : 0.002398081534772182\n",
      "rule : 0.011990407673860911\n",
      "based : 0.02158273381294964\n",
      "probabilistic : 0.002398081534772182\n",
      "e : 0.02158273381294964\n",
      "statistical : 0.019184652278177457\n",
      "recently : 0.004796163069544364\n",
      "neural : 0.016786570743405275\n",
      "network : 0.007194244604316547\n",
      "machine : 0.016786570743405275\n",
      "learning : 0.014388489208633094\n",
      "approaches : 0.014388489208633094\n",
      "goal : 0.002398081534772182\n",
      "capable : 0.002398081534772182\n",
      "understanding : 0.009592326139088728\n",
      "contents : 0.002398081534772182\n",
      "documents : 0.007194244604316547\n",
      "including : 0.002398081534772182\n",
      "contextual : 0.002398081534772182\n",
      "nuances : 0.002398081534772182\n",
      "within : 0.002398081534772182\n",
      "technology : 0.004796163069544364\n",
      "accurately : 0.002398081534772182\n",
      "extract : 0.002398081534772182\n",
      "information : 0.002398081534772182\n",
      "insights : 0.002398081534772182\n",
      "contained : 0.002398081534772182\n",
      "well : 0.004796163069544364\n",
      "categorize : 0.002398081534772182\n",
      "organize : 0.002398081534772182\n",
      "challenges : 0.002398081534772182\n",
      "frequently : 0.004796163069544364\n",
      "involve : 0.004796163069544364\n",
      "recognition : 0.002398081534772182\n",
      "generation : 0.004796163069544364\n",
      "roots : 0.002398081534772182\n",
      "already : 0.002398081534772182\n",
      "alan : 0.002398081534772182\n",
      "turing : 0.004796163069544364\n",
      "published : 0.002398081534772182\n",
      "article : 0.002398081534772182\n",
      "titled : 0.002398081534772182\n",
      "computing : 0.002398081534772182\n",
      "machinery : 0.002398081534772182\n",
      "intelligence : 0.009592326139088728\n",
      "proposed : 0.004796163069544364\n",
      "called : 0.002398081534772182\n",
      "test : 0.004796163069544364\n",
      "criterion : 0.002398081534772182\n",
      "though : 0.004796163069544364\n",
      "time : 0.004796163069544364\n",
      "articulated : 0.002398081534772182\n",
      "problem : 0.002398081534772182\n",
      "separate : 0.002398081534772182\n",
      "artificial : 0.004796163069544364\n",
      "includes : 0.002398081534772182\n",
      "task : 0.002398081534772182\n",
      "automated : 0.002398081534772182\n",
      "interpretation : 0.002398081534772182\n",
      "premise : 0.002398081534772182\n",
      "symbolic : 0.009592326139088728\n",
      "summarized : 0.002398081534772182\n",
      "john : 0.002398081534772182\n",
      "searle : 0.002398081534772182\n",
      "chinese : 0.004796163069544364\n",
      "room : 0.002398081534772182\n",
      "experiment : 0.002398081534772182\n",
      "given : 0.004796163069544364\n",
      "collection : 0.002398081534772182\n",
      "rules : 0.014388489208633094\n",
      "g : 0.016786570743405275\n",
      "phrasebook : 0.002398081534772182\n",
      "questions : 0.002398081534772182\n",
      "matching : 0.002398081534772182\n",
      "answers : 0.002398081534772182\n",
      "emulates : 0.002398081534772182\n",
      "tasks : 0.02158273381294964\n",
      "applying : 0.002398081534772182\n",
      "data : 0.002398081534772182\n",
      "confronts : 0.002398081534772182\n",
      "systems : 0.007194244604316547\n",
      "complex : 0.002398081534772182\n",
      "sets : 0.002398081534772182\n",
      "hand : 0.007194244604316547\n",
      "written : 0.002398081534772182\n",
      "starting : 0.002398081534772182\n",
      "late : 0.004796163069544364\n",
      "however : 0.004796163069544364\n",
      "revolution : 0.002398081534772182\n",
      "introduction : 0.004796163069544364\n",
      "algorithms : 0.004796163069544364\n",
      "due : 0.004796163069544364\n",
      "steady : 0.002398081534772182\n",
      "increase : 0.002398081534772182\n",
      "computational : 0.007194244604316547\n",
      "power : 0.002398081534772182\n",
      "see : 0.004796163069544364\n",
      "moore : 0.002398081534772182\n",
      "law : 0.002398081534772182\n",
      "gradual : 0.002398081534772182\n",
      "lessening : 0.002398081534772182\n",
      "dominance : 0.002398081534772182\n",
      "chomskyan : 0.002398081534772182\n",
      "theories : 0.002398081534772182\n",
      "transformational : 0.002398081534772182\n",
      "grammar : 0.009592326139088728\n",
      "whose : 0.002398081534772182\n",
      "theoretical : 0.002398081534772182\n",
      "underpinnings : 0.002398081534772182\n",
      "discouraged : 0.002398081534772182\n",
      "sort : 0.002398081534772182\n",
      "corpus : 0.002398081534772182\n",
      "underlies : 0.002398081534772182\n",
      "approach : 0.02158273381294964\n",
      "word : 0.007194244604316547\n",
      "n : 0.002398081534772182\n",
      "gram : 0.002398081534772182\n",
      "model : 0.004796163069544364\n",
      "best : 0.002398081534772182\n",
      "algorithm : 0.002398081534772182\n",
      "overperformed : 0.002398081534772182\n",
      "multi : 0.002398081534772182\n",
      "layer : 0.007194244604316547\n",
      "perceptron : 0.002398081534772182\n",
      "single : 0.004796163069544364\n",
      "hidden : 0.009592326139088728\n",
      "context : 0.004796163069544364\n",
      "length : 0.002398081534772182\n",
      "several : 0.002398081534772182\n",
      "words : 0.007194244604316547\n",
      "trained : 0.002398081534772182\n",
      "million : 0.002398081534772182\n",
      "cpu : 0.002398081534772182\n",
      "cluster : 0.002398081534772182\n",
      "modelling : 0.004796163069544364\n",
      "yoshua : 0.002398081534772182\n",
      "bengio : 0.002398081534772182\n",
      "co : 0.004796163069544364\n",
      "authors : 0.004796163069544364\n",
      "tomáš : 0.002398081534772182\n",
      "mikolov : 0.002398081534772182\n",
      "phd : 0.002398081534772182\n",
      "student : 0.002398081534772182\n",
      "brno : 0.002398081534772182\n",
      "university : 0.002398081534772182\n",
      "applied : 0.004796163069544364\n",
      "simple : 0.002398081534772182\n",
      "recurrent : 0.002398081534772182\n",
      "following : 0.004796163069544364\n",
      "years : 0.002398081534772182\n",
      "went : 0.002398081534772182\n",
      "develop : 0.004796163069544364\n",
      "wordvec : 0.002398081534772182\n",
      "representation : 0.002398081534772182\n",
      "deep : 0.002398081534772182\n",
      "style : 0.002398081534772182\n",
      "featuring : 0.002398081534772182\n",
      "many : 0.007194244604316547\n",
      "layers : 0.002398081534772182\n",
      "methods : 0.004796163069544364\n",
      "became : 0.002398081534772182\n",
      "widespread : 0.002398081534772182\n",
      "popularity : 0.002398081534772182\n",
      "partly : 0.002398081534772182\n",
      "flurry : 0.002398081534772182\n",
      "results : 0.004796163069544364\n",
      "showing : 0.002398081534772182\n",
      "techniques : 0.002398081534772182\n",
      "achieve : 0.004796163069544364\n",
      "state : 0.002398081534772182\n",
      "art : 0.002398081534772182\n",
      "modeling : 0.002398081534772182\n",
      "parsing : 0.004796163069544364\n",
      "increasingly : 0.004796163069544364\n",
      "important : 0.002398081534772182\n",
      "medicine : 0.002398081534772182\n",
      "healthcare : 0.002398081534772182\n",
      "helps : 0.002398081534772182\n",
      "analyze : 0.002398081534772182\n",
      "notes : 0.002398081534772182\n",
      "electronic : 0.002398081534772182\n",
      "health : 0.002398081534772182\n",
      "records : 0.002398081534772182\n",
      "would : 0.002398081534772182\n",
      "otherwise : 0.002398081534772182\n",
      "inaccessible : 0.002398081534772182\n",
      "study : 0.004796163069544364\n",
      "seeking : 0.002398081534772182\n",
      "improve : 0.002398081534772182\n",
      "care : 0.002398081534772182\n",
      "protect : 0.002398081534772182\n",
      "patient : 0.002398081534772182\n",
      "privacy : 0.002398081534772182\n",
      "coding : 0.002398081534772182\n",
      "set : 0.002398081534772182\n",
      "manipulating : 0.004796163069544364\n",
      "symbols : 0.004796163069544364\n",
      "coupled : 0.002398081534772182\n",
      "dictionary : 0.002398081534772182\n",
      "lookup : 0.002398081534772182\n",
      "historically : 0.002398081534772182\n",
      "first : 0.002398081534772182\n",
      "used : 0.007194244604316547\n",
      "ai : 0.007194244604316547\n",
      "general : 0.002398081534772182\n",
      "particular : 0.002398081534772182\n",
      "writing : 0.002398081534772182\n",
      "grammars : 0.002398081534772182\n",
      "devising : 0.002398081534772182\n",
      "heuristic : 0.002398081534772182\n",
      "stemming : 0.002398081534772182\n",
      "include : 0.002398081534772182\n",
      "networks : 0.004796163069544364\n",
      "advantages : 0.002398081534772182\n",
      "although : 0.004796163069544364\n",
      "still : 0.004796163069544364\n",
      "use : 0.002398081534772182\n",
      "become : 0.002398081534772182\n",
      "mostly : 0.002398081534772182\n",
      "obsolete : 0.004796163069544364\n",
      "advance : 0.002398081534772182\n",
      "llms : 0.002398081534772182\n",
      "commonly : 0.007194244604316547\n",
      "mid : 0.002398081534772182\n",
      "ended : 0.002398081534772182\n",
      "period : 0.002398081534772182\n",
      "winter : 0.002398081534772182\n",
      "caused : 0.002398081534772182\n",
      "inefficiencies : 0.002398081534772182\n",
      "earliest : 0.002398081534772182\n",
      "decision : 0.002398081534772182\n",
      "trees : 0.002398081534772182\n",
      "producing : 0.002398081534772182\n",
      "hard : 0.002398081534772182\n",
      "similar : 0.002398081534772182\n",
      "old : 0.004796163069544364\n",
      "markov : 0.002398081534772182\n",
      "models : 0.007194244604316547\n",
      "part : 0.007194244604316547\n",
      "tagging : 0.004796163069544364\n",
      "announced : 0.002398081534772182\n",
      "end : 0.002398081534772182\n",
      "major : 0.004796163069544364\n",
      "drawback : 0.002398081534772182\n",
      "require : 0.002398081534772182\n",
      "elaborate : 0.002398081534772182\n",
      "feature : 0.002398081534772182\n",
      "engineering : 0.002398081534772182\n",
      "since : 0.004796163069544364\n",
      "replaced : 0.002398081534772182\n",
      "embeddings : 0.002398081534772182\n",
      "capture : 0.002398081534772182\n",
      "semantic : 0.002398081534772182\n",
      "properties : 0.002398081534772182\n",
      "intermediate : 0.004796163069544364\n",
      "dependency : 0.002398081534772182\n",
      "needed : 0.002398081534772182\n",
      "anymore : 0.002398081534772182\n",
      "translation : 0.004796163069544364\n",
      "newly : 0.002398081534772182\n",
      "invented : 0.002398081534772182\n",
      "sequence : 0.004796163069544364\n",
      "transformations : 0.002398081534772182\n",
      "made : 0.004796163069544364\n",
      "steps : 0.002398081534772182\n",
      "alignment : 0.002398081534772182\n",
      "previously : 0.002398081534772182\n",
      "necessary : 0.002398081534772182\n",
      "list : 0.002398081534772182\n",
      "researched : 0.002398081534772182\n",
      "direct : 0.002398081534772182\n",
      "real : 0.002398081534772182\n",
      "world : 0.002398081534772182\n",
      "applications : 0.004796163069544364\n",
      "others : 0.002398081534772182\n",
      "serve : 0.002398081534772182\n",
      "subtasks : 0.002398081534772182\n",
      "aid : 0.002398081534772182\n",
      "solving : 0.002398081534772182\n",
      "larger : 0.002398081534772182\n",
      "closely : 0.002398081534772182\n",
      "intertwined : 0.002398081534772182\n",
      "subdivided : 0.002398081534772182\n",
      "categories : 0.002398081534772182\n",
      "convenience : 0.002398081534772182\n",
      "coarse : 0.002398081534772182\n",
      "division : 0.002398081534772182\n",
      "long : 0.004796163069544364\n",
      "standing : 0.004796163069544364\n",
      "trends : 0.007194244604316547\n",
      "field : 0.002398081534772182\n",
      "possible : 0.002398081534772182\n",
      "extrapolate : 0.002398081534772182\n",
      "future : 0.002398081534772182\n",
      "directions : 0.002398081534772182\n",
      "three : 0.002398081534772182\n",
      "among : 0.004796163069544364\n",
      "topics : 0.002398081534772182\n",
      "series : 0.002398081534772182\n",
      "conll : 0.004796163069544364\n",
      "shared : 0.004796163069544364\n",
      "observed : 0.002398081534772182\n",
      "higher : 0.002398081534772182\n",
      "level : 0.002398081534772182\n",
      "aspects : 0.007194244604316547\n",
      "emulate : 0.002398081534772182\n",
      "intelligent : 0.002398081534772182\n",
      "behaviour : 0.004796163069544364\n",
      "apparent : 0.002398081534772182\n",
      "comprehension : 0.002398081534772182\n",
      "broadly : 0.002398081534772182\n",
      "speaking : 0.002398081534772182\n",
      "technical : 0.002398081534772182\n",
      "operationalization : 0.002398081534772182\n",
      "advanced : 0.002398081534772182\n",
      "cognitive : 0.03117505995203837\n",
      "represents : 0.002398081534772182\n",
      "one : 0.002398081534772182\n",
      "developmental : 0.002398081534772182\n",
      "trajectories : 0.002398081534772182\n",
      "cognition : 0.002398081534772182\n",
      "refers : 0.002398081534772182\n",
      "mental : 0.002398081534772182\n",
      "action : 0.002398081534772182\n",
      "process : 0.002398081534772182\n",
      "acquiring : 0.002398081534772182\n",
      "knowledge : 0.004796163069544364\n",
      "thought : 0.002398081534772182\n",
      "experience : 0.002398081534772182\n",
      "senses : 0.002398081534772182\n",
      "scientific : 0.002398081534772182\n",
      "mind : 0.002398081534772182\n",
      "processes : 0.002398081534772182\n",
      "branch : 0.002398081534772182\n",
      "combining : 0.002398081534772182\n",
      "research : 0.002398081534772182\n",
      "psychology : 0.002398081534772182\n",
      "especially : 0.002398081534772182\n",
      "age : 0.002398081534772182\n",
      "area : 0.002398081534772182\n",
      "maintained : 0.002398081534772182\n",
      "strong : 0.002398081534772182\n",
      "ties : 0.004796163069544364\n",
      "studies : 0.002398081534772182\n",
      "example : 0.002398081534772182\n",
      "george : 0.002398081534772182\n",
      "lakoff : 0.002398081534772182\n",
      "offers : 0.002398081534772182\n",
      "methodology : 0.002398081534772182\n",
      "build : 0.002398081534772182\n",
      "perspective : 0.002398081534772182\n",
      "along : 0.002398081534772182\n",
      "findings : 0.002398081534772182\n",
      "two : 0.002398081534772182\n",
      "defining : 0.002398081534772182\n",
      "historical : 0.002398081534772182\n",
      "heritage : 0.002398081534772182\n",
      "less : 0.002398081534772182\n",
      "addressed : 0.002398081534772182\n",
      "turn : 0.002398081534772182\n",
      "nevertheless : 0.002398081534772182\n",
      "towards : 0.002398081534772182\n",
      "technically : 0.002398081534772182\n",
      "operationalizable : 0.002398081534772182\n",
      "frameworks : 0.004796163069544364\n",
      "pursued : 0.002398081534772182\n",
      "various : 0.002398081534772182\n",
      "functional : 0.002398081534772182\n",
      "construction : 0.002398081534772182\n",
      "psycholinguistics : 0.002398081534772182\n",
      "neuroscience : 0.002398081534772182\n",
      "act : 0.002398081534772182\n",
      "r : 0.002398081534772182\n",
      "limited : 0.002398081534772182\n",
      "uptake : 0.002398081534772182\n",
      "mainstream : 0.002398081534772182\n",
      "measured : 0.002398081534772182\n",
      "presence : 0.002398081534772182\n",
      "conferences : 0.002398081534772182\n",
      "acl : 0.002398081534772182\n",
      "ideas : 0.004796163069544364\n",
      "revived : 0.002398081534772182\n",
      "explainability : 0.002398081534772182\n",
      "notion : 0.002398081534772182\n",
      "likewise : 0.002398081534772182\n",
      "inherent : 0.002398081534772182\n",
      "multimodal : 0.002398081534772182\n",
      "rarely : 0.002398081534772182\n",
      "explicit : 0.002398081534772182\n",
      "developments : 0.002398081534772182\n",
      "specifically : 0.002398081534772182\n",
      "tools : 0.002398081534772182\n",
      "technologies : 0.002398081534772182\n",
      "large : 0.002398081534772182\n"
     ]
    }
   ],
   "source": [
    "_find_weighted_frequency(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str= mytext\n",
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to make a frequency table to store frequencies of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frequency_table(txt_str) -> dict:\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(txt_str)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freq_table = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to create a matrix to store frequencies of the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frequency_matrix(sentences):\n",
    "    freq_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        freq_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return freq_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to perform TF and form a matrix of it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to form a bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define  a fucntion to perform IDF and form a matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to perform TF-IDF and store in matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),f_table2.items()):  \n",
    "                                                                            # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to Score the Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(tf_idf_matrix) -> dict:\n",
    "    \n",
    "\n",
    "    sentence_value = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentence_value[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentence_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to compute the average scores of the sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_average_score(sentence_value) -> int:\n",
    "    \n",
    "    sum_values = 0\n",
    "    for entry in sentence_value:\n",
    "        sum_values += sentence_value[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sum_values / len(sentence_value))\n",
    "\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define a function to form a summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_summary(sentences, sentence_value, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentence_value and sentence_value[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calling all the  functions to perform sentence scoring, TF-IDF and summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summarization(text):\n",
    "    \n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    total_documents = len(sentences)                                            #Sentence Tokenize\n",
    "    print(sentences)\n",
    "    \n",
    "    freq_matrix = make_frequency_matrix(sentences)                #Create the Frequency matrix of the words in each sentence.\n",
    "    print(freq_matrix)\n",
    "\n",
    "   \n",
    "    \n",
    "    tf_matrix = make_tf_matrix(freq_matrix)                              #Calculate the term frequency and generate a matrix\n",
    "    print(tf_matrix)\n",
    "\n",
    "    \n",
    "    count_doc_per_words = make_documents_per_words(freq_matrix)            #create a table to store the documents per words\n",
    "    print(count_doc_per_words)\n",
    "\n",
    "    \n",
    "    \n",
    "    idf_matrix = make_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    print(idf_matrix)                                                             #Calculate the IDF and generate a matrix\n",
    "\n",
    "    \n",
    "    tf_idf_matrix = make_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    print(tf_idf_matrix)                                                        #Calculate the TF-IDF and generate a matrix\n",
    "\n",
    "    \n",
    "    sentence_scores = score_sentences(tf_idf_matrix)\n",
    "    print(sentence_scores)                                                  #call the function and score the sentences\n",
    "\n",
    "    \n",
    "    threshold = cal_average_score(sentence_scores)\n",
    "    print(threshold)                                                           #Find the threshold\n",
    "\n",
    "    \n",
    "    summary = form_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "    return summary                                                           #call the function and generate the summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRINT FINAL RESULT WITH SENTENCE SCORE, TF-IDF MATRIX AND SUMMARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics.', 'It is primarily concerned with giving computers the ability to support and manipulate human language.', 'It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e.', 'statistical and, most recently, neural network-based) machine learning approaches.', 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.', 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.', 'Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.', 'Natural language processing has its roots in the 1950s.', 'Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.', 'The proposed test includes a task that involves the automated interpretation and generation of natural language.', \"The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\", 'Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.', 'Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.', \"This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g.\", 'transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.', '[7]\\nIn 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.', '[8]\\nIn 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.', 'In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.', 'That popularity was due partly to a flurry of results showing that such techniques[10][11] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[12] and parsing.', '[13][14] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[15] or protect patient privacy.', '[16]\\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[17][18] such as by writing grammars or devising heuristic rules for stemming.', 'Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \\nAlthough rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023.', 'Before that they were commonly used:\\nIn the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.', '[19][20]\\nThe earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.', 'Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.', 'A major drawback of statistical methods is that they require elaborate feature engineering.', 'Since 2015,[21] the statistical approach was replaced by neural networks approach, using word embeddings to capture semantic properties of words.', 'Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore.', 'Neural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.', 'The following is a list of some of the most commonly researched tasks in natural language processing.', 'Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.', 'Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience.', 'A coarse division is given below.', 'Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP.', 'As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[43]\\nMost higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language.', 'More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).', 'Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.', '\"[44] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.', '[45] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.', '[46] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.', 'As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[47] with two defining aspects:\\nTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s.', 'Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[49] functional grammar,[50] construction grammar,[51] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[52] of the ACL).', 'More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".', '[53] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[54] and developments in Artificial intelligence, specifically tools and technologies using Large language model approaches.', '[55]']\n",
      "{'Natural languag': {'natur': 1, 'languag': 1, 'process': 1, 'ha': 1, 'root': 1, '1950': 1, '.': 1}, 'It is primarily': {'primarili': 1, 'concern': 1, 'give': 1, 'comput': 1, 'abil': 1, 'support': 1, 'manipul': 1, 'human': 1, 'languag': 1, '.': 1}, 'It involves pro': {'involv': 1, 'process': 1, 'natur': 1, 'languag': 1, 'dataset': 1, ',': 2, 'text': 1, 'corpora': 2, 'speech': 1, 'use': 1, 'either': 1, 'rule-bas': 1, 'probabilist': 1, '(': 1, 'i.e': 1, '.': 1}, 'statistical and': {'statist': 1, ',': 2, 'recent': 1, 'neural': 1, 'network-bas': 1, ')': 1, 'machin': 1, 'learn': 1, 'approach': 1, '.': 1}, 'The goal is a c': {'goal': 1, 'comput': 1, 'capabl': 1, '``': 1, 'understand': 1, \"''\": 1, 'content': 1, 'document': 1, ',': 1, 'includ': 1, 'contextu': 1, 'nuanc': 1, 'languag': 1, 'within': 1, '.': 1}, 'The technology ': {'technolog': 1, 'accur': 1, 'extract': 1, 'inform': 1, 'insight': 1, 'contain': 1, 'document': 2, 'well': 1, 'categor': 1, 'organ': 1, 'themselv': 1, '.': 1}, 'Challenges in n': {'challeng': 1, 'natur': 1, 'languag': 1, 'process': 1, 'frequent': 1, 'involv': 1, 'speech': 1, 'recognit': 1, ',': 2, 'natural-languag': 2, 'understand': 1, 'gener': 1, '.': 1}, 'Already in 1950': {'alreadi': 1, '1950': 1, ',': 2, 'alan': 1, 'ture': 2, 'publish': 1, 'articl': 1, 'titl': 1, '``': 1, 'comput': 1, 'machineri': 1, 'intellig': 3, \"''\": 1, 'propos': 1, 'call': 1, 'test': 1, 'criterion': 1, 'though': 1, 'time': 1, 'wa': 1, 'articul': 1, 'problem': 1, 'separ': 1, 'artifici': 1, '.': 1}, 'The proposed te': {'propos': 1, 'test': 1, 'includ': 1, 'task': 1, 'involv': 1, 'autom': 1, 'interpret': 1, 'gener': 1, 'natur': 1, 'languag': 1, '.': 1}, 'The premise of ': {'premis': 1, 'symbol': 1, 'nlp': 2, 'well-summar': 1, 'john': 1, 'searl': 1, \"'s\": 1, 'chines': 2, 'room': 1, 'experi': 1, ':': 1, 'given': 1, 'collect': 1, 'rule': 2, '(': 2, 'e.g.': 1, ',': 3, 'phrasebook': 1, 'question': 1, 'match': 1, 'answer': 1, ')': 2, 'comput': 1, 'emul': 1, 'natur': 1, 'languag': 1, 'understand': 1, 'task': 1, 'appli': 1, 'data': 1, 'confront': 1, '.': 1}, 'Up to the 1980s': {'1980': 1, ',': 1, 'natur': 1, 'languag': 1, 'process': 1, 'system': 1, 'base': 1, 'complex': 1, 'set': 1, 'hand-written': 1, 'rule': 1, '.': 1}, 'Starting in the': {'start': 1, 'late': 1, '1980': 1, ',': 2, 'howev': 1, 'wa': 1, 'revolut': 1, 'natur': 1, 'languag': 2, 'process': 2, 'introduct': 1, 'machin': 1, 'learn': 1, 'algorithm': 1, '.': 1}, 'This was due to': {'thi': 1, 'wa': 1, 'due': 1, 'steadi': 1, 'increas': 1, 'comput': 1, 'power': 1, '(': 2, 'see': 1, 'moor': 1, \"'s\": 1, 'law': 1, ')': 1, 'gradual': 1, 'lessen': 1, 'domin': 1, 'chomskyan': 1, 'theori': 1, 'linguist': 1, 'e.g': 1, '.': 1}, 'transformationa': {'transform': 1, 'grammar': 1, ')': 1, ',': 1, 'whose': 1, 'theoret': 1, 'underpin': 1, 'discourag': 1, 'sort': 1, 'corpu': 1, 'linguist': 1, 'underli': 1, 'machine-learn': 1, 'approach': 1, 'languag': 1, 'process': 1, '.': 1}, '[7]\\nIn 2003, wo': {'[': 1, '7': 1, ']': 1, '2003': 1, ',': 3, 'word': 3, 'n-gram': 1, 'model': 2, 'time': 1, 'best': 1, 'statist': 1, 'algorithm': 1, 'wa': 1, 'overperform': 1, 'multi-lay': 1, 'perceptron': 1, '(': 1, 'singl': 1, 'hidden': 1, 'layer': 1, 'context': 1, 'length': 1, 'sever': 1, 'train': 1, '14': 1, 'million': 1, 'cpu': 1, 'cluster': 1, 'languag': 1, ')': 1, 'yoshua': 1, 'bengio': 1, 'co-author': 1, '.': 1}, '[8]\\nIn 2010, To': {'[': 2, '8': 1, ']': 2, '2010': 1, ',': 2, 'tomáš': 1, 'mikolov': 1, '(': 1, 'phd': 1, 'student': 1, 'brno': 1, 'univers': 1, 'technolog': 1, ')': 1, 'co-author': 1, 'appli': 1, 'simpl': 1, 'recurr': 1, 'neural': 1, 'network': 1, 'singl': 1, 'hidden': 1, 'layer': 1, 'languag': 1, 'model': 1, '9': 1, 'follow': 1, 'year': 1, 'went': 1, 'develop': 1, 'word2vec': 1, '.': 1}, 'In the 2010s, r': {'2010': 1, ',': 1, 'represent': 1, 'learn': 2, 'deep': 1, 'neural': 1, 'network-styl': 1, '(': 1, 'featur': 1, 'mani': 1, 'hidden': 1, 'layer': 1, ')': 1, 'machin': 1, 'method': 1, 'becam': 1, 'widespread': 1, 'natur': 1, 'languag': 1, 'process': 1, '.': 1}, 'That popularity': {'popular': 1, 'wa': 1, 'due': 1, 'partli': 1, 'flurri': 1, 'result': 2, 'show': 1, 'techniqu': 1, '[': 3, '10': 1, ']': 3, '11': 1, 'achiev': 1, 'state-of-the-art': 1, 'mani': 1, 'natur': 1, 'languag': 2, 'task': 1, ',': 2, 'e.g.': 1, 'model': 1, '12': 1, 'pars': 1, '.': 1}, '[13][14] This i': {'[': 3, '13': 1, ']': 3, '14': 1, 'thi': 1, 'increasingli': 1, 'import': 1, 'medicin': 1, 'healthcar': 1, ',': 1, 'nlp': 1, 'help': 1, 'analyz': 1, 'note': 1, 'text': 1, 'electron': 1, 'health': 1, 'record': 1, 'would': 1, 'otherwis': 1, 'inaccess': 1, 'studi': 1, 'seek': 1, 'improv': 1, 'care': 1, '15': 1, 'protect': 1, 'patient': 1, 'privaci': 1, '.': 1}, '[16]\\nSymbolic a': {'[': 3, '16': 1, ']': 3, 'symbol': 2, 'approach': 2, ',': 4, 'i.e.': 1, 'hand-cod': 1, 'set': 1, 'rule': 2, 'manipul': 1, 'coupl': 1, 'dictionari': 1, 'lookup': 1, 'wa': 1, 'histor': 1, 'first': 1, 'use': 1, 'ai': 1, 'gener': 1, 'nlp': 1, 'particular': 1, ':': 1, '17': 1, '18': 1, 'write': 1, 'grammar': 1, 'devis': 1, 'heurist': 1, 'stem': 1, '.': 1}, 'Machine learnin': {'machin': 1, 'learn': 1, 'approach': 2, ',': 4, 'includ': 1, 'statist': 1, 'neural': 1, 'network': 1, 'hand': 1, 'mani': 1, 'advantag': 1, 'symbol': 2, ':': 1, 'although': 1, 'rule-bas': 1, 'system': 1, 'manipul': 1, 'still': 1, 'use': 1, '2020': 1, 'becom': 1, 'mostli': 1, 'obsolet': 1, 'advanc': 1, 'llm': 1, '2023': 1, '.': 1}, 'Before that the': {'befor': 1, 'commonli': 1, 'use': 1, ':': 1, 'late': 1, '1980': 1, 'mid-1990': 1, ',': 2, 'statist': 1, 'approach': 2, 'end': 1, 'period': 1, 'ai': 1, 'winter': 1, 'wa': 1, 'caus': 1, 'ineffici': 1, 'rule-bas': 1, '.': 1}, '[19][20]\\nThe ea': {'[': 2, '19': 1, ']': 2, '20': 1, 'earliest': 1, 'decis': 1, 'tree': 1, ',': 2, 'produc': 1, 'system': 1, 'hard': 1, 'if–then': 1, 'rule': 1, 'still': 1, 'veri': 1, 'similar': 1, 'old': 1, 'rule-bas': 1, 'approach': 1, '.': 1}, 'Only the introd': {'onli': 1, 'introduct': 1, 'hidden': 1, 'markov': 1, 'model': 1, ',': 2, 'appli': 1, 'part-of-speech': 1, 'tag': 1, 'announc': 1, 'end': 1, 'old': 1, 'rule-bas': 1, 'approach': 1, '.': 1}, 'A major drawbac': {'major': 1, 'drawback': 1, 'statist': 1, 'method': 1, 'requir': 1, 'elabor': 1, 'featur': 1, 'engin': 1, '.': 1}, 'Since 2015,[21]': {'sinc': 1, '2015': 1, ',': 2, '[': 1, '21': 1, ']': 1, 'statist': 1, 'approach': 2, 'wa': 1, 'replac': 1, 'neural': 1, 'network': 1, 'use': 1, 'word': 2, 'embed': 1, 'captur': 1, 'semant': 1, 'properti': 1, '.': 1}, 'Intermediate ta': {'intermedi': 1, 'task': 1, '(': 1, 'e.g.': 1, ',': 1, 'part-of-speech': 1, 'tag': 1, 'depend': 1, 'pars': 1, ')': 1, 'need': 1, 'anymor': 1, '.': 1}, 'Neural machine ': {'neural': 1, 'machin': 2, 'translat': 2, ',': 4, 'base': 1, 'then-newly-inv': 1, 'sequence-to-sequ': 1, 'transform': 1, 'made': 1, 'obsolet': 1, 'intermedi': 1, 'step': 1, 'word': 1, 'align': 1, 'previous': 1, 'necessari': 1, 'statist': 1, '.': 1}, 'The following i': {'follow': 1, 'list': 1, 'commonli': 1, 'research': 1, 'task': 1, 'natur': 1, 'languag': 1, 'process': 1, '.': 1}, 'Some of these t': {'task': 2, 'direct': 1, 'real-world': 1, 'applic': 1, ',': 1, 'commonli': 1, 'serv': 1, 'subtask': 1, 'use': 1, 'aid': 1, 'solv': 1, 'larger': 1, '.': 1}, 'Though natural ': {'though': 1, 'natur': 1, 'languag': 1, 'process': 1, 'task': 1, 'close': 1, 'intertwin': 1, ',': 1, 'subdivid': 1, 'categori': 1, 'conveni': 1, '.': 1}, 'A coarse divisi': {'coars': 1, 'divis': 1, 'given': 1, '.': 1}, 'Based on long-s': {'base': 1, 'long-stand': 1, 'trend': 1, 'field': 1, ',': 1, 'possibl': 1, 'extrapol': 1, 'futur': 1, 'direct': 1, 'nlp': 1, '.': 1}, 'As of 2020, thr': {'2020': 1, ',': 1, 'three': 1, 'trend': 1, 'among': 1, 'topic': 1, 'long-stand': 1, 'seri': 1, 'conll': 1, 'share': 1, 'task': 1, 'observ': 1, ':': 1, '[': 1, '43': 1, ']': 1, 'higher-level': 1, 'nlp': 1, 'applic': 1, 'involv': 1, 'aspect': 1, 'emul': 1, 'intellig': 1, 'behaviour': 1, 'appar': 1, 'comprehens': 1, 'natur': 1, 'languag': 1, '.': 1}, 'More broadly sp': {'broadli': 1, 'speak': 1, ',': 1, 'technic': 1, 'operation': 1, 'increasingli': 1, 'advanc': 1, 'aspect': 1, 'cognit': 1, 'behaviour': 1, 'repres': 1, 'one': 1, 'development': 1, 'trajectori': 1, 'nlp': 1, '(': 1, 'see': 1, 'trend': 1, 'among': 1, 'conll': 1, 'share': 1, 'task': 1, 'abov': 1, ')': 1, '.': 1}, 'Cognition refer': {'cognit': 1, 'refer': 1, '``': 1, 'mental': 1, 'action': 1, 'process': 1, 'acquir': 1, 'knowledg': 1, 'understand': 1, 'thought': 1, ',': 2, 'experi': 1, 'sens': 1, '.': 1}, '\"[44] Cognitive': {'``': 1, '[': 1, '44': 1, ']': 1, 'cognit': 1, 'scienc': 1, 'interdisciplinari': 1, ',': 1, 'scientif': 1, 'studi': 1, 'mind': 1, 'process': 1, '.': 1}, '[45] Cognitive ': {'[': 1, '45': 1, ']': 1, 'cognit': 1, 'linguist': 3, 'interdisciplinari': 1, 'branch': 1, ',': 1, 'combin': 1, 'knowledg': 1, 'research': 1, 'psycholog': 1, '.': 1}, '[46] Especially': {'[': 1, '46': 1, ']': 1, 'especi': 1, 'dure': 1, 'age': 1, 'symbol': 1, 'nlp': 1, ',': 1, 'area': 1, 'comput': 1, 'linguist': 1, 'maintain': 1, 'strong': 1, 'tie': 1, 'cognit': 1, 'studi': 1, '.': 1}, 'As an example, ': {'exampl': 1, ',': 4, 'georg': 1, 'lakoff': 1, 'offer': 1, 'methodolog': 1, 'build': 1, 'natur': 1, 'languag': 1, 'process': 1, '(': 1, 'nlp': 2, ')': 1, 'algorithm': 1, 'perspect': 1, 'cognit': 3, 'scienc': 1, 'along': 1, 'find': 1, 'linguist': 2, '[': 1, '47': 1, ']': 1, 'two': 1, 'defin': 1, 'aspect': 1, ':': 1, 'tie': 1, 'part': 1, 'histor': 1, 'heritag': 1, 'less': 1, 'frequent': 1, 'address': 1, 'sinc': 1, 'statist': 1, 'turn': 1, 'dure': 1, '1990': 1, '.': 1}, 'Nevertheless, a': {'nevertheless': 1, ',': 9, 'approach': 1, 'develop': 1, 'cognit': 3, 'model': 1, 'toward': 1, 'technic': 1, 'operationaliz': 1, 'framework': 2, 'pursu': 1, 'context': 1, 'variou': 1, 'e.g.': 2, 'grammar': 3, '[': 4, '49': 1, ']': 4, 'function': 1, '50': 1, 'construct': 1, '51': 1, 'comput': 1, 'psycholinguist': 1, 'neurosci': 1, '(': 2, 'act-r': 1, ')': 2, 'howev': 1, 'limit': 1, 'uptak': 1, 'mainstream': 1, 'nlp': 1, 'measur': 1, 'presenc': 1, 'major': 1, 'confer': 1, '52': 1, 'acl': 1, '.': 1}, 'More recently, ': {'recent': 1, ',': 3, 'idea': 1, 'cognit': 2, 'nlp': 1, 'reviv': 1, 'approach': 1, 'achiev': 1, 'explain': 1, 'e.g.': 1, 'notion': 1, '``': 1, 'ai': 1, \"''\": 1, '.': 1}, '[53] Likewise, ': {'[': 2, '53': 1, ']': 2, 'likewis': 1, ',': 2, 'idea': 1, 'cognit': 1, 'nlp': 2, 'inher': 1, 'neural': 1, 'model': 2, 'multimod': 1, '(': 1, 'although': 1, 'rare': 1, 'made': 1, 'explicit': 1, ')': 1, '54': 1, 'develop': 1, 'artifici': 1, 'intellig': 1, 'specif': 1, 'tool': 1, 'technolog': 1, 'use': 1, 'larg': 1, 'languag': 1, 'approach': 1, '.': 1}, '[55]': {'[': 1, '55': 1, ']': 1}}\n",
      "{'Natural languag': {'natur': 0.14285714285714285, 'languag': 0.14285714285714285, 'process': 0.14285714285714285, 'ha': 0.14285714285714285, 'root': 0.14285714285714285, '1950': 0.14285714285714285, '.': 0.14285714285714285}, 'It is primarily': {'primarili': 0.1, 'concern': 0.1, 'give': 0.1, 'comput': 0.1, 'abil': 0.1, 'support': 0.1, 'manipul': 0.1, 'human': 0.1, 'languag': 0.1, '.': 0.1}, 'It involves pro': {'involv': 0.0625, 'process': 0.0625, 'natur': 0.0625, 'languag': 0.0625, 'dataset': 0.0625, ',': 0.125, 'text': 0.0625, 'corpora': 0.125, 'speech': 0.0625, 'use': 0.0625, 'either': 0.0625, 'rule-bas': 0.0625, 'probabilist': 0.0625, '(': 0.0625, 'i.e': 0.0625, '.': 0.0625}, 'statistical and': {'statist': 0.1, ',': 0.2, 'recent': 0.1, 'neural': 0.1, 'network-bas': 0.1, ')': 0.1, 'machin': 0.1, 'learn': 0.1, 'approach': 0.1, '.': 0.1}, 'The goal is a c': {'goal': 0.06666666666666667, 'comput': 0.06666666666666667, 'capabl': 0.06666666666666667, '``': 0.06666666666666667, 'understand': 0.06666666666666667, \"''\": 0.06666666666666667, 'content': 0.06666666666666667, 'document': 0.06666666666666667, ',': 0.06666666666666667, 'includ': 0.06666666666666667, 'contextu': 0.06666666666666667, 'nuanc': 0.06666666666666667, 'languag': 0.06666666666666667, 'within': 0.06666666666666667, '.': 0.06666666666666667}, 'The technology ': {'technolog': 0.08333333333333333, 'accur': 0.08333333333333333, 'extract': 0.08333333333333333, 'inform': 0.08333333333333333, 'insight': 0.08333333333333333, 'contain': 0.08333333333333333, 'document': 0.16666666666666666, 'well': 0.08333333333333333, 'categor': 0.08333333333333333, 'organ': 0.08333333333333333, 'themselv': 0.08333333333333333, '.': 0.08333333333333333}, 'Challenges in n': {'challeng': 0.07692307692307693, 'natur': 0.07692307692307693, 'languag': 0.07692307692307693, 'process': 0.07692307692307693, 'frequent': 0.07692307692307693, 'involv': 0.07692307692307693, 'speech': 0.07692307692307693, 'recognit': 0.07692307692307693, ',': 0.15384615384615385, 'natural-languag': 0.15384615384615385, 'understand': 0.07692307692307693, 'gener': 0.07692307692307693, '.': 0.07692307692307693}, 'Already in 1950': {'alreadi': 0.04, '1950': 0.04, ',': 0.08, 'alan': 0.04, 'ture': 0.08, 'publish': 0.04, 'articl': 0.04, 'titl': 0.04, '``': 0.04, 'comput': 0.04, 'machineri': 0.04, 'intellig': 0.12, \"''\": 0.04, 'propos': 0.04, 'call': 0.04, 'test': 0.04, 'criterion': 0.04, 'though': 0.04, 'time': 0.04, 'wa': 0.04, 'articul': 0.04, 'problem': 0.04, 'separ': 0.04, 'artifici': 0.04, '.': 0.04}, 'The proposed te': {'propos': 0.09090909090909091, 'test': 0.09090909090909091, 'includ': 0.09090909090909091, 'task': 0.09090909090909091, 'involv': 0.09090909090909091, 'autom': 0.09090909090909091, 'interpret': 0.09090909090909091, 'gener': 0.09090909090909091, 'natur': 0.09090909090909091, 'languag': 0.09090909090909091, '.': 0.09090909090909091}, 'The premise of ': {'premis': 0.03125, 'symbol': 0.03125, 'nlp': 0.0625, 'well-summar': 0.03125, 'john': 0.03125, 'searl': 0.03125, \"'s\": 0.03125, 'chines': 0.0625, 'room': 0.03125, 'experi': 0.03125, ':': 0.03125, 'given': 0.03125, 'collect': 0.03125, 'rule': 0.0625, '(': 0.0625, 'e.g.': 0.03125, ',': 0.09375, 'phrasebook': 0.03125, 'question': 0.03125, 'match': 0.03125, 'answer': 0.03125, ')': 0.0625, 'comput': 0.03125, 'emul': 0.03125, 'natur': 0.03125, 'languag': 0.03125, 'understand': 0.03125, 'task': 0.03125, 'appli': 0.03125, 'data': 0.03125, 'confront': 0.03125, '.': 0.03125}, 'Up to the 1980s': {'1980': 0.08333333333333333, ',': 0.08333333333333333, 'natur': 0.08333333333333333, 'languag': 0.08333333333333333, 'process': 0.08333333333333333, 'system': 0.08333333333333333, 'base': 0.08333333333333333, 'complex': 0.08333333333333333, 'set': 0.08333333333333333, 'hand-written': 0.08333333333333333, 'rule': 0.08333333333333333, '.': 0.08333333333333333}, 'Starting in the': {'start': 0.06666666666666667, 'late': 0.06666666666666667, '1980': 0.06666666666666667, ',': 0.13333333333333333, 'howev': 0.06666666666666667, 'wa': 0.06666666666666667, 'revolut': 0.06666666666666667, 'natur': 0.06666666666666667, 'languag': 0.13333333333333333, 'process': 0.13333333333333333, 'introduct': 0.06666666666666667, 'machin': 0.06666666666666667, 'learn': 0.06666666666666667, 'algorithm': 0.06666666666666667, '.': 0.06666666666666667}, 'This was due to': {'thi': 0.047619047619047616, 'wa': 0.047619047619047616, 'due': 0.047619047619047616, 'steadi': 0.047619047619047616, 'increas': 0.047619047619047616, 'comput': 0.047619047619047616, 'power': 0.047619047619047616, '(': 0.09523809523809523, 'see': 0.047619047619047616, 'moor': 0.047619047619047616, \"'s\": 0.047619047619047616, 'law': 0.047619047619047616, ')': 0.047619047619047616, 'gradual': 0.047619047619047616, 'lessen': 0.047619047619047616, 'domin': 0.047619047619047616, 'chomskyan': 0.047619047619047616, 'theori': 0.047619047619047616, 'linguist': 0.047619047619047616, 'e.g': 0.047619047619047616, '.': 0.047619047619047616}, 'transformationa': {'transform': 0.058823529411764705, 'grammar': 0.058823529411764705, ')': 0.058823529411764705, ',': 0.058823529411764705, 'whose': 0.058823529411764705, 'theoret': 0.058823529411764705, 'underpin': 0.058823529411764705, 'discourag': 0.058823529411764705, 'sort': 0.058823529411764705, 'corpu': 0.058823529411764705, 'linguist': 0.058823529411764705, 'underli': 0.058823529411764705, 'machine-learn': 0.058823529411764705, 'approach': 0.058823529411764705, 'languag': 0.058823529411764705, 'process': 0.058823529411764705, '.': 0.058823529411764705}, '[7]\\nIn 2003, wo': {'[': 0.029411764705882353, '7': 0.029411764705882353, ']': 0.029411764705882353, '2003': 0.029411764705882353, ',': 0.08823529411764706, 'word': 0.08823529411764706, 'n-gram': 0.029411764705882353, 'model': 0.058823529411764705, 'time': 0.029411764705882353, 'best': 0.029411764705882353, 'statist': 0.029411764705882353, 'algorithm': 0.029411764705882353, 'wa': 0.029411764705882353, 'overperform': 0.029411764705882353, 'multi-lay': 0.029411764705882353, 'perceptron': 0.029411764705882353, '(': 0.029411764705882353, 'singl': 0.029411764705882353, 'hidden': 0.029411764705882353, 'layer': 0.029411764705882353, 'context': 0.029411764705882353, 'length': 0.029411764705882353, 'sever': 0.029411764705882353, 'train': 0.029411764705882353, '14': 0.029411764705882353, 'million': 0.029411764705882353, 'cpu': 0.029411764705882353, 'cluster': 0.029411764705882353, 'languag': 0.029411764705882353, ')': 0.029411764705882353, 'yoshua': 0.029411764705882353, 'bengio': 0.029411764705882353, 'co-author': 0.029411764705882353, '.': 0.029411764705882353}, '[8]\\nIn 2010, To': {'[': 0.0625, '8': 0.03125, ']': 0.0625, '2010': 0.03125, ',': 0.0625, 'tomáš': 0.03125, 'mikolov': 0.03125, '(': 0.03125, 'phd': 0.03125, 'student': 0.03125, 'brno': 0.03125, 'univers': 0.03125, 'technolog': 0.03125, ')': 0.03125, 'co-author': 0.03125, 'appli': 0.03125, 'simpl': 0.03125, 'recurr': 0.03125, 'neural': 0.03125, 'network': 0.03125, 'singl': 0.03125, 'hidden': 0.03125, 'layer': 0.03125, 'languag': 0.03125, 'model': 0.03125, '9': 0.03125, 'follow': 0.03125, 'year': 0.03125, 'went': 0.03125, 'develop': 0.03125, 'word2vec': 0.03125, '.': 0.03125}, 'In the 2010s, r': {'2010': 0.047619047619047616, ',': 0.047619047619047616, 'represent': 0.047619047619047616, 'learn': 0.09523809523809523, 'deep': 0.047619047619047616, 'neural': 0.047619047619047616, 'network-styl': 0.047619047619047616, '(': 0.047619047619047616, 'featur': 0.047619047619047616, 'mani': 0.047619047619047616, 'hidden': 0.047619047619047616, 'layer': 0.047619047619047616, ')': 0.047619047619047616, 'machin': 0.047619047619047616, 'method': 0.047619047619047616, 'becam': 0.047619047619047616, 'widespread': 0.047619047619047616, 'natur': 0.047619047619047616, 'languag': 0.047619047619047616, 'process': 0.047619047619047616, '.': 0.047619047619047616}, 'That popularity': {'popular': 0.041666666666666664, 'wa': 0.041666666666666664, 'due': 0.041666666666666664, 'partli': 0.041666666666666664, 'flurri': 0.041666666666666664, 'result': 0.08333333333333333, 'show': 0.041666666666666664, 'techniqu': 0.041666666666666664, '[': 0.125, '10': 0.041666666666666664, ']': 0.125, '11': 0.041666666666666664, 'achiev': 0.041666666666666664, 'state-of-the-art': 0.041666666666666664, 'mani': 0.041666666666666664, 'natur': 0.041666666666666664, 'languag': 0.08333333333333333, 'task': 0.041666666666666664, ',': 0.08333333333333333, 'e.g.': 0.041666666666666664, 'model': 0.041666666666666664, '12': 0.041666666666666664, 'pars': 0.041666666666666664, '.': 0.041666666666666664}, '[13][14] This i': {'[': 0.1, '13': 0.03333333333333333, ']': 0.1, '14': 0.03333333333333333, 'thi': 0.03333333333333333, 'increasingli': 0.03333333333333333, 'import': 0.03333333333333333, 'medicin': 0.03333333333333333, 'healthcar': 0.03333333333333333, ',': 0.03333333333333333, 'nlp': 0.03333333333333333, 'help': 0.03333333333333333, 'analyz': 0.03333333333333333, 'note': 0.03333333333333333, 'text': 0.03333333333333333, 'electron': 0.03333333333333333, 'health': 0.03333333333333333, 'record': 0.03333333333333333, 'would': 0.03333333333333333, 'otherwis': 0.03333333333333333, 'inaccess': 0.03333333333333333, 'studi': 0.03333333333333333, 'seek': 0.03333333333333333, 'improv': 0.03333333333333333, 'care': 0.03333333333333333, '15': 0.03333333333333333, 'protect': 0.03333333333333333, 'patient': 0.03333333333333333, 'privaci': 0.03333333333333333, '.': 0.03333333333333333}, '[16]\\nSymbolic a': {'[': 0.0967741935483871, '16': 0.03225806451612903, ']': 0.0967741935483871, 'symbol': 0.06451612903225806, 'approach': 0.06451612903225806, ',': 0.12903225806451613, 'i.e.': 0.03225806451612903, 'hand-cod': 0.03225806451612903, 'set': 0.03225806451612903, 'rule': 0.06451612903225806, 'manipul': 0.03225806451612903, 'coupl': 0.03225806451612903, 'dictionari': 0.03225806451612903, 'lookup': 0.03225806451612903, 'wa': 0.03225806451612903, 'histor': 0.03225806451612903, 'first': 0.03225806451612903, 'use': 0.03225806451612903, 'ai': 0.03225806451612903, 'gener': 0.03225806451612903, 'nlp': 0.03225806451612903, 'particular': 0.03225806451612903, ':': 0.03225806451612903, '17': 0.03225806451612903, '18': 0.03225806451612903, 'write': 0.03225806451612903, 'grammar': 0.03225806451612903, 'devis': 0.03225806451612903, 'heurist': 0.03225806451612903, 'stem': 0.03225806451612903, '.': 0.03225806451612903}, 'Machine learnin': {'machin': 0.037037037037037035, 'learn': 0.037037037037037035, 'approach': 0.07407407407407407, ',': 0.14814814814814814, 'includ': 0.037037037037037035, 'statist': 0.037037037037037035, 'neural': 0.037037037037037035, 'network': 0.037037037037037035, 'hand': 0.037037037037037035, 'mani': 0.037037037037037035, 'advantag': 0.037037037037037035, 'symbol': 0.07407407407407407, ':': 0.037037037037037035, 'although': 0.037037037037037035, 'rule-bas': 0.037037037037037035, 'system': 0.037037037037037035, 'manipul': 0.037037037037037035, 'still': 0.037037037037037035, 'use': 0.037037037037037035, '2020': 0.037037037037037035, 'becom': 0.037037037037037035, 'mostli': 0.037037037037037035, 'obsolet': 0.037037037037037035, 'advanc': 0.037037037037037035, 'llm': 0.037037037037037035, '2023': 0.037037037037037035, '.': 0.037037037037037035}, 'Before that the': {'befor': 0.05263157894736842, 'commonli': 0.05263157894736842, 'use': 0.05263157894736842, ':': 0.05263157894736842, 'late': 0.05263157894736842, '1980': 0.05263157894736842, 'mid-1990': 0.05263157894736842, ',': 0.10526315789473684, 'statist': 0.05263157894736842, 'approach': 0.10526315789473684, 'end': 0.05263157894736842, 'period': 0.05263157894736842, 'ai': 0.05263157894736842, 'winter': 0.05263157894736842, 'wa': 0.05263157894736842, 'caus': 0.05263157894736842, 'ineffici': 0.05263157894736842, 'rule-bas': 0.05263157894736842, '.': 0.05263157894736842}, '[19][20]\\nThe ea': {'[': 0.1, '19': 0.05, ']': 0.1, '20': 0.05, 'earliest': 0.05, 'decis': 0.05, 'tree': 0.05, ',': 0.1, 'produc': 0.05, 'system': 0.05, 'hard': 0.05, 'if–then': 0.05, 'rule': 0.05, 'still': 0.05, 'veri': 0.05, 'similar': 0.05, 'old': 0.05, 'rule-bas': 0.05, 'approach': 0.05, '.': 0.05}, 'Only the introd': {'onli': 0.06666666666666667, 'introduct': 0.06666666666666667, 'hidden': 0.06666666666666667, 'markov': 0.06666666666666667, 'model': 0.06666666666666667, ',': 0.13333333333333333, 'appli': 0.06666666666666667, 'part-of-speech': 0.06666666666666667, 'tag': 0.06666666666666667, 'announc': 0.06666666666666667, 'end': 0.06666666666666667, 'old': 0.06666666666666667, 'rule-bas': 0.06666666666666667, 'approach': 0.06666666666666667, '.': 0.06666666666666667}, 'A major drawbac': {'major': 0.1111111111111111, 'drawback': 0.1111111111111111, 'statist': 0.1111111111111111, 'method': 0.1111111111111111, 'requir': 0.1111111111111111, 'elabor': 0.1111111111111111, 'featur': 0.1111111111111111, 'engin': 0.1111111111111111, '.': 0.1111111111111111}, 'Since 2015,[21]': {'sinc': 0.05263157894736842, '2015': 0.05263157894736842, ',': 0.10526315789473684, '[': 0.05263157894736842, '21': 0.05263157894736842, ']': 0.05263157894736842, 'statist': 0.05263157894736842, 'approach': 0.10526315789473684, 'wa': 0.05263157894736842, 'replac': 0.05263157894736842, 'neural': 0.05263157894736842, 'network': 0.05263157894736842, 'use': 0.05263157894736842, 'word': 0.10526315789473684, 'embed': 0.05263157894736842, 'captur': 0.05263157894736842, 'semant': 0.05263157894736842, 'properti': 0.05263157894736842, '.': 0.05263157894736842}, 'Intermediate ta': {'intermedi': 0.07692307692307693, 'task': 0.07692307692307693, '(': 0.07692307692307693, 'e.g.': 0.07692307692307693, ',': 0.07692307692307693, 'part-of-speech': 0.07692307692307693, 'tag': 0.07692307692307693, 'depend': 0.07692307692307693, 'pars': 0.07692307692307693, ')': 0.07692307692307693, 'need': 0.07692307692307693, 'anymor': 0.07692307692307693, '.': 0.07692307692307693}, 'Neural machine ': {'neural': 0.05555555555555555, 'machin': 0.1111111111111111, 'translat': 0.1111111111111111, ',': 0.2222222222222222, 'base': 0.05555555555555555, 'then-newly-inv': 0.05555555555555555, 'sequence-to-sequ': 0.05555555555555555, 'transform': 0.05555555555555555, 'made': 0.05555555555555555, 'obsolet': 0.05555555555555555, 'intermedi': 0.05555555555555555, 'step': 0.05555555555555555, 'word': 0.05555555555555555, 'align': 0.05555555555555555, 'previous': 0.05555555555555555, 'necessari': 0.05555555555555555, 'statist': 0.05555555555555555, '.': 0.05555555555555555}, 'The following i': {'follow': 0.1111111111111111, 'list': 0.1111111111111111, 'commonli': 0.1111111111111111, 'research': 0.1111111111111111, 'task': 0.1111111111111111, 'natur': 0.1111111111111111, 'languag': 0.1111111111111111, 'process': 0.1111111111111111, '.': 0.1111111111111111}, 'Some of these t': {'task': 0.15384615384615385, 'direct': 0.07692307692307693, 'real-world': 0.07692307692307693, 'applic': 0.07692307692307693, ',': 0.07692307692307693, 'commonli': 0.07692307692307693, 'serv': 0.07692307692307693, 'subtask': 0.07692307692307693, 'use': 0.07692307692307693, 'aid': 0.07692307692307693, 'solv': 0.07692307692307693, 'larger': 0.07692307692307693, '.': 0.07692307692307693}, 'Though natural ': {'though': 0.08333333333333333, 'natur': 0.08333333333333333, 'languag': 0.08333333333333333, 'process': 0.08333333333333333, 'task': 0.08333333333333333, 'close': 0.08333333333333333, 'intertwin': 0.08333333333333333, ',': 0.08333333333333333, 'subdivid': 0.08333333333333333, 'categori': 0.08333333333333333, 'conveni': 0.08333333333333333, '.': 0.08333333333333333}, 'A coarse divisi': {'coars': 0.25, 'divis': 0.25, 'given': 0.25, '.': 0.25}, 'Based on long-s': {'base': 0.09090909090909091, 'long-stand': 0.09090909090909091, 'trend': 0.09090909090909091, 'field': 0.09090909090909091, ',': 0.09090909090909091, 'possibl': 0.09090909090909091, 'extrapol': 0.09090909090909091, 'futur': 0.09090909090909091, 'direct': 0.09090909090909091, 'nlp': 0.09090909090909091, '.': 0.09090909090909091}, 'As of 2020, thr': {'2020': 0.034482758620689655, ',': 0.034482758620689655, 'three': 0.034482758620689655, 'trend': 0.034482758620689655, 'among': 0.034482758620689655, 'topic': 0.034482758620689655, 'long-stand': 0.034482758620689655, 'seri': 0.034482758620689655, 'conll': 0.034482758620689655, 'share': 0.034482758620689655, 'task': 0.034482758620689655, 'observ': 0.034482758620689655, ':': 0.034482758620689655, '[': 0.034482758620689655, '43': 0.034482758620689655, ']': 0.034482758620689655, 'higher-level': 0.034482758620689655, 'nlp': 0.034482758620689655, 'applic': 0.034482758620689655, 'involv': 0.034482758620689655, 'aspect': 0.034482758620689655, 'emul': 0.034482758620689655, 'intellig': 0.034482758620689655, 'behaviour': 0.034482758620689655, 'appar': 0.034482758620689655, 'comprehens': 0.034482758620689655, 'natur': 0.034482758620689655, 'languag': 0.034482758620689655, '.': 0.034482758620689655}, 'More broadly sp': {'broadli': 0.04, 'speak': 0.04, ',': 0.04, 'technic': 0.04, 'operation': 0.04, 'increasingli': 0.04, 'advanc': 0.04, 'aspect': 0.04, 'cognit': 0.04, 'behaviour': 0.04, 'repres': 0.04, 'one': 0.04, 'development': 0.04, 'trajectori': 0.04, 'nlp': 0.04, '(': 0.04, 'see': 0.04, 'trend': 0.04, 'among': 0.04, 'conll': 0.04, 'share': 0.04, 'task': 0.04, 'abov': 0.04, ')': 0.04, '.': 0.04}, 'Cognition refer': {'cognit': 0.07142857142857142, 'refer': 0.07142857142857142, '``': 0.07142857142857142, 'mental': 0.07142857142857142, 'action': 0.07142857142857142, 'process': 0.07142857142857142, 'acquir': 0.07142857142857142, 'knowledg': 0.07142857142857142, 'understand': 0.07142857142857142, 'thought': 0.07142857142857142, ',': 0.14285714285714285, 'experi': 0.07142857142857142, 'sens': 0.07142857142857142, '.': 0.07142857142857142}, '\"[44] Cognitive': {'``': 0.07692307692307693, '[': 0.07692307692307693, '44': 0.07692307692307693, ']': 0.07692307692307693, 'cognit': 0.07692307692307693, 'scienc': 0.07692307692307693, 'interdisciplinari': 0.07692307692307693, ',': 0.07692307692307693, 'scientif': 0.07692307692307693, 'studi': 0.07692307692307693, 'mind': 0.07692307692307693, 'process': 0.07692307692307693, '.': 0.07692307692307693}, '[45] Cognitive ': {'[': 0.07692307692307693, '45': 0.07692307692307693, ']': 0.07692307692307693, 'cognit': 0.07692307692307693, 'linguist': 0.23076923076923078, 'interdisciplinari': 0.07692307692307693, 'branch': 0.07692307692307693, ',': 0.07692307692307693, 'combin': 0.07692307692307693, 'knowledg': 0.07692307692307693, 'research': 0.07692307692307693, 'psycholog': 0.07692307692307693, '.': 0.07692307692307693}, '[46] Especially': {'[': 0.05555555555555555, '46': 0.05555555555555555, ']': 0.05555555555555555, 'especi': 0.05555555555555555, 'dure': 0.05555555555555555, 'age': 0.05555555555555555, 'symbol': 0.05555555555555555, 'nlp': 0.05555555555555555, ',': 0.05555555555555555, 'area': 0.05555555555555555, 'comput': 0.05555555555555555, 'linguist': 0.05555555555555555, 'maintain': 0.05555555555555555, 'strong': 0.05555555555555555, 'tie': 0.05555555555555555, 'cognit': 0.05555555555555555, 'studi': 0.05555555555555555, '.': 0.05555555555555555}, 'As an example, ': {'exampl': 0.025, ',': 0.1, 'georg': 0.025, 'lakoff': 0.025, 'offer': 0.025, 'methodolog': 0.025, 'build': 0.025, 'natur': 0.025, 'languag': 0.025, 'process': 0.025, '(': 0.025, 'nlp': 0.05, ')': 0.025, 'algorithm': 0.025, 'perspect': 0.025, 'cognit': 0.075, 'scienc': 0.025, 'along': 0.025, 'find': 0.025, 'linguist': 0.05, '[': 0.025, '47': 0.025, ']': 0.025, 'two': 0.025, 'defin': 0.025, 'aspect': 0.025, ':': 0.025, 'tie': 0.025, 'part': 0.025, 'histor': 0.025, 'heritag': 0.025, 'less': 0.025, 'frequent': 0.025, 'address': 0.025, 'sinc': 0.025, 'statist': 0.025, 'turn': 0.025, 'dure': 0.025, '1990': 0.025, '.': 0.025}, 'Nevertheless, a': {'nevertheless': 0.025, ',': 0.225, 'approach': 0.025, 'develop': 0.025, 'cognit': 0.075, 'model': 0.025, 'toward': 0.025, 'technic': 0.025, 'operationaliz': 0.025, 'framework': 0.05, 'pursu': 0.025, 'context': 0.025, 'variou': 0.025, 'e.g.': 0.05, 'grammar': 0.075, '[': 0.1, '49': 0.025, ']': 0.1, 'function': 0.025, '50': 0.025, 'construct': 0.025, '51': 0.025, 'comput': 0.025, 'psycholinguist': 0.025, 'neurosci': 0.025, '(': 0.05, 'act-r': 0.025, ')': 0.05, 'howev': 0.025, 'limit': 0.025, 'uptak': 0.025, 'mainstream': 0.025, 'nlp': 0.025, 'measur': 0.025, 'presenc': 0.025, 'major': 0.025, 'confer': 0.025, '52': 0.025, 'acl': 0.025, '.': 0.025}, 'More recently, ': {'recent': 0.06666666666666667, ',': 0.2, 'idea': 0.06666666666666667, 'cognit': 0.13333333333333333, 'nlp': 0.06666666666666667, 'reviv': 0.06666666666666667, 'approach': 0.06666666666666667, 'achiev': 0.06666666666666667, 'explain': 0.06666666666666667, 'e.g.': 0.06666666666666667, 'notion': 0.06666666666666667, '``': 0.06666666666666667, 'ai': 0.06666666666666667, \"''\": 0.06666666666666667, '.': 0.06666666666666667}, '[53] Likewise, ': {'[': 0.06666666666666667, '53': 0.03333333333333333, ']': 0.06666666666666667, 'likewis': 0.03333333333333333, ',': 0.06666666666666667, 'idea': 0.03333333333333333, 'cognit': 0.03333333333333333, 'nlp': 0.06666666666666667, 'inher': 0.03333333333333333, 'neural': 0.03333333333333333, 'model': 0.06666666666666667, 'multimod': 0.03333333333333333, '(': 0.03333333333333333, 'although': 0.03333333333333333, 'rare': 0.03333333333333333, 'made': 0.03333333333333333, 'explicit': 0.03333333333333333, ')': 0.03333333333333333, '54': 0.03333333333333333, 'develop': 0.03333333333333333, 'artifici': 0.03333333333333333, 'intellig': 0.03333333333333333, 'specif': 0.03333333333333333, 'tool': 0.03333333333333333, 'technolog': 0.03333333333333333, 'use': 0.03333333333333333, 'larg': 0.03333333333333333, 'languag': 0.03333333333333333, 'approach': 0.03333333333333333, '.': 0.03333333333333333}, '[55]': {'[': 0.3333333333333333, '55': 0.3333333333333333, ']': 0.3333333333333333}}\n",
      "{'natur': 13, 'languag': 19, 'process': 12, 'ha': 1, 'root': 1, '1950': 2, '.': 43, 'primarili': 1, 'concern': 1, 'give': 1, 'comput': 7, 'abil': 1, 'support': 1, 'manipul': 3, 'human': 1, 'involv': 4, 'dataset': 1, ',': 35, 'text': 2, 'corpora': 1, 'speech': 2, 'use': 7, 'either': 1, 'rule-bas': 5, 'probabilist': 1, '(': 11, 'i.e': 1, 'statist': 8, 'recent': 2, 'neural': 7, 'network-bas': 1, ')': 12, 'machin': 5, 'learn': 4, 'approach': 11, 'goal': 1, 'capabl': 1, '``': 5, 'understand': 4, \"''\": 3, 'content': 1, 'document': 2, 'includ': 3, 'contextu': 1, 'nuanc': 1, 'within': 1, 'technolog': 3, 'accur': 1, 'extract': 1, 'inform': 1, 'insight': 1, 'contain': 1, 'well': 1, 'categor': 1, 'organ': 1, 'themselv': 1, 'challeng': 1, 'frequent': 2, 'recognit': 1, 'natural-languag': 1, 'gener': 3, 'alreadi': 1, 'alan': 1, 'ture': 1, 'publish': 1, 'articl': 1, 'titl': 1, 'machineri': 1, 'intellig': 3, 'propos': 2, 'call': 1, 'test': 2, 'criterion': 1, 'though': 2, 'time': 2, 'wa': 8, 'articul': 1, 'problem': 1, 'separ': 1, 'artifici': 2, 'task': 9, 'autom': 1, 'interpret': 1, 'premis': 1, 'symbol': 4, 'nlp': 11, 'well-summar': 1, 'john': 1, 'searl': 1, \"'s\": 2, 'chines': 1, 'room': 1, 'experi': 2, ':': 6, 'given': 2, 'collect': 1, 'rule': 4, 'e.g.': 5, 'phrasebook': 1, 'question': 1, 'match': 1, 'answer': 1, 'emul': 2, 'appli': 3, 'data': 1, 'confront': 1, '1980': 3, 'system': 3, 'base': 3, 'complex': 1, 'set': 2, 'hand-written': 1, 'start': 1, 'late': 2, 'howev': 2, 'revolut': 1, 'introduct': 2, 'algorithm': 3, 'thi': 2, 'due': 2, 'steadi': 1, 'increas': 1, 'power': 1, 'see': 2, 'moor': 1, 'law': 1, 'gradual': 1, 'lessen': 1, 'domin': 1, 'chomskyan': 1, 'theori': 1, 'linguist': 5, 'e.g': 1, 'transform': 2, 'grammar': 3, 'whose': 1, 'theoret': 1, 'underpin': 1, 'discourag': 1, 'sort': 1, 'corpu': 1, 'underli': 1, 'machine-learn': 1, '[': 15, '7': 1, ']': 15, '2003': 1, 'word': 3, 'n-gram': 1, 'model': 6, 'best': 1, 'overperform': 1, 'multi-lay': 1, 'perceptron': 1, 'singl': 2, 'hidden': 4, 'layer': 3, 'context': 2, 'length': 1, 'sever': 1, 'train': 1, '14': 2, 'million': 1, 'cpu': 1, 'cluster': 1, 'yoshua': 1, 'bengio': 1, 'co-author': 2, '8': 1, '2010': 2, 'tomáš': 1, 'mikolov': 1, 'phd': 1, 'student': 1, 'brno': 1, 'univers': 1, 'simpl': 1, 'recurr': 1, 'network': 3, '9': 1, 'follow': 2, 'year': 1, 'went': 1, 'develop': 3, 'word2vec': 1, 'represent': 1, 'deep': 1, 'network-styl': 1, 'featur': 2, 'mani': 3, 'method': 2, 'becam': 1, 'widespread': 1, 'popular': 1, 'partli': 1, 'flurri': 1, 'result': 1, 'show': 1, 'techniqu': 1, '10': 1, '11': 1, 'achiev': 2, 'state-of-the-art': 1, '12': 1, 'pars': 2, '13': 1, 'increasingli': 2, 'import': 1, 'medicin': 1, 'healthcar': 1, 'help': 1, 'analyz': 1, 'note': 1, 'electron': 1, 'health': 1, 'record': 1, 'would': 1, 'otherwis': 1, 'inaccess': 1, 'studi': 3, 'seek': 1, 'improv': 1, 'care': 1, '15': 1, 'protect': 1, 'patient': 1, 'privaci': 1, '16': 1, 'i.e.': 1, 'hand-cod': 1, 'coupl': 1, 'dictionari': 1, 'lookup': 1, 'histor': 2, 'first': 1, 'ai': 3, 'particular': 1, '17': 1, '18': 1, 'write': 1, 'devis': 1, 'heurist': 1, 'stem': 1, 'hand': 1, 'advantag': 1, 'although': 2, 'still': 2, '2020': 2, 'becom': 1, 'mostli': 1, 'obsolet': 2, 'advanc': 2, 'llm': 1, '2023': 1, 'befor': 1, 'commonli': 3, 'mid-1990': 1, 'end': 2, 'period': 1, 'winter': 1, 'caus': 1, 'ineffici': 1, '19': 1, '20': 1, 'earliest': 1, 'decis': 1, 'tree': 1, 'produc': 1, 'hard': 1, 'if–then': 1, 'veri': 1, 'similar': 1, 'old': 2, 'onli': 1, 'markov': 1, 'part-of-speech': 2, 'tag': 2, 'announc': 1, 'major': 2, 'drawback': 1, 'requir': 1, 'elabor': 1, 'engin': 1, 'sinc': 2, '2015': 1, '21': 1, 'replac': 1, 'embed': 1, 'captur': 1, 'semant': 1, 'properti': 1, 'intermedi': 2, 'depend': 1, 'need': 1, 'anymor': 1, 'translat': 1, 'then-newly-inv': 1, 'sequence-to-sequ': 1, 'made': 2, 'step': 1, 'align': 1, 'previous': 1, 'necessari': 1, 'list': 1, 'research': 2, 'direct': 2, 'real-world': 1, 'applic': 2, 'serv': 1, 'subtask': 1, 'aid': 1, 'solv': 1, 'larger': 1, 'close': 1, 'intertwin': 1, 'subdivid': 1, 'categori': 1, 'conveni': 1, 'coars': 1, 'divis': 1, 'long-stand': 2, 'trend': 3, 'field': 1, 'possibl': 1, 'extrapol': 1, 'futur': 1, 'three': 1, 'among': 2, 'topic': 1, 'seri': 1, 'conll': 2, 'share': 2, 'observ': 1, '43': 1, 'higher-level': 1, 'aspect': 3, 'behaviour': 2, 'appar': 1, 'comprehens': 1, 'broadli': 1, 'speak': 1, 'technic': 2, 'operation': 1, 'cognit': 9, 'repres': 1, 'one': 1, 'development': 1, 'trajectori': 1, 'abov': 1, 'refer': 1, 'mental': 1, 'action': 1, 'acquir': 1, 'knowledg': 2, 'thought': 1, 'sens': 1, '44': 1, 'scienc': 2, 'interdisciplinari': 2, 'scientif': 1, 'mind': 1, '45': 1, 'branch': 1, 'combin': 1, 'psycholog': 1, '46': 1, 'especi': 1, 'dure': 2, 'age': 1, 'area': 1, 'maintain': 1, 'strong': 1, 'tie': 2, 'exampl': 1, 'georg': 1, 'lakoff': 1, 'offer': 1, 'methodolog': 1, 'build': 1, 'perspect': 1, 'along': 1, 'find': 1, '47': 1, 'two': 1, 'defin': 1, 'part': 1, 'heritag': 1, 'less': 1, 'address': 1, 'turn': 1, '1990': 1, 'nevertheless': 1, 'toward': 1, 'operationaliz': 1, 'framework': 1, 'pursu': 1, 'variou': 1, '49': 1, 'function': 1, '50': 1, 'construct': 1, '51': 1, 'psycholinguist': 1, 'neurosci': 1, 'act-r': 1, 'limit': 1, 'uptak': 1, 'mainstream': 1, 'measur': 1, 'presenc': 1, 'confer': 1, '52': 1, 'acl': 1, 'idea': 2, 'reviv': 1, 'explain': 1, 'notion': 1, '53': 1, 'likewis': 1, 'inher': 1, 'multimod': 1, 'rare': 1, 'explicit': 1, '54': 1, 'specif': 1, 'tool': 1, 'larg': 1, '55': 1}\n",
      "{'Natural languag': {'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, 'ha': 1.6532125137753437, 'root': 1.6532125137753437, '1950': 1.3521825181113625, '.': 0.019744058195757187}, 'It is primarily': {'primarili': 1.6532125137753437, 'concern': 1.6532125137753437, 'give': 1.6532125137753437, 'comput': 0.8081144737610869, 'abil': 1.6532125137753437, 'support': 1.6532125137753437, 'manipul': 1.1760912590556813, 'human': 1.6532125137753437, 'languag': 0.3744589128225147, '.': 0.019744058195757187}, 'It involves pro': {'involv': 1.0511525224473812, 'process': 0.5740312677277188, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'dataset': 1.6532125137753437, ',': 0.10914446942506807, 'text': 1.3521825181113625, 'corpora': 1.6532125137753437, 'speech': 1.3521825181113625, 'use': 0.8081144737610869, 'either': 1.6532125137753437, 'rule-bas': 0.9542425094393249, 'probabilist': 1.6532125137753437, '(': 0.6118198286171186, 'i.e': 1.6532125137753437, '.': 0.019744058195757187}, 'statistical and': {'statist': 0.7501225267834001, ',': 0.10914446942506807, 'recent': 1.3521825181113625, 'neural': 0.8081144737610869, 'network-bas': 1.6532125137753437, ')': 0.5740312677277188, 'machin': 0.9542425094393249, 'learn': 1.0511525224473812, 'approach': 0.6118198286171186, '.': 0.019744058195757187}, 'The goal is a c': {'goal': 1.6532125137753437, 'comput': 0.8081144737610869, 'capabl': 1.6532125137753437, '``': 0.9542425094393249, 'understand': 1.0511525224473812, \"''\": 1.1760912590556813, 'content': 1.6532125137753437, 'document': 1.3521825181113625, ',': 0.10914446942506807, 'includ': 1.1760912590556813, 'contextu': 1.6532125137753437, 'nuanc': 1.6532125137753437, 'languag': 0.3744589128225147, 'within': 1.6532125137753437, '.': 0.019744058195757187}, 'The technology ': {'technolog': 1.1760912590556813, 'accur': 1.6532125137753437, 'extract': 1.6532125137753437, 'inform': 1.6532125137753437, 'insight': 1.6532125137753437, 'contain': 1.6532125137753437, 'document': 1.3521825181113625, 'well': 1.6532125137753437, 'categor': 1.6532125137753437, 'organ': 1.6532125137753437, 'themselv': 1.6532125137753437, '.': 0.019744058195757187}, 'Challenges in n': {'challeng': 1.6532125137753437, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, 'frequent': 1.3521825181113625, 'involv': 1.0511525224473812, 'speech': 1.3521825181113625, 'recognit': 1.6532125137753437, ',': 0.10914446942506807, 'natural-languag': 1.6532125137753437, 'understand': 1.0511525224473812, 'gener': 1.1760912590556813, '.': 0.019744058195757187}, 'Already in 1950': {'alreadi': 1.6532125137753437, '1950': 1.3521825181113625, ',': 0.10914446942506807, 'alan': 1.6532125137753437, 'ture': 1.6532125137753437, 'publish': 1.6532125137753437, 'articl': 1.6532125137753437, 'titl': 1.6532125137753437, '``': 0.9542425094393249, 'comput': 0.8081144737610869, 'machineri': 1.6532125137753437, 'intellig': 1.1760912590556813, \"''\": 1.1760912590556813, 'propos': 1.3521825181113625, 'call': 1.6532125137753437, 'test': 1.3521825181113625, 'criterion': 1.6532125137753437, 'though': 1.3521825181113625, 'time': 1.3521825181113625, 'wa': 0.7501225267834001, 'articul': 1.6532125137753437, 'problem': 1.6532125137753437, 'separ': 1.6532125137753437, 'artifici': 1.3521825181113625, '.': 0.019744058195757187}, 'The proposed te': {'propos': 1.3521825181113625, 'test': 1.3521825181113625, 'includ': 1.1760912590556813, 'task': 0.6989700043360189, 'involv': 1.0511525224473812, 'autom': 1.6532125137753437, 'interpret': 1.6532125137753437, 'gener': 1.1760912590556813, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, '.': 0.019744058195757187}, 'The premise of ': {'premis': 1.6532125137753437, 'symbol': 1.0511525224473812, 'nlp': 0.6118198286171186, 'well-summar': 1.6532125137753437, 'john': 1.6532125137753437, 'searl': 1.6532125137753437, \"'s\": 1.3521825181113625, 'chines': 1.6532125137753437, 'room': 1.6532125137753437, 'experi': 1.3521825181113625, ':': 0.8750612633917001, 'given': 1.3521825181113625, 'collect': 1.6532125137753437, 'rule': 1.0511525224473812, '(': 0.6118198286171186, 'e.g.': 0.9542425094393249, ',': 0.10914446942506807, 'phrasebook': 1.6532125137753437, 'question': 1.6532125137753437, 'match': 1.6532125137753437, 'answer': 1.6532125137753437, ')': 0.5740312677277188, 'comput': 0.8081144737610869, 'emul': 1.3521825181113625, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'understand': 1.0511525224473812, 'task': 0.6989700043360189, 'appli': 1.1760912590556813, 'data': 1.6532125137753437, 'confront': 1.6532125137753437, '.': 0.019744058195757187}, 'Up to the 1980s': {'1980': 1.1760912590556813, ',': 0.10914446942506807, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, 'system': 1.1760912590556813, 'base': 1.1760912590556813, 'complex': 1.6532125137753437, 'set': 1.3521825181113625, 'hand-written': 1.6532125137753437, 'rule': 1.0511525224473812, '.': 0.019744058195757187}, 'Starting in the': {'start': 1.6532125137753437, 'late': 1.3521825181113625, '1980': 1.1760912590556813, ',': 0.10914446942506807, 'howev': 1.3521825181113625, 'wa': 0.7501225267834001, 'revolut': 1.6532125137753437, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, 'introduct': 1.3521825181113625, 'machin': 0.9542425094393249, 'learn': 1.0511525224473812, 'algorithm': 1.1760912590556813, '.': 0.019744058195757187}, 'This was due to': {'thi': 1.3521825181113625, 'wa': 0.7501225267834001, 'due': 1.3521825181113625, 'steadi': 1.6532125137753437, 'increas': 1.6532125137753437, 'comput': 0.8081144737610869, 'power': 1.6532125137753437, '(': 0.6118198286171186, 'see': 1.3521825181113625, 'moor': 1.6532125137753437, \"'s\": 1.3521825181113625, 'law': 1.6532125137753437, ')': 0.5740312677277188, 'gradual': 1.6532125137753437, 'lessen': 1.6532125137753437, 'domin': 1.6532125137753437, 'chomskyan': 1.6532125137753437, 'theori': 1.6532125137753437, 'linguist': 0.9542425094393249, 'e.g': 1.6532125137753437, '.': 0.019744058195757187}, 'transformationa': {'transform': 1.3521825181113625, 'grammar': 1.1760912590556813, ')': 0.5740312677277188, ',': 0.10914446942506807, 'whose': 1.6532125137753437, 'theoret': 1.6532125137753437, 'underpin': 1.6532125137753437, 'discourag': 1.6532125137753437, 'sort': 1.6532125137753437, 'corpu': 1.6532125137753437, 'linguist': 0.9542425094393249, 'underli': 1.6532125137753437, 'machine-learn': 1.6532125137753437, 'approach': 0.6118198286171186, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, '.': 0.019744058195757187}, '[7]\\nIn 2003, wo': {'[': 0.47712125471966244, '7': 1.6532125137753437, ']': 0.47712125471966244, '2003': 1.6532125137753437, ',': 0.10914446942506807, 'word': 1.1760912590556813, 'n-gram': 1.6532125137753437, 'model': 0.8750612633917001, 'time': 1.3521825181113625, 'best': 1.6532125137753437, 'statist': 0.7501225267834001, 'algorithm': 1.1760912590556813, 'wa': 0.7501225267834001, 'overperform': 1.6532125137753437, 'multi-lay': 1.6532125137753437, 'perceptron': 1.6532125137753437, '(': 0.6118198286171186, 'singl': 1.3521825181113625, 'hidden': 1.0511525224473812, 'layer': 1.1760912590556813, 'context': 1.3521825181113625, 'length': 1.6532125137753437, 'sever': 1.6532125137753437, 'train': 1.6532125137753437, '14': 1.3521825181113625, 'million': 1.6532125137753437, 'cpu': 1.6532125137753437, 'cluster': 1.6532125137753437, 'languag': 0.3744589128225147, ')': 0.5740312677277188, 'yoshua': 1.6532125137753437, 'bengio': 1.6532125137753437, 'co-author': 1.3521825181113625, '.': 0.019744058195757187}, '[8]\\nIn 2010, To': {'[': 0.47712125471966244, '8': 1.6532125137753437, ']': 0.47712125471966244, '2010': 1.3521825181113625, ',': 0.10914446942506807, 'tomáš': 1.6532125137753437, 'mikolov': 1.6532125137753437, '(': 0.6118198286171186, 'phd': 1.6532125137753437, 'student': 1.6532125137753437, 'brno': 1.6532125137753437, 'univers': 1.6532125137753437, 'technolog': 1.1760912590556813, ')': 0.5740312677277188, 'co-author': 1.3521825181113625, 'appli': 1.1760912590556813, 'simpl': 1.6532125137753437, 'recurr': 1.6532125137753437, 'neural': 0.8081144737610869, 'network': 1.1760912590556813, 'singl': 1.3521825181113625, 'hidden': 1.0511525224473812, 'layer': 1.1760912590556813, 'languag': 0.3744589128225147, 'model': 0.8750612633917001, '9': 1.6532125137753437, 'follow': 1.3521825181113625, 'year': 1.6532125137753437, 'went': 1.6532125137753437, 'develop': 1.1760912590556813, 'word2vec': 1.6532125137753437, '.': 0.019744058195757187}, 'In the 2010s, r': {'2010': 1.3521825181113625, ',': 0.10914446942506807, 'represent': 1.6532125137753437, 'learn': 1.0511525224473812, 'deep': 1.6532125137753437, 'neural': 0.8081144737610869, 'network-styl': 1.6532125137753437, '(': 0.6118198286171186, 'featur': 1.3521825181113625, 'mani': 1.1760912590556813, 'hidden': 1.0511525224473812, 'layer': 1.1760912590556813, ')': 0.5740312677277188, 'machin': 0.9542425094393249, 'method': 1.3521825181113625, 'becam': 1.6532125137753437, 'widespread': 1.6532125137753437, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, '.': 0.019744058195757187}, 'That popularity': {'popular': 1.6532125137753437, 'wa': 0.7501225267834001, 'due': 1.3521825181113625, 'partli': 1.6532125137753437, 'flurri': 1.6532125137753437, 'result': 1.6532125137753437, 'show': 1.6532125137753437, 'techniqu': 1.6532125137753437, '[': 0.47712125471966244, '10': 1.6532125137753437, ']': 0.47712125471966244, '11': 1.6532125137753437, 'achiev': 1.3521825181113625, 'state-of-the-art': 1.6532125137753437, 'mani': 1.1760912590556813, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'task': 0.6989700043360189, ',': 0.10914446942506807, 'e.g.': 0.9542425094393249, 'model': 0.8750612633917001, '12': 1.6532125137753437, 'pars': 1.3521825181113625, '.': 0.019744058195757187}, '[13][14] This i': {'[': 0.47712125471966244, '13': 1.6532125137753437, ']': 0.47712125471966244, '14': 1.3521825181113625, 'thi': 1.3521825181113625, 'increasingli': 1.3521825181113625, 'import': 1.6532125137753437, 'medicin': 1.6532125137753437, 'healthcar': 1.6532125137753437, ',': 0.10914446942506807, 'nlp': 0.6118198286171186, 'help': 1.6532125137753437, 'analyz': 1.6532125137753437, 'note': 1.6532125137753437, 'text': 1.3521825181113625, 'electron': 1.6532125137753437, 'health': 1.6532125137753437, 'record': 1.6532125137753437, 'would': 1.6532125137753437, 'otherwis': 1.6532125137753437, 'inaccess': 1.6532125137753437, 'studi': 1.1760912590556813, 'seek': 1.6532125137753437, 'improv': 1.6532125137753437, 'care': 1.6532125137753437, '15': 1.6532125137753437, 'protect': 1.6532125137753437, 'patient': 1.6532125137753437, 'privaci': 1.6532125137753437, '.': 0.019744058195757187}, '[16]\\nSymbolic a': {'[': 0.47712125471966244, '16': 1.6532125137753437, ']': 0.47712125471966244, 'symbol': 1.0511525224473812, 'approach': 0.6118198286171186, ',': 0.10914446942506807, 'i.e.': 1.6532125137753437, 'hand-cod': 1.6532125137753437, 'set': 1.3521825181113625, 'rule': 1.0511525224473812, 'manipul': 1.1760912590556813, 'coupl': 1.6532125137753437, 'dictionari': 1.6532125137753437, 'lookup': 1.6532125137753437, 'wa': 0.7501225267834001, 'histor': 1.3521825181113625, 'first': 1.6532125137753437, 'use': 0.8081144737610869, 'ai': 1.1760912590556813, 'gener': 1.1760912590556813, 'nlp': 0.6118198286171186, 'particular': 1.6532125137753437, ':': 0.8750612633917001, '17': 1.6532125137753437, '18': 1.6532125137753437, 'write': 1.6532125137753437, 'grammar': 1.1760912590556813, 'devis': 1.6532125137753437, 'heurist': 1.6532125137753437, 'stem': 1.6532125137753437, '.': 0.019744058195757187}, 'Machine learnin': {'machin': 0.9542425094393249, 'learn': 1.0511525224473812, 'approach': 0.6118198286171186, ',': 0.10914446942506807, 'includ': 1.1760912590556813, 'statist': 0.7501225267834001, 'neural': 0.8081144737610869, 'network': 1.1760912590556813, 'hand': 1.6532125137753437, 'mani': 1.1760912590556813, 'advantag': 1.6532125137753437, 'symbol': 1.0511525224473812, ':': 0.8750612633917001, 'although': 1.3521825181113625, 'rule-bas': 0.9542425094393249, 'system': 1.1760912590556813, 'manipul': 1.1760912590556813, 'still': 1.3521825181113625, 'use': 0.8081144737610869, '2020': 1.3521825181113625, 'becom': 1.6532125137753437, 'mostli': 1.6532125137753437, 'obsolet': 1.3521825181113625, 'advanc': 1.3521825181113625, 'llm': 1.6532125137753437, '2023': 1.6532125137753437, '.': 0.019744058195757187}, 'Before that the': {'befor': 1.6532125137753437, 'commonli': 1.1760912590556813, 'use': 0.8081144737610869, ':': 0.8750612633917001, 'late': 1.3521825181113625, '1980': 1.1760912590556813, 'mid-1990': 1.6532125137753437, ',': 0.10914446942506807, 'statist': 0.7501225267834001, 'approach': 0.6118198286171186, 'end': 1.3521825181113625, 'period': 1.6532125137753437, 'ai': 1.1760912590556813, 'winter': 1.6532125137753437, 'wa': 0.7501225267834001, 'caus': 1.6532125137753437, 'ineffici': 1.6532125137753437, 'rule-bas': 0.9542425094393249, '.': 0.019744058195757187}, '[19][20]\\nThe ea': {'[': 0.47712125471966244, '19': 1.6532125137753437, ']': 0.47712125471966244, '20': 1.6532125137753437, 'earliest': 1.6532125137753437, 'decis': 1.6532125137753437, 'tree': 1.6532125137753437, ',': 0.10914446942506807, 'produc': 1.6532125137753437, 'system': 1.1760912590556813, 'hard': 1.6532125137753437, 'if–then': 1.6532125137753437, 'rule': 1.0511525224473812, 'still': 1.3521825181113625, 'veri': 1.6532125137753437, 'similar': 1.6532125137753437, 'old': 1.3521825181113625, 'rule-bas': 0.9542425094393249, 'approach': 0.6118198286171186, '.': 0.019744058195757187}, 'Only the introd': {'onli': 1.6532125137753437, 'introduct': 1.3521825181113625, 'hidden': 1.0511525224473812, 'markov': 1.6532125137753437, 'model': 0.8750612633917001, ',': 0.10914446942506807, 'appli': 1.1760912590556813, 'part-of-speech': 1.3521825181113625, 'tag': 1.3521825181113625, 'announc': 1.6532125137753437, 'end': 1.3521825181113625, 'old': 1.3521825181113625, 'rule-bas': 0.9542425094393249, 'approach': 0.6118198286171186, '.': 0.019744058195757187}, 'A major drawbac': {'major': 1.3521825181113625, 'drawback': 1.6532125137753437, 'statist': 0.7501225267834001, 'method': 1.3521825181113625, 'requir': 1.6532125137753437, 'elabor': 1.6532125137753437, 'featur': 1.3521825181113625, 'engin': 1.6532125137753437, '.': 0.019744058195757187}, 'Since 2015,[21]': {'sinc': 1.3521825181113625, '2015': 1.6532125137753437, ',': 0.10914446942506807, '[': 0.47712125471966244, '21': 1.6532125137753437, ']': 0.47712125471966244, 'statist': 0.7501225267834001, 'approach': 0.6118198286171186, 'wa': 0.7501225267834001, 'replac': 1.6532125137753437, 'neural': 0.8081144737610869, 'network': 1.1760912590556813, 'use': 0.8081144737610869, 'word': 1.1760912590556813, 'embed': 1.6532125137753437, 'captur': 1.6532125137753437, 'semant': 1.6532125137753437, 'properti': 1.6532125137753437, '.': 0.019744058195757187}, 'Intermediate ta': {'intermedi': 1.3521825181113625, 'task': 0.6989700043360189, '(': 0.6118198286171186, 'e.g.': 0.9542425094393249, ',': 0.10914446942506807, 'part-of-speech': 1.3521825181113625, 'tag': 1.3521825181113625, 'depend': 1.6532125137753437, 'pars': 1.3521825181113625, ')': 0.5740312677277188, 'need': 1.6532125137753437, 'anymor': 1.6532125137753437, '.': 0.019744058195757187}, 'Neural machine ': {'neural': 0.8081144737610869, 'machin': 0.9542425094393249, 'translat': 1.6532125137753437, ',': 0.10914446942506807, 'base': 1.1760912590556813, 'then-newly-inv': 1.6532125137753437, 'sequence-to-sequ': 1.6532125137753437, 'transform': 1.3521825181113625, 'made': 1.3521825181113625, 'obsolet': 1.3521825181113625, 'intermedi': 1.3521825181113625, 'step': 1.6532125137753437, 'word': 1.1760912590556813, 'align': 1.6532125137753437, 'previous': 1.6532125137753437, 'necessari': 1.6532125137753437, 'statist': 0.7501225267834001, '.': 0.019744058195757187}, 'The following i': {'follow': 1.3521825181113625, 'list': 1.6532125137753437, 'commonli': 1.1760912590556813, 'research': 1.3521825181113625, 'task': 0.6989700043360189, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, '.': 0.019744058195757187}, 'Some of these t': {'task': 0.6989700043360189, 'direct': 1.3521825181113625, 'real-world': 1.6532125137753437, 'applic': 1.3521825181113625, ',': 0.10914446942506807, 'commonli': 1.1760912590556813, 'serv': 1.6532125137753437, 'subtask': 1.6532125137753437, 'use': 0.8081144737610869, 'aid': 1.6532125137753437, 'solv': 1.6532125137753437, 'larger': 1.6532125137753437, '.': 0.019744058195757187}, 'Though natural ': {'though': 1.3521825181113625, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, 'task': 0.6989700043360189, 'close': 1.6532125137753437, 'intertwin': 1.6532125137753437, ',': 0.10914446942506807, 'subdivid': 1.6532125137753437, 'categori': 1.6532125137753437, 'conveni': 1.6532125137753437, '.': 0.019744058195757187}, 'A coarse divisi': {'coars': 1.6532125137753437, 'divis': 1.6532125137753437, 'given': 1.3521825181113625, '.': 0.019744058195757187}, 'Based on long-s': {'base': 1.1760912590556813, 'long-stand': 1.3521825181113625, 'trend': 1.1760912590556813, 'field': 1.6532125137753437, ',': 0.10914446942506807, 'possibl': 1.6532125137753437, 'extrapol': 1.6532125137753437, 'futur': 1.6532125137753437, 'direct': 1.3521825181113625, 'nlp': 0.6118198286171186, '.': 0.019744058195757187}, 'As of 2020, thr': {'2020': 1.3521825181113625, ',': 0.10914446942506807, 'three': 1.6532125137753437, 'trend': 1.1760912590556813, 'among': 1.3521825181113625, 'topic': 1.6532125137753437, 'long-stand': 1.3521825181113625, 'seri': 1.6532125137753437, 'conll': 1.3521825181113625, 'share': 1.3521825181113625, 'task': 0.6989700043360189, 'observ': 1.6532125137753437, ':': 0.8750612633917001, '[': 0.47712125471966244, '43': 1.6532125137753437, ']': 0.47712125471966244, 'higher-level': 1.6532125137753437, 'nlp': 0.6118198286171186, 'applic': 1.3521825181113625, 'involv': 1.0511525224473812, 'aspect': 1.1760912590556813, 'emul': 1.3521825181113625, 'intellig': 1.1760912590556813, 'behaviour': 1.3521825181113625, 'appar': 1.6532125137753437, 'comprehens': 1.6532125137753437, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, '.': 0.019744058195757187}, 'More broadly sp': {'broadli': 1.6532125137753437, 'speak': 1.6532125137753437, ',': 0.10914446942506807, 'technic': 1.3521825181113625, 'operation': 1.6532125137753437, 'increasingli': 1.3521825181113625, 'advanc': 1.3521825181113625, 'aspect': 1.1760912590556813, 'cognit': 0.6989700043360189, 'behaviour': 1.3521825181113625, 'repres': 1.6532125137753437, 'one': 1.6532125137753437, 'development': 1.6532125137753437, 'trajectori': 1.6532125137753437, 'nlp': 0.6118198286171186, '(': 0.6118198286171186, 'see': 1.3521825181113625, 'trend': 1.1760912590556813, 'among': 1.3521825181113625, 'conll': 1.3521825181113625, 'share': 1.3521825181113625, 'task': 0.6989700043360189, 'abov': 1.6532125137753437, ')': 0.5740312677277188, '.': 0.019744058195757187}, 'Cognition refer': {'cognit': 0.6989700043360189, 'refer': 1.6532125137753437, '``': 0.9542425094393249, 'mental': 1.6532125137753437, 'action': 1.6532125137753437, 'process': 0.5740312677277188, 'acquir': 1.6532125137753437, 'knowledg': 1.3521825181113625, 'understand': 1.0511525224473812, 'thought': 1.6532125137753437, ',': 0.10914446942506807, 'experi': 1.3521825181113625, 'sens': 1.6532125137753437, '.': 0.019744058195757187}, '\"[44] Cognitive': {'``': 0.9542425094393249, '[': 0.47712125471966244, '44': 1.6532125137753437, ']': 0.47712125471966244, 'cognit': 0.6989700043360189, 'scienc': 1.3521825181113625, 'interdisciplinari': 1.3521825181113625, ',': 0.10914446942506807, 'scientif': 1.6532125137753437, 'studi': 1.1760912590556813, 'mind': 1.6532125137753437, 'process': 0.5740312677277188, '.': 0.019744058195757187}, '[45] Cognitive ': {'[': 0.47712125471966244, '45': 1.6532125137753437, ']': 0.47712125471966244, 'cognit': 0.6989700043360189, 'linguist': 0.9542425094393249, 'interdisciplinari': 1.3521825181113625, 'branch': 1.6532125137753437, ',': 0.10914446942506807, 'combin': 1.6532125137753437, 'knowledg': 1.3521825181113625, 'research': 1.3521825181113625, 'psycholog': 1.6532125137753437, '.': 0.019744058195757187}, '[46] Especially': {'[': 0.47712125471966244, '46': 1.6532125137753437, ']': 0.47712125471966244, 'especi': 1.6532125137753437, 'dure': 1.3521825181113625, 'age': 1.6532125137753437, 'symbol': 1.0511525224473812, 'nlp': 0.6118198286171186, ',': 0.10914446942506807, 'area': 1.6532125137753437, 'comput': 0.8081144737610869, 'linguist': 0.9542425094393249, 'maintain': 1.6532125137753437, 'strong': 1.6532125137753437, 'tie': 1.3521825181113625, 'cognit': 0.6989700043360189, 'studi': 1.1760912590556813, '.': 0.019744058195757187}, 'As an example, ': {'exampl': 1.6532125137753437, ',': 0.10914446942506807, 'georg': 1.6532125137753437, 'lakoff': 1.6532125137753437, 'offer': 1.6532125137753437, 'methodolog': 1.6532125137753437, 'build': 1.6532125137753437, 'natur': 0.5392691614685069, 'languag': 0.3744589128225147, 'process': 0.5740312677277188, '(': 0.6118198286171186, 'nlp': 0.6118198286171186, ')': 0.5740312677277188, 'algorithm': 1.1760912590556813, 'perspect': 1.6532125137753437, 'cognit': 0.6989700043360189, 'scienc': 1.3521825181113625, 'along': 1.6532125137753437, 'find': 1.6532125137753437, 'linguist': 0.9542425094393249, '[': 0.47712125471966244, '47': 1.6532125137753437, ']': 0.47712125471966244, 'two': 1.6532125137753437, 'defin': 1.6532125137753437, 'aspect': 1.1760912590556813, ':': 0.8750612633917001, 'tie': 1.3521825181113625, 'part': 1.6532125137753437, 'histor': 1.3521825181113625, 'heritag': 1.6532125137753437, 'less': 1.6532125137753437, 'frequent': 1.3521825181113625, 'address': 1.6532125137753437, 'sinc': 1.3521825181113625, 'statist': 0.7501225267834001, 'turn': 1.6532125137753437, 'dure': 1.3521825181113625, '1990': 1.6532125137753437, '.': 0.019744058195757187}, 'Nevertheless, a': {'nevertheless': 1.6532125137753437, ',': 0.10914446942506807, 'approach': 0.6118198286171186, 'develop': 1.1760912590556813, 'cognit': 0.6989700043360189, 'model': 0.8750612633917001, 'toward': 1.6532125137753437, 'technic': 1.3521825181113625, 'operationaliz': 1.6532125137753437, 'framework': 1.6532125137753437, 'pursu': 1.6532125137753437, 'context': 1.3521825181113625, 'variou': 1.6532125137753437, 'e.g.': 0.9542425094393249, 'grammar': 1.1760912590556813, '[': 0.47712125471966244, '49': 1.6532125137753437, ']': 0.47712125471966244, 'function': 1.6532125137753437, '50': 1.6532125137753437, 'construct': 1.6532125137753437, '51': 1.6532125137753437, 'comput': 0.8081144737610869, 'psycholinguist': 1.6532125137753437, 'neurosci': 1.6532125137753437, '(': 0.6118198286171186, 'act-r': 1.6532125137753437, ')': 0.5740312677277188, 'howev': 1.3521825181113625, 'limit': 1.6532125137753437, 'uptak': 1.6532125137753437, 'mainstream': 1.6532125137753437, 'nlp': 0.6118198286171186, 'measur': 1.6532125137753437, 'presenc': 1.6532125137753437, 'major': 1.3521825181113625, 'confer': 1.6532125137753437, '52': 1.6532125137753437, 'acl': 1.6532125137753437, '.': 0.019744058195757187}, 'More recently, ': {'recent': 1.3521825181113625, ',': 0.10914446942506807, 'idea': 1.3521825181113625, 'cognit': 0.6989700043360189, 'nlp': 0.6118198286171186, 'reviv': 1.6532125137753437, 'approach': 0.6118198286171186, 'achiev': 1.3521825181113625, 'explain': 1.6532125137753437, 'e.g.': 0.9542425094393249, 'notion': 1.6532125137753437, '``': 0.9542425094393249, 'ai': 1.1760912590556813, \"''\": 1.1760912590556813, '.': 0.019744058195757187}, '[53] Likewise, ': {'[': 0.47712125471966244, '53': 1.6532125137753437, ']': 0.47712125471966244, 'likewis': 1.6532125137753437, ',': 0.10914446942506807, 'idea': 1.3521825181113625, 'cognit': 0.6989700043360189, 'nlp': 0.6118198286171186, 'inher': 1.6532125137753437, 'neural': 0.8081144737610869, 'model': 0.8750612633917001, 'multimod': 1.6532125137753437, '(': 0.6118198286171186, 'although': 1.3521825181113625, 'rare': 1.6532125137753437, 'made': 1.3521825181113625, 'explicit': 1.6532125137753437, ')': 0.5740312677277188, '54': 1.6532125137753437, 'develop': 1.1760912590556813, 'artifici': 1.3521825181113625, 'intellig': 1.1760912590556813, 'specif': 1.6532125137753437, 'tool': 1.6532125137753437, 'technolog': 1.1760912590556813, 'use': 0.8081144737610869, 'larg': 1.6532125137753437, 'languag': 0.3744589128225147, 'approach': 0.6118198286171186, '.': 0.019744058195757187}, '[55]': {'[': 0.47712125471966244, '55': 1.6532125137753437, ']': 0.47712125471966244}}\n",
      "{'Natural languag': {'natur': 0.07703845163835812, 'languag': 0.053494130403216385, 'process': 0.08200446681824554, 'ha': 0.23617321625362053, 'root': 0.23617321625362053, '1950': 0.19316893115876604, '.': 0.0028205797422510264}, 'It is primarily': {'primarili': 0.1653212513775344, 'concern': 0.1653212513775344, 'give': 0.1653212513775344, 'comput': 0.0808114473761087, 'abil': 0.1653212513775344, 'support': 0.1653212513775344, 'manipul': 0.11760912590556814, 'human': 0.1653212513775344, 'languag': 0.037445891282251474, '.': 0.0019744058195757187}, 'It involves pro': {'involv': 0.06569703265296133, 'process': 0.03587695423298243, 'natur': 0.03370432259178168, 'languag': 0.02340368205140717, 'dataset': 0.10332578211095898, ',': 0.013643058678133509, 'text': 0.08451140738196015, 'corpora': 0.20665156422191797, 'speech': 0.08451140738196015, 'use': 0.05050715461006793, 'either': 0.10332578211095898, 'rule-bas': 0.059640156839957804, 'probabilist': 0.10332578211095898, '(': 0.03823873928856991, 'i.e': 0.10332578211095898, '.': 0.0012340036372348242}, 'statistical and': {'statist': 0.07501225267834001, ',': 0.021828893885013616, 'recent': 0.13521825181113625, 'neural': 0.0808114473761087, 'network-bas': 0.1653212513775344, ')': 0.05740312677277189, 'machin': 0.09542425094393249, 'learn': 0.10511525224473812, 'approach': 0.061181982861711864, '.': 0.0019744058195757187}, 'The goal is a c': {'goal': 0.1102141675850229, 'comput': 0.053874298250739125, 'capabl': 0.1102141675850229, '``': 0.06361616729595498, 'understand': 0.07007683482982541, \"''\": 0.07840608393704543, 'content': 0.1102141675850229, 'document': 0.09014550120742416, ',': 0.007276297961671205, 'includ': 0.07840608393704543, 'contextu': 0.1102141675850229, 'nuanc': 0.1102141675850229, 'languag': 0.024963927521500978, 'within': 0.1102141675850229, '.': 0.0013162705463838124}, 'The technology ': {'technolog': 0.09800760492130678, 'accur': 0.13776770948127864, 'extract': 0.13776770948127864, 'inform': 0.13776770948127864, 'insight': 0.13776770948127864, 'contain': 0.13776770948127864, 'document': 0.2253637530185604, 'well': 0.13776770948127864, 'categor': 0.13776770948127864, 'organ': 0.13776770948127864, 'themselv': 0.13776770948127864, '.': 0.0016453381829797655}, 'Challenges in n': {'challeng': 0.12717019336733415, 'natur': 0.04148224318988515, 'languag': 0.028804531755578056, 'process': 0.044156251363670684, 'frequent': 0.1040140398547202, 'involv': 0.08085788634210625, 'speech': 0.1040140398547202, 'recognit': 0.12717019336733415, ',': 0.016791456834625858, 'natural-languag': 0.2543403867346683, 'understand': 0.08085788634210625, 'gener': 0.09046855838889857, '.': 0.0015187737073659375}, 'Already in 1950': {'alreadi': 0.06612850055101376, '1950': 0.0540873007244545, ',': 0.008731557554005447, 'alan': 0.06612850055101376, 'ture': 0.1322570011020275, 'publish': 0.06612850055101376, 'articl': 0.06612850055101376, 'titl': 0.06612850055101376, '``': 0.038169700377572995, 'comput': 0.03232457895044348, 'machineri': 0.06612850055101376, 'intellig': 0.14113095108668175, \"''\": 0.047043650362227254, 'propos': 0.0540873007244545, 'call': 0.06612850055101376, 'test': 0.0540873007244545, 'criterion': 0.06612850055101376, 'though': 0.0540873007244545, 'time': 0.0540873007244545, 'wa': 0.030004901071336004, 'articul': 0.06612850055101376, 'problem': 0.06612850055101376, 'separ': 0.06612850055101376, 'artifici': 0.0540873007244545, '.': 0.0007897623278302875}, 'The proposed te': {'propos': 0.12292568346466932, 'test': 0.12292568346466932, 'includ': 0.10691738718688013, 'task': 0.06354272766691081, 'involv': 0.09555932022248921, 'autom': 0.15029204670684942, 'interpret': 0.15029204670684942, 'gener': 0.10691738718688013, 'natur': 0.04902446922440972, 'languag': 0.03404171934750134, '.': 0.0017949143814324716}, 'The premise of ': {'premis': 0.05166289105547949, 'symbol': 0.03284851632648066, 'nlp': 0.03823873928856991, 'well-summar': 0.05166289105547949, 'john': 0.05166289105547949, 'searl': 0.05166289105547949, \"'s\": 0.04225570369098008, 'chines': 0.10332578211095898, 'room': 0.05166289105547949, 'experi': 0.04225570369098008, ':': 0.027345664480990628, 'given': 0.04225570369098008, 'collect': 0.05166289105547949, 'rule': 0.06569703265296133, '(': 0.03823873928856991, 'e.g.': 0.029820078419978902, ',': 0.010232294008600132, 'phrasebook': 0.05166289105547949, 'question': 0.05166289105547949, 'match': 0.05166289105547949, 'answer': 0.05166289105547949, ')': 0.03587695423298243, 'comput': 0.025253577305033966, 'emul': 0.04225570369098008, 'natur': 0.01685216129589084, 'languag': 0.011701841025703584, 'understand': 0.03284851632648066, 'task': 0.02184281263550059, 'appli': 0.03675285184549004, 'data': 0.05166289105547949, 'confront': 0.05166289105547949, '.': 0.0006170018186174121}, 'Up to the 1980s': {'1980': 0.09800760492130678, ',': 0.009095372452089006, 'natur': 0.04493909678904224, 'languag': 0.031204909401876223, 'process': 0.0478359389773099, 'system': 0.09800760492130678, 'base': 0.09800760492130678, 'complex': 0.13776770948127864, 'set': 0.1126818765092802, 'hand-written': 0.13776770948127864, 'rule': 0.08759604353728176, '.': 0.0016453381829797655}, 'Starting in the': {'start': 0.1102141675850229, 'late': 0.09014550120742416, '1980': 0.07840608393704543, ',': 0.01455259592334241, 'howev': 0.09014550120742416, 'wa': 0.05000816845222667, 'revolut': 0.1102141675850229, 'natur': 0.035951277431233795, 'languag': 0.049927855043001956, 'process': 0.07653750236369584, 'introduct': 0.09014550120742416, 'machin': 0.06361616729595498, 'learn': 0.07007683482982541, 'algorithm': 0.07840608393704543, '.': 0.0013162705463838124}, 'This was due to': {'thi': 0.06438964371958869, 'wa': 0.03572012032301905, 'due': 0.06438964371958869, 'steadi': 0.07872440541787351, 'increas': 0.07872440541787351, 'comput': 0.038481641607670806, 'power': 0.07872440541787351, '(': 0.05826855510639224, 'see': 0.06438964371958869, 'moor': 0.07872440541787351, \"'s\": 0.06438964371958869, 'law': 0.07872440541787351, ')': 0.027334822272748516, 'gradual': 0.07872440541787351, 'lessen': 0.07872440541787351, 'domin': 0.07872440541787351, 'chomskyan': 0.07872440541787351, 'theori': 0.07872440541787351, 'linguist': 0.04544011949711071, 'e.g': 0.07872440541787351, '.': 0.0009401932474170088}, 'transformationa': {'transform': 0.0795401481241978, 'grammar': 0.06918183876798126, ')': 0.03376654516045405, ',': 0.006420262907356945, 'whose': 0.0972477949279614, 'theoret': 0.0972477949279614, 'underpin': 0.0972477949279614, 'discourag': 0.0972477949279614, 'sort': 0.0972477949279614, 'corpu': 0.0972477949279614, 'linguist': 0.056131912319960287, 'underli': 0.0972477949279614, 'machine-learn': 0.0972477949279614, 'approach': 0.035989401683359915, 'languag': 0.02202699487191263, 'process': 0.03376654516045405, '.': 0.001161415187985717}, '[7]\\nIn 2003, wo': {'[': 0.014032978079990072, '7': 0.0486238974639807, ']': 0.014032978079990072, '2003': 0.0486238974639807, ',': 0.009630394361035419, 'word': 0.10377275815197189, 'n-gram': 0.0486238974639807, 'model': 0.05147419196421765, 'time': 0.0397700740620989, 'best': 0.0486238974639807, 'statist': 0.022062427258335297, 'algorithm': 0.03459091938399063, 'wa': 0.022062427258335297, 'overperform': 0.0486238974639807, 'multi-lay': 0.0486238974639807, 'perceptron': 0.0486238974639807, '(': 0.017994700841679957, 'singl': 0.0397700740620989, 'hidden': 0.030916250660217096, 'layer': 0.03459091938399063, 'context': 0.0397700740620989, 'length': 0.0486238974639807, 'sever': 0.0486238974639807, 'train': 0.0486238974639807, '14': 0.0397700740620989, 'million': 0.0486238974639807, 'cpu': 0.0486238974639807, 'cluster': 0.0486238974639807, 'languag': 0.011013497435956315, ')': 0.016883272580227024, 'yoshua': 0.0486238974639807, 'bengio': 0.0486238974639807, 'co-author': 0.0397700740620989, '.': 0.0005807075939928584}, '[8]\\nIn 2010, To': {'[': 0.029820078419978902, '8': 0.05166289105547949, ']': 0.029820078419978902, '2010': 0.04225570369098008, ',': 0.0068215293390667545, 'tomáš': 0.05166289105547949, 'mikolov': 0.05166289105547949, '(': 0.019119369644284956, 'phd': 0.05166289105547949, 'student': 0.05166289105547949, 'brno': 0.05166289105547949, 'univers': 0.05166289105547949, 'technolog': 0.03675285184549004, ')': 0.017938477116491214, 'co-author': 0.04225570369098008, 'appli': 0.03675285184549004, 'simpl': 0.05166289105547949, 'recurr': 0.05166289105547949, 'neural': 0.025253577305033966, 'network': 0.03675285184549004, 'singl': 0.04225570369098008, 'hidden': 0.03284851632648066, 'layer': 0.03675285184549004, 'languag': 0.011701841025703584, 'model': 0.027345664480990628, '9': 0.05166289105547949, 'follow': 0.04225570369098008, 'year': 0.05166289105547949, 'went': 0.05166289105547949, 'develop': 0.03675285184549004, 'word2vec': 0.05166289105547949, '.': 0.0006170018186174121}, 'In the 2010s, r': {'2010': 0.06438964371958869, ',': 0.005197355686908003, 'represent': 0.07872440541787351, 'learn': 0.10010976404260773, 'deep': 0.07872440541787351, 'neural': 0.038481641607670806, 'network-styl': 0.07872440541787351, '(': 0.02913427755319612, 'featur': 0.06438964371958869, 'mani': 0.056004345669318153, 'hidden': 0.05005488202130386, 'layer': 0.056004345669318153, ')': 0.027334822272748516, 'machin': 0.04544011949711071, 'method': 0.06438964371958869, 'becam': 0.07872440541787351, 'widespread': 0.07872440541787351, 'natur': 0.025679483879452708, 'languag': 0.017831376801072128, 'process': 0.027334822272748516, '.': 0.0009401932474170088}, 'That popularity': {'popular': 0.06888385474063932, 'wa': 0.031255105282641665, 'due': 0.0563409382546401, 'partli': 0.06888385474063932, 'flurri': 0.06888385474063932, 'result': 0.13776770948127864, 'show': 0.06888385474063932, 'techniqu': 0.06888385474063932, '[': 0.059640156839957804, '10': 0.06888385474063932, ']': 0.059640156839957804, '11': 0.06888385474063932, 'achiev': 0.0563409382546401, 'state-of-the-art': 0.06888385474063932, 'mani': 0.04900380246065339, 'natur': 0.02246954839452112, 'languag': 0.031204909401876223, 'task': 0.02912375018066745, ',': 0.009095372452089006, 'e.g.': 0.039760104559971865, 'model': 0.03646088597465417, '12': 0.06888385474063932, 'pars': 0.0563409382546401, '.': 0.0008226690914898828}, '[13][14] This i': {'[': 0.047712125471966245, '13': 0.05510708379251145, ']': 0.047712125471966245, '14': 0.04507275060371208, 'thi': 0.04507275060371208, 'increasingli': 0.04507275060371208, 'import': 0.05510708379251145, 'medicin': 0.05510708379251145, 'healthcar': 0.05510708379251145, ',': 0.0036381489808356023, 'nlp': 0.020393994287237285, 'help': 0.05510708379251145, 'analyz': 0.05510708379251145, 'note': 0.05510708379251145, 'text': 0.04507275060371208, 'electron': 0.05510708379251145, 'health': 0.05510708379251145, 'record': 0.05510708379251145, 'would': 0.05510708379251145, 'otherwis': 0.05510708379251145, 'inaccess': 0.05510708379251145, 'studi': 0.039203041968522714, 'seek': 0.05510708379251145, 'improv': 0.05510708379251145, 'care': 0.05510708379251145, '15': 0.05510708379251145, 'protect': 0.05510708379251145, 'patient': 0.05510708379251145, 'privaci': 0.05510708379251145, '.': 0.0006581352731919062}, '[16]\\nSymbolic a': {'[': 0.04617302465028991, '16': 0.05332943592823689, ']': 0.04617302465028991, 'symbol': 0.06781629177079879, 'approach': 0.039472247007556034, ',': 0.014083157345170074, 'i.e.': 0.05332943592823689, 'hand-cod': 0.05332943592823689, 'set': 0.043618790906818146, 'rule': 0.06781629177079879, 'manipul': 0.03793842771147359, 'coupl': 0.05332943592823689, 'dictionari': 0.05332943592823689, 'lookup': 0.05332943592823689, 'wa': 0.024197500863980648, 'histor': 0.043618790906818146, 'first': 0.05332943592823689, 'use': 0.026068208831002804, 'ai': 0.03793842771147359, 'gener': 0.03793842771147359, 'nlp': 0.019736123503778017, 'particular': 0.05332943592823689, ':': 0.02822778269005484, '17': 0.05332943592823689, '18': 0.05332943592823689, 'write': 0.05332943592823689, 'grammar': 0.03793842771147359, 'devis': 0.05332943592823689, 'heurist': 0.05332943592823689, 'stem': 0.05332943592823689, '.': 0.0006369051030889415}, 'Machine learnin': {'machin': 0.035342315164419436, 'learn': 0.03893157490545856, 'approach': 0.04531998730497174, ',': 0.01616955102593601, 'includ': 0.04355893552058079, 'statist': 0.027782315806792593, 'neural': 0.02993016569485507, 'network': 0.04355893552058079, 'hand': 0.061230093102790505, 'mani': 0.04355893552058079, 'advantag': 0.061230093102790505, 'symbol': 0.07786314981091712, ':': 0.032409676421914814, 'although': 0.05008083400412453, 'rule-bas': 0.035342315164419436, 'system': 0.04355893552058079, 'manipul': 0.04355893552058079, 'still': 0.05008083400412453, 'use': 0.02993016569485507, '2020': 0.05008083400412453, 'becom': 0.061230093102790505, 'mostli': 0.061230093102790505, 'obsolet': 0.05008083400412453, 'advanc': 0.05008083400412453, 'llm': 0.061230093102790505, '2023': 0.061230093102790505, '.': 0.0007312614146576735}, 'Before that the': {'befor': 0.0870111849355444, 'commonli': 0.061899539950299017, 'use': 0.04253234072426773, ':': 0.04605585596798421, 'late': 0.0711675009532296, '1980': 0.061899539950299017, 'mid-1990': 0.0870111849355444, ',': 0.011488891518428218, 'statist': 0.0394801329886, 'approach': 0.06440208722285458, 'end': 0.0711675009532296, 'period': 0.0870111849355444, 'ai': 0.061899539950299017, 'winter': 0.0870111849355444, 'wa': 0.0394801329886, 'caus': 0.0870111849355444, 'ineffici': 0.0870111849355444, 'rule-bas': 0.05022328997049078, '.': 0.0010391609576714308}, '[19][20]\\nThe ea': {'[': 0.047712125471966245, '19': 0.0826606256887672, ']': 0.047712125471966245, '20': 0.0826606256887672, 'earliest': 0.0826606256887672, 'decis': 0.0826606256887672, 'tree': 0.0826606256887672, ',': 0.010914446942506808, 'produc': 0.0826606256887672, 'system': 0.05880456295278407, 'hard': 0.0826606256887672, 'if–then': 0.0826606256887672, 'rule': 0.05255762612236906, 'still': 0.06760912590556813, 'veri': 0.0826606256887672, 'similar': 0.0826606256887672, 'old': 0.06760912590556813, 'rule-bas': 0.047712125471966245, 'approach': 0.030590991430855932, '.': 0.0009872029097878594}, 'Only the introd': {'onli': 0.1102141675850229, 'introduct': 0.09014550120742416, 'hidden': 0.07007683482982541, 'markov': 0.1102141675850229, 'model': 0.05833741755944667, ',': 0.01455259592334241, 'appli': 0.07840608393704543, 'part-of-speech': 0.09014550120742416, 'tag': 0.09014550120742416, 'announc': 0.1102141675850229, 'end': 0.09014550120742416, 'old': 0.09014550120742416, 'rule-bas': 0.06361616729595498, 'approach': 0.04078798857447457, '.': 0.0013162705463838124}, 'A major drawbac': {'major': 0.1502425020123736, 'drawback': 0.1836902793083715, 'statist': 0.08334694742037778, 'method': 0.1502425020123736, 'requir': 0.1836902793083715, 'elabor': 0.1836902793083715, 'featur': 0.1502425020123736, 'engin': 0.1836902793083715, '.': 0.0021937842439730204}, 'Since 2015,[21]': {'sinc': 0.0711675009532296, '2015': 0.0870111849355444, ',': 0.011488891518428218, '[': 0.02511164498524539, '21': 0.0870111849355444, ']': 0.02511164498524539, 'statist': 0.0394801329886, 'approach': 0.06440208722285458, 'wa': 0.0394801329886, 'replac': 0.0870111849355444, 'neural': 0.04253234072426773, 'network': 0.061899539950299017, 'use': 0.04253234072426773, 'word': 0.12379907990059803, 'embed': 0.0870111849355444, 'captur': 0.0870111849355444, 'semant': 0.0870111849355444, 'properti': 0.0870111849355444, '.': 0.0010391609576714308}, 'Intermediate ta': {'intermedi': 0.1040140398547202, 'task': 0.05376692341046299, '(': 0.04706306373977836, 'e.g.': 0.07340326995687114, ',': 0.008395728417312929, 'part-of-speech': 0.1040140398547202, 'tag': 0.1040140398547202, 'depend': 0.12717019336733415, 'pars': 0.1040140398547202, ')': 0.044156251363670684, 'need': 0.12717019336733415, 'anymor': 0.12717019336733415, '.': 0.0015187737073659375}, 'Neural machine ': {'neural': 0.044895248542282606, 'machin': 0.1060269454932583, 'translat': 0.1836902793083715, ',': 0.024254326538904015, 'base': 0.06533840328087118, 'then-newly-inv': 0.09184513965418575, 'sequence-to-sequ': 0.09184513965418575, 'transform': 0.0751212510061868, 'made': 0.0751212510061868, 'obsolet': 0.0751212510061868, 'intermedi': 0.0751212510061868, 'step': 0.09184513965418575, 'word': 0.06533840328087118, 'align': 0.09184513965418575, 'previous': 0.09184513965418575, 'necessari': 0.09184513965418575, 'statist': 0.04167347371018889, '.': 0.0010968921219865102}, 'The following i': {'follow': 0.1502425020123736, 'list': 0.1836902793083715, 'commonli': 0.13067680656174235, 'research': 0.1502425020123736, 'task': 0.0776633338151132, 'natur': 0.05991879571872299, 'languag': 0.041606545869168295, 'process': 0.06378125196974653, '.': 0.0021937842439730204}, 'Some of these t': {'task': 0.10753384682092598, 'direct': 0.1040140398547202, 'real-world': 0.12717019336733415, 'applic': 0.1040140398547202, ',': 0.008395728417312929, 'commonli': 0.09046855838889857, 'serv': 0.12717019336733415, 'subtask': 0.12717019336733415, 'use': 0.06216265182777592, 'aid': 0.12717019336733415, 'solv': 0.12717019336733415, 'larger': 0.12717019336733415, '.': 0.0015187737073659375}, 'Though natural ': {'though': 0.1126818765092802, 'natur': 0.04493909678904224, 'languag': 0.031204909401876223, 'process': 0.0478359389773099, 'task': 0.0582475003613349, 'close': 0.13776770948127864, 'intertwin': 0.13776770948127864, ',': 0.009095372452089006, 'subdivid': 0.13776770948127864, 'categori': 0.13776770948127864, 'conveni': 0.13776770948127864, '.': 0.0016453381829797655}, 'A coarse divisi': {'coars': 0.41330312844383593, 'divis': 0.41330312844383593, 'given': 0.3380456295278406, '.': 0.004936014548939297}, 'Based on long-s': {'base': 0.10691738718688013, 'long-stand': 0.12292568346466932, 'trend': 0.10691738718688013, 'field': 0.15029204670684942, ',': 0.009922224493188006, 'possibl': 0.15029204670684942, 'extrapol': 0.15029204670684942, 'futur': 0.15029204670684942, 'direct': 0.12292568346466932, 'nlp': 0.055619984419738054, '.': 0.0017949143814324716}, 'As of 2020, thr': {'2020': 0.04662698338315043, ',': 0.0037636023939678644, 'three': 0.05700732806121875, 'trend': 0.04055487100192005, 'among': 0.04662698338315043, 'topic': 0.05700732806121875, 'long-stand': 0.04662698338315043, 'seri': 0.05700732806121875, 'conll': 0.04662698338315043, 'share': 0.04662698338315043, 'task': 0.024102413942621338, 'observ': 0.05700732806121875, ':': 0.030174526323851727, '[': 0.016452457059298705, '43': 0.05700732806121875, ']': 0.016452457059298705, 'higher-level': 0.05700732806121875, 'nlp': 0.021097235469555813, 'applic': 0.04662698338315043, 'involv': 0.03624663870508211, 'aspect': 0.04055487100192005, 'emul': 0.04662698338315043, 'intellig': 0.04055487100192005, 'behaviour': 0.04662698338315043, 'appar': 0.05700732806121875, 'comprehens': 0.05700732806121875, 'natur': 0.018595488326500237, 'languag': 0.012912376304224645, '.': 0.0006808295929571443}, 'More broadly sp': {'broadli': 0.06612850055101376, 'speak': 0.06612850055101376, ',': 0.004365778777002723, 'technic': 0.0540873007244545, 'operation': 0.06612850055101376, 'increasingli': 0.0540873007244545, 'advanc': 0.0540873007244545, 'aspect': 0.047043650362227254, 'cognit': 0.027958800173440754, 'behaviour': 0.0540873007244545, 'repres': 0.06612850055101376, 'one': 0.06612850055101376, 'development': 0.06612850055101376, 'trajectori': 0.06612850055101376, 'nlp': 0.024472793144684743, '(': 0.024472793144684743, 'see': 0.0540873007244545, 'trend': 0.047043650362227254, 'among': 0.0540873007244545, 'conll': 0.0540873007244545, 'share': 0.0540873007244545, 'task': 0.027958800173440754, 'abov': 0.06612850055101376, ')': 0.022961250709108753, '.': 0.0007897623278302875}, 'Cognition refer': {'cognit': 0.0499264288811442, 'refer': 0.11808660812681027, '``': 0.06816017924566606, 'mental': 0.11808660812681027, 'action': 0.11808660812681027, 'process': 0.04100223340912277, 'acquir': 0.11808660812681027, 'knowledg': 0.09658446557938302, 'understand': 0.07508232303195579, 'thought': 0.11808660812681027, ',': 0.01559206706072401, 'experi': 0.09658446557938302, 'sens': 0.11808660812681027, '.': 0.0014102898711255132}, '\"[44] Cognitive': {'``': 0.07340326995687114, '[': 0.03670163497843557, '44': 0.12717019336733415, ']': 0.03670163497843557, 'cognit': 0.05376692341046299, 'scienc': 0.1040140398547202, 'interdisciplinari': 0.1040140398547202, ',': 0.008395728417312929, 'scientif': 0.12717019336733415, 'studi': 0.09046855838889857, 'mind': 0.12717019336733415, 'process': 0.044156251363670684, '.': 0.0015187737073659375}, '[45] Cognitive ': {'[': 0.03670163497843557, '45': 0.12717019336733415, ']': 0.03670163497843557, 'cognit': 0.05376692341046299, 'linguist': 0.22020980987061345, 'interdisciplinari': 0.1040140398547202, 'branch': 0.12717019336733415, ',': 0.008395728417312929, 'combin': 0.12717019336733415, 'knowledg': 0.1040140398547202, 'research': 0.1040140398547202, 'psycholog': 0.12717019336733415, '.': 0.0015187737073659375}, '[46] Especially': {'[': 0.026506736373314577, '46': 0.09184513965418575, ']': 0.026506736373314577, 'especi': 0.09184513965418575, 'dure': 0.0751212510061868, 'age': 0.09184513965418575, 'symbol': 0.05839736235818784, 'nlp': 0.03398999047872881, ',': 0.006063581634726004, 'area': 0.09184513965418575, 'comput': 0.044895248542282606, 'linguist': 0.05301347274662915, 'maintain': 0.09184513965418575, 'strong': 0.09184513965418575, 'tie': 0.0751212510061868, 'cognit': 0.0388316669075566, 'studi': 0.06533840328087118, '.': 0.0010968921219865102}, 'As an example, ': {'exampl': 0.0413303128443836, ',': 0.010914446942506808, 'georg': 0.0413303128443836, 'lakoff': 0.0413303128443836, 'offer': 0.0413303128443836, 'methodolog': 0.0413303128443836, 'build': 0.0413303128443836, 'natur': 0.013481729036712672, 'languag': 0.009361472820562869, 'process': 0.014350781693192972, '(': 0.015295495715427966, 'nlp': 0.030590991430855932, ')': 0.014350781693192972, 'algorithm': 0.029402281476392036, 'perspect': 0.0413303128443836, 'cognit': 0.05242275032520141, 'scienc': 0.03380456295278406, 'along': 0.0413303128443836, 'find': 0.0413303128443836, 'linguist': 0.047712125471966245, '[': 0.011928031367991561, '47': 0.0413303128443836, ']': 0.011928031367991561, 'two': 0.0413303128443836, 'defin': 0.0413303128443836, 'aspect': 0.029402281476392036, ':': 0.021876531584792504, 'tie': 0.03380456295278406, 'part': 0.0413303128443836, 'histor': 0.03380456295278406, 'heritag': 0.0413303128443836, 'less': 0.0413303128443836, 'frequent': 0.03380456295278406, 'address': 0.0413303128443836, 'sinc': 0.03380456295278406, 'statist': 0.018753063169585003, 'turn': 0.0413303128443836, 'dure': 0.03380456295278406, '1990': 0.0413303128443836, '.': 0.0004936014548939297}, 'Nevertheless, a': {'nevertheless': 0.0413303128443836, ',': 0.024557505620640317, 'approach': 0.015295495715427966, 'develop': 0.029402281476392036, 'cognit': 0.05242275032520141, 'model': 0.021876531584792504, 'toward': 0.0413303128443836, 'technic': 0.03380456295278406, 'operationaliz': 0.0413303128443836, 'framework': 0.0826606256887672, 'pursu': 0.0413303128443836, 'context': 0.03380456295278406, 'variou': 0.0413303128443836, 'e.g.': 0.047712125471966245, 'grammar': 0.0882068444291761, '[': 0.047712125471966245, '49': 0.0413303128443836, ']': 0.047712125471966245, 'function': 0.0413303128443836, '50': 0.0413303128443836, 'construct': 0.0413303128443836, '51': 0.0413303128443836, 'comput': 0.020202861844027174, 'psycholinguist': 0.0413303128443836, 'neurosci': 0.0413303128443836, '(': 0.030590991430855932, 'act-r': 0.0413303128443836, ')': 0.028701563386385943, 'howev': 0.03380456295278406, 'limit': 0.0413303128443836, 'uptak': 0.0413303128443836, 'mainstream': 0.0413303128443836, 'nlp': 0.015295495715427966, 'measur': 0.0413303128443836, 'presenc': 0.0413303128443836, 'major': 0.03380456295278406, 'confer': 0.0413303128443836, '52': 0.0413303128443836, 'acl': 0.0413303128443836, '.': 0.0004936014548939297}, 'More recently, ': {'recent': 0.09014550120742416, ',': 0.021828893885013616, 'idea': 0.09014550120742416, 'cognit': 0.09319600057813585, 'nlp': 0.04078798857447457, 'reviv': 0.1102141675850229, 'approach': 0.04078798857447457, 'achiev': 0.09014550120742416, 'explain': 0.1102141675850229, 'e.g.': 0.06361616729595498, 'notion': 0.1102141675850229, '``': 0.06361616729595498, 'ai': 0.07840608393704543, \"''\": 0.07840608393704543, '.': 0.0013162705463838124}, '[53] Likewise, ': {'[': 0.03180808364797749, '53': 0.05510708379251145, ']': 0.03180808364797749, 'likewis': 0.05510708379251145, ',': 0.007276297961671205, 'idea': 0.04507275060371208, 'cognit': 0.02329900014453396, 'nlp': 0.04078798857447457, 'inher': 0.05510708379251145, 'neural': 0.026937149125369562, 'model': 0.05833741755944667, 'multimod': 0.05510708379251145, '(': 0.020393994287237285, 'although': 0.04507275060371208, 'rare': 0.05510708379251145, 'made': 0.04507275060371208, 'explicit': 0.05510708379251145, ')': 0.01913437559092396, '54': 0.05510708379251145, 'develop': 0.039203041968522714, 'artifici': 0.04507275060371208, 'intellig': 0.039203041968522714, 'specif': 0.05510708379251145, 'tool': 0.05510708379251145, 'technolog': 0.039203041968522714, 'use': 0.026937149125369562, 'larg': 0.05510708379251145, 'languag': 0.012481963760750489, 'approach': 0.020393994287237285, '.': 0.0006581352731919062}, '[55]': {'[': 0.15904041823988746, '55': 0.5510708379251146, ']': 0.15904041823988746}}\n",
      "{'Natural languag': 0.12583899889543973, 'It is primarily': 0.12297683786487104, 'It involves pro': 0.06943266325079818, 'statistical and': 0.0799291115770863, 'The goal is a c': 0.0752910980665152, 'The technology ': 0.13041050678786287, 'Challenges in n': 0.08474203393100106, 'Already in 1950': 0.059295576529600096, 'The proposed te': 0.09129394414177648, 'The premise of ': 0.04113968970289012, 'Up to the 1980s': 0.0753797341313614, 'Starting in the': 0.0673109119034716, 'This was due to': 0.06331964221568198, 'transformationa': 0.06564514256513848, '[7]\\nIn 2003, wo': 0.03858374280306281, '[8]\\nIn 2010, To': 0.03830283723778846, 'In the 2010s, r': 0.05077801849852408, 'That popularity': 0.053967569932893054, '[13][14] This i': 0.048058341657293245, '[16]\\nSymbolic a': 0.04406464367231148, 'Machine learnin': 0.04464192165368397, 'Before that the': 0.06025276966892209, '[19][20]\\nThe ea': 0.06294078577365055, 'Only the introd': 0.07389755783057753, 'A major drawbac': 0.14122548388166195, 'Since 2015,[21]': 0.06090119960253251, 'Intermediate ta': 0.07891313462433426, 'Neural machine ': 0.07688165634592199, 'The following i': 0.095557311279065, 'Some of these t': 0.09547144608274806, 'Though natural ': 0.08287404834002544, 'A coarse divisi': 0.2923969752411129, 'Based on long-s': 0.10256285922044137, 'As of 2020, thr': 0.0390074872323473, 'More broadly sp': 0.04755174757513573, 'Cognition refer': 0.08234729295852614, '\"[44] Cognitive': 0.07189626423176126, '[45] Cognitive ': 0.09061672295354797, '[46] Especially': 0.058664079486393655, 'As an example, ': 0.0319759351485817, 'Nevertheless, a': 0.038899943665776975, 'More recently, ': 0.0722027100667883, '[53] Likewise, ': 0.03897415330772309, '[55]': 0.2897172248016298}\n",
      "0.08082117628041495\n",
      " Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Natural language processing has its roots in the 1950s. A major drawback of statistical methods is that they require elaborate feature engineering. A coarse division is given below. [55]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    result = run_summarization(text_str)                                      #Print the final result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim==3.8.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 3.8.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: http://radimrehurek.com/gensim\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPLv2.1\n",
      "Location: c:\\users\\nehal\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: smart-open, numpy, scipy, six\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensim (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip show gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SUMMARY USING GenSim Text Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches.\\nThe premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\\nUp to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.\\nIn 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.\\nIn the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.\\nMachine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\\nThe following is a list of some of the most commonly researched tasks in natural language processing.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize                       # summarize using GENSIM\n",
    "summarize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cognitive',\n",
       " 'cognition',\n",
       " 'nlp',\n",
       " 'computers',\n",
       " 'computing',\n",
       " 'computational',\n",
       " 'model',\n",
       " 'modelling',\n",
       " 'modeling',\n",
       " 'models',\n",
       " 'word',\n",
       " 'words',\n",
       " 'neural',\n",
       " 'natural language processing',\n",
       " 'process',\n",
       " 'processes',\n",
       " 'grammar',\n",
       " 'grammars',\n",
       " 'task',\n",
       " 'tasks',\n",
       " 'linguistics',\n",
       " 'approaches',\n",
       " 'approach',\n",
       " 'rule',\n",
       " 'rules',\n",
       " 'statistical',\n",
       " 'based',\n",
       " 'symbolic',\n",
       " 'symbols',\n",
       " 'network machine learning',\n",
       " 'involves',\n",
       " 'experiment',\n",
       " 'experience',\n",
       " 'intelligence',\n",
       " 'turing',\n",
       " 'aspects',\n",
       " 'layers',\n",
       " 'test',\n",
       " 'speech',\n",
       " 'interdisciplinary',\n",
       " 'intelligent behaviour',\n",
       " 'results',\n",
       " 'semantic',\n",
       " 'extract',\n",
       " 'underpinnings',\n",
       " 'achieve',\n",
       " 'health',\n",
       " 'major',\n",
       " 'hidden layer',\n",
       " 'featuring',\n",
       " 'feature',\n",
       " 'frequently involve',\n",
       " 'networks',\n",
       " 'technical',\n",
       " 'technically operationalizable',\n",
       " 'george',\n",
       " 'lakoff',\n",
       " 'intermediate',\n",
       " 'chinese room',\n",
       " 'including',\n",
       " 'includes',\n",
       " 'include',\n",
       " 'future']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "keywords(mytext, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
