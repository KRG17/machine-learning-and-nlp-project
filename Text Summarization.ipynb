{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.summarization import summarize as gensim_summarize\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    scraped_textdata = urllib.request.urlopen(url)\n",
    "    textdata = scraped_textdata.read()\n",
    "    parsed_textdata = BeautifulSoup(textdata, 'lxml')\n",
    "    paragraphs = parsed_textdata.find_all('p')\n",
    "    formatted_text = \"\"\n",
    "    for para in paragraphs:\n",
    "        formatted_text += para.text\n",
    "    return formatted_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text_without_numbers = re.sub(r'\\d+', '', text)\n",
    "    return text_without_numbers\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    tokenizer = re.compile(r'\\w+')\n",
    "    return [token.lower() for token in tokens if tokenizer.match(token)]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    tag_map = defaultdict(lambda: wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    return [lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens)]\n",
    "\n",
    "def calculate_tf(tokens):\n",
    "    tf_dict = {}\n",
    "    total_words = len(tokens)\n",
    "    for word in tokens:\n",
    "        tf_dict[word] = tf_dict.get(word, 0) + 1 / total_words\n",
    "    return tf_dict\n",
    "\n",
    "def calculate_idf(docs):\n",
    "    idf_dict = {}\n",
    "    total_docs = len(docs)\n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "    for word, freq in idf_dict.items():\n",
    "        idf_dict[word] = math.log(total_docs / (freq + 1))\n",
    "    return idf_dict\n",
    "\n",
    "def calculate_tfidf(tf, idf):\n",
    "    tfidf_dict = {}\n",
    "    for word, tf_value in tf.items():\n",
    "        tfidf_dict[word] = tf_value * idf.get(word, 0)\n",
    "    return tfidf_dict\n",
    "\n",
    "def form_summary(sentences, tfidf_scores, threshold):\n",
    "    summary = ''\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if tfidf_scores[i] >= threshold:\n",
    "            summary += \" \" + sentence\n",
    "    return summary\n",
    "\n",
    "def run_summarization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens_without_punctuations = remove_punctuation(tokens)\n",
    "    tokens_without_stopwords = remove_stopwords(tokens_without_punctuations)\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens_without_stopwords)\n",
    "\n",
    "    # TF-IDF Calculation\n",
    "    tf = calculate_tf(lemmatized_tokens)\n",
    "    idf = calculate_idf([lemmatized_tokens])\n",
    "    tfidf = calculate_tfidf(tf, idf)\n",
    "\n",
    "    # Rank sentences based on TF-IDF scores\n",
    "    tfidf_scores = [sum(tfidf.get(token, 0) for token in lemmatize_tokens(remove_stopwords(remove_punctuation(tokenize_text(sentence))))) for sentence in sentences]\n",
    "\n",
    "    # Print top sentences and form summary using TF-IDF\n",
    "    k = min(5, len(sentences))  # Adjust the number of sentences in the summary\n",
    "    print(\"Top {} sentences using TF-IDF:\".format(k))\n",
    "    for i in range(k):\n",
    "        print(sentences[i])\n",
    "\n",
    "    # Average score for threshold\n",
    "    threshold = sum(tfidf_scores) / len(tfidf_scores)\n",
    "\n",
    "    # Form summary based on threshold using TF-IDF\n",
    "    summary_tfidf = form_summary(sentences, tfidf_scores, threshold)\n",
    "    print(\"\\nSummary using TF-IDF:\")\n",
    "    print(summary_tfidf)\n",
    "\n",
    "    # Gensim Summarization\n",
    "    summary_gensim = gensim_summarize(text, ratio=0.2)  # You can adjust the ratio\n",
    "    print(\"\\nSummary using Gensim:\")\n",
    "    print(summary_gensim)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "    text = scrape_webpage(url)\n",
    "\n",
    "    # Preprocess and tokenize the text\n",
    "    processed_text = preprocess_text(text)\n",
    "    run_summarization(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdcff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==3.8.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59868dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
